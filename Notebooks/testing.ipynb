{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e0d5c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parse_code import *\n",
    "from parse_files import *\n",
    "from print_contents import *\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "from langchain.chains.base import Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import Field\n",
    "from typing import List, Optional, Sequence\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "#torch.cuda.is_available()\n",
    "dotenv.load_dotenv(\".env.private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9123f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some helper functions\n",
    "# count tokens in the results\n",
    "def count_tokens(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def count_all_tokens(texts, tokenizer):\n",
    "    total_tokens = 0\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "def join_metadata(metadata):\n",
    "    key, value = list(metadata.items())[0]\n",
    "    return f\"{key}: {value}\"\n",
    "\n",
    "def run_command(command):\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    assert result.returncode == 0, f\"Command '{command}' failed with error: {result.stderr}\"\n",
    "\n",
    "def format_document(doc):\n",
    "    # Join all metadata key-value pairs as \"key: value\"\n",
    "    meta_str = \"\\n\".join(f\"{k}: {v}\" for k, v in doc.metadata.items())\n",
    "    # Combine metadata and content for the prompt\n",
    "    return f\"{meta_str}\\n{doc.page_content}\"\n",
    "def format_documents(docs):\n",
    "    return \"\\n\\n\".join(format_document(doc) for doc in docs)\n",
    "\n",
    "def format_code_entry(entry):\n",
    "    meta = [\n",
    "        f\"file: {entry.get('file', '')}\",\n",
    "        f\"type: {entry.get('type', '')}\",\n",
    "        f\"name: {entry.get('name', '')}\",\n",
    "        f\"start_line: {entry.get('start_line', '')}\"\n",
    "    ]\n",
    "    docstring = entry.get('docstring', '')\n",
    "    code = entry.get('code', '')\n",
    "    meta_str = \", \".join(meta)\n",
    "    docstring_str = f\"\\nDocstring:\\n{docstring}\" if docstring else \"\"\n",
    "    return f\"{meta_str}\\n{docstring_str}\\ncontent:\\n{code}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22556c5f",
   "metadata": {},
   "source": [
    "### Parsing the source material for retrieval\n",
    "\n",
    "These are for parsing the source code and the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "3ce85555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system calls -- I wrote some cmdline scripts to do the parsing\n",
    "call_1 = \"uv run parse_code.py ../App/python-server/ --replace-source ../App --replace-target ./App -o ./data/backend_code.txt\"\n",
    "call_2 = \"uv run print_contents.py -d ../ -r --exclude-dirs exclude_dirs.txt --exclude-files exclude_files.txt -o ./data/project_structure.txt\"\n",
    "call_3 = \"uv run parse_files.py --exclude-dirs exclude_dirs.txt --exclude-files exclude_files.txt -o ./data/project_files.txt ../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "37d294d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command(call_1)\n",
    "run_command(call_2)\n",
    "run_command(call_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d0165",
   "metadata": {},
   "source": [
    "Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ea95c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read project files, and only keep md, txt, wsd files\n",
    "# the file is in key-value format, where the key is the file name and the value is the content\n",
    "with open(\"./data/project_files.txt\", \"r\") as f:\n",
    "    file_dict = json.load(f)\n",
    "\n",
    "for key in list(file_dict.keys()):\n",
    "    if key.endswith(('requirements.txt', 'ohjeistusta.md')):\n",
    "        del file_dict[key]\n",
    "    if not key.endswith(('.md', '.txt', '.wsd')):\n",
    "        del file_dict[key]\n",
    "\n",
    "with open(\"./data/raw_project_documents.json\", \"w\") as f:\n",
    "    json.dump(file_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9eb149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires that raw_project_documents.json already exists\n",
    "with open(\"./data/raw_project_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    file_dict = json.load(f)\n",
    "\n",
    "documents_json = []\n",
    "for file_path, content in file_dict.items():\n",
    "    doc_type = \"markdown document\" if file_path.endswith(\".md\") else \"document\"\n",
    "    documents_json.append({\n",
    "        \"file\": file_path,\n",
    "        \"type\": doc_type,\n",
    "        \"content\": content\n",
    "    })\n",
    "\n",
    "with open(\"./data/project_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents_json, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "612dd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents\n",
    "with open(\"./data/project_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b7b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to documents and markdown documents\n",
    "documents_dicts = []\n",
    "markdown_documents_dicts = []\n",
    "for doc in documents_json:\n",
    "    if doc[\"type\"] == \"document\":\n",
    "        documents_dicts.append(doc)\n",
    "    elif doc[\"type\"] == \"markdown document\":\n",
    "        markdown_documents_dicts.append(doc)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown document type: {doc['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1c38b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/backend_code.txt', 'r', encoding='utf-8') as f:\n",
    "    code_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6865ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_code_entries = [format_code_entry(entry) for entry in code_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e3c1c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_dicts), len(markdown_documents_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "12e0f0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': './App/README.md',\n",
       " 'type': 'markdown document',\n",
       " 'content': '# Eprice App\\n\\nThe Eprice App is a containerized application that allows users to view the market price of electricity in Finland, both current and historical. The app is built with a modern tech stack, including a Svelte frontend, a FastAPI backend, a PostgreSQL database, and various tools for testing and data management.\\n\\n## Features\\n\\n- **Electricity Price Viewer**: View current and historical electricity prices in Finland.\\n- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\\n- **FastAPI Backend**: A Python-based backend for handling API requests and business logic.\\n- **PostgreSQL Database**: A robust database for storing electricity price data.\\n- **Flyway Migrations**: Manage database schema changes with ease.\\n- **Testing**: End-to-end tests with Playwright and backend API tests with Pytest.\\n- **Chat Engine**: A chat-based interface for interacting with the app.\\n- **Data Loading**: Load and update electricity price data into the database using scripts in a dedicated container.\\n\\n---\\n\\n## Project Structure\\n.\\n\\n ├── README.md # Root README file\\n\\n ├── compose.yaml # Docker Compose configuration \\n \\n ├── chat-engine/ # Chat engine service \\n \\n ├── client/ # Svelte frontend service\\n \\n ├── data-preparation/ # Scripts and data for populating the database\\n \\n ├── database-migrations/ # Flyway migration scripts \\n \\n ├── e2e-tests/ # Playwright end-to-end tests\\n \\n ├── python-server/ # FastAPI backend service\\n \\n └── project.env # Environment variables for the project\\n\\n\\n---\\n\\n## Getting Started\\n\\n### Prerequisites\\n\\n- **Docker** and **Docker Compose**: Install Docker Desktop or Docker CLI.\\n- **Deno**: Required for local development of the frontend (see `client/README.md`).\\n\\n---\\n\\n### Running the App\\n\\n1. Clone the repository:\\n    ```bash\\n    git clone <repository-url>\\n    cd Eprice\\n    ```\\n\\n2. Build and start the containers:\\n    ```bash\\n    docker compose up --build\\n    ```\\n\\n3. Access the services:\\n\\n* Frontend: http://localhost:5173\\n* Backend: http://localhost:8000\\n\\n4. To stop the containers:\\n    ```bash\\n    docker compose down\\n    ```\\n\\n### Testing\\n\\n\\n1. Run Playwright tests:\\n    ```bash\\n    docker compose run --rm --entrypoint=npx e2e-tests playwright test\\n    ```\\n\\n2. Run Pytest for backend API (using uv inside the container):\\n    ```bash\\n    docker compose run backend-tests [uv run pytest]\\n    ```\\n\\n### Environment Variables\\n\\n* Use `.env.local` for local development (gitignored)\\n\\n* Use `project.env` for containerrized development\\n\\n* To inspect environment variables inside a container:\\n    ```bash\\n    docker exec -it <container_name> bash\\n    printenv\\n    ```\\n\\n### Services overview\\n\\n1. Frontend (Client)\\nBuilt with Svelte and Vite.\\nLocated in the client/ directory.\\nSee client/README.md for more details.\\n\\n2. Backend (Python Server)\\nBuilt with FastAPI.\\nLocated in the python-server/ directory.\\nSee python-server/README.md for more details.\\n\\n3. Database\\nPostgreSQL database for storing electricity price data.\\nFlyway is used for managing schema migrations (database-migrations/).\\n\\n4. Chat Engine\\nA chat-based interface for interacting with the app.\\nLocated in the chat-engine/ directory.\\n\\n5. Data Preparation\\nScripts for loading and updating electricity price data.\\nLocated in the data-preparation/ directory.\\n\\n### Contributing\\n\\n1. Fork the repository.\\n\\n2. Create a feature branch:\\n\\n    ```bash\\n    git checkout -b feature-name\\n    ```\\n\\n3. Commit your changes:\\n\\n    ```\\n    git commit -m \"Add feature-name\"\\n    ```\\n\\n4. Push to your branch:\\n\\n    ```bash\\n    git push origin feature-name\\n    ```\\n\\n5. Open pull request.\\n\\n\\n## License\\n\\nThis project us under MIT license: https://mit-license.org/\\n\\n## Acknowledgments\\n\\nElectricity price data is sourced from Pörssisähkö API.\\n\\n## Additional notes\\n\\n**Install docker and docker compose**. Maybe easiest to just install docker desktop, especially on windows.\\n\\n**Postgres is not needed on local machine** (unless you want to run outside containers).\\n\\nThe compose.yaml and the individual Dockerfiles are sufficient to run the App. Docker does the installing for the containers. But you can still run `deno install --allow-scripts`, if you want to run on local host. On windows, after running deno install, `node_modules/` that are loaded into client should not be copied into the container -- the container is using arch-linux as base image. You can either remove those, or add your own `.dockerignore` file.\\n\\nRun using docker compose:\\n\\n`docker compose up --build` (no need to build everytime)\\n\\nYou can also simply `ctrl+C` to shut down the containers, or\\n\\n`docker compose down` to tear down.\\n\\n\\n### **About environment variables**\\n\\nKeep private information private, preferably :-). You can use `.env.local` convention, and keep them gitignored. For public api keys, while developing, we can all get our own api keys.\\n\\n**If you are unsure what environment variables are loaded on your container launch -- either by docker from project.env, or by services using other tools, like dotenv -- you can always go inside the container to check:**\\n\\n```\\ndocker compose up -d <service_name> # launch the container\\n\\ndocker exec -it <container_name> bash # go into cmdline inside\\n\\n(container): printenv # or echo etc.\\n```\\n\\n**For developing client:** There is a sort of a bug in the denolands alpine image, which prevents us from installing with optional flags -- in our case `deno install --allow-scripts`. This means that the node modules need to be copied from local. This is not an issue if you are using linux (or wsl2 on Windows). Later, there might be a change in the client\\'s base image later on to fix this issue.\\n\\nAnd note that the container names are not necessarily same as the service name (they are derived from it though); you can check running cont\\'s with `docker <container> ps`.\\n\\n### Running without Docker\\n\\nCan be done, but needlessly cumbersome. Ask Paavo for the how.\\n\\n\\n### Starting a new client build from scratch\\n\\nIf you want to start the client build from scratch, for example with typescript checking enabled, run:\\n\\n```bash\\ndeno run -A npm:sv@latest create client\\n```\\n\\nfrom the root directory, and choose from the given options. For this project, we have used the most minimal build setup (SveleteKit minimal, no TS typechecking, nothing added, with deno itself for dependency management).\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_documents_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "0585064c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': './App/python-server/serving_over_net.txt',\n",
       " 'type': 'document',\n",
       " 'content': 'PUBLIC_API_URL=http://80.221.17.169:8000\\n\\n\"http://192.168.10.46:5173\",\\n\"http://80.221.17.169:5173\",'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10544aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name = \"BAAI/bge-large-en-v1.5\" #\"BAAI/bge-small-en\"\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "# Create the embeddings object\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2449e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's maximum sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's maximum sequence length: {SentenceTransformer(model_name).max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e87562db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding dimensionality: 1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's embedding dimensionality: {len(embedding_model.embed_query('some random query'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68359b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which headers to split on and their metadata keys\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Heading 1\"),\n",
    "    (\"##\", \"Sub heading\"),\n",
    "    (\"###\", \"Sub-sub heading\"),\n",
    "]\n",
    "\n",
    "# Initialize the Markdown splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b778dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = documents_dicts + markdown_documents_dicts\n",
    "data = []\n",
    "for doc in all_documents:\n",
    "    if doc[\"type\"] == \"document\":\n",
    "        chunks = text_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            embedding = embedding_model.embed_query(chunk)\n",
    "            data.append({\n",
    "                \"file\": doc[\"file\"],\n",
    "                \"type\": doc[\"type\"],\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": None,\n",
    "                \"embedding\": embedding\n",
    "            })\n",
    "    elif doc[\"type\"] == \"markdown document\":\n",
    "        chunks = markdown_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            subchunks = text_splitter.split_text(chunk.page_content)\n",
    "            for subchunk in subchunks:\n",
    "                embedding = embedding_model.embed_query(subchunk)\n",
    "                data.append({\n",
    "                    \"file\": doc[\"file\"],\n",
    "                    \"type\": doc[\"type\"],\n",
    "                    \"content\": subchunk,\n",
    "                    \"metadata\": join_metadata(chunk.metadata),\n",
    "                    \"embedding\": embedding_model.embed_query(subchunk)\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "325195f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Documents\n",
    "documents = []\n",
    "embeddings_list = []\n",
    "for item in data:\n",
    "    doc = Document(\n",
    "        page_content=item['content'],\n",
    "        metadata={\n",
    "            'file': item['file'],\n",
    "            'type': item['type'],\n",
    "            'heading': item['metadata'],\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    embeddings_list.append(np.array(item['embedding'], dtype=np.float32))\n",
    "\n",
    "embeddings_matrix = np.vstack(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8e02413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and embed code files, matching doc structure\n",
    "code_chunks = []\n",
    "for entry in code_data:\n",
    "    # Combine docstring and code for context, or just use code\n",
    "    docstring = entry.get(\"docstring\", \"\")\n",
    "    code_text = entry.get(\"code\", \"\")\n",
    "    full_text = f\"{docstring}\\n{code_text}\" if docstring else code_text\n",
    "\n",
    "    # Chunk the code\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    for chunk in chunks:\n",
    "        # Prepare metadata: include all keys except file, type, code, docstring, and start_line\n",
    "        metadata = {k: v for k, v in entry.items() if k not in [\"file\", \"type\", \"code\", \"docstring\", \"start_line\"]}\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"file\": entry.get(\"file\", \"\"),\n",
    "                \"type\": entry.get(\"type\", \"\"),\n",
    "                \"metadata\": metadata if metadata else None\n",
    "            }\n",
    "        )\n",
    "        embedding = embedding_model.embed_query(chunk)\n",
    "        code_chunks.append((doc, embedding))\n",
    "\n",
    "# Unpack for later use\n",
    "code_documents = [doc for doc, _ in code_chunks]\n",
    "code_embeddings_list = [np.array(emb, dtype=np.float32) for _, emb in code_chunks]\n",
    "code_embeddings_matrix = np.vstack(code_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "839ac83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine code and other documents\n",
    "documents.extend(code_documents)\n",
    "embeddings_matrix = np.vstack((embeddings_matrix, code_embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bc94858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "dimension = 1024#embeddings_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # or another index type\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings_matrix)\n",
    "\n",
    "# Create docstore dict mapping index IDs to Document objects\n",
    "# Create InMemoryDocstore wrapping your documents dict\n",
    "docstore = InMemoryDocstore({i: doc for i, doc in enumerate(documents)})\n",
    "index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "\n",
    "# Correct FAISS vector store initialization\n",
    "vector_store = FAISS(\n",
    "    embedding_function=None,  # embeddings already computed\n",
    "    index=index,              # FAISS index object here\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql+psycopg://username:password@localhost:5432/database\"\n",
    "collection_name = \"document_collection\"\n",
    "vector_store = PGVector(\n",
    "    embeddings=embedding_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection_string,\n",
    ")\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6c5fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you describe the App\"\n",
    "query_embedding = embedding_model.embed_query(query)  # get embedding for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35eb50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ./Documents/diagrams/sources/use_case.wsd\n",
      "Content: @startuml\n",
      "title Use Case Diagram for Electricity Market App\n",
      "\n",
      "actor \"Signed-in User\" as User\n",
      "actor \"Visitor\" as Visitor\n",
      "\n",
      "package \"Frontend (Svelte)\" {\n",
      "    usecase \"View Current Electricity Prices\" as ViewPrices\n",
      "    usecase \"Request Historical/predicted Data\" as RequestHistorical\n",
      "    usecase \"Chat with LLM\" as ChatWithLLM\n",
      "}\n",
      "\n",
      "package \"Database\" {\n",
      "    usecase \"Store and Retrieve Cached Data\" as CacheDB\n",
      "    usecase \"Store and Retrieve User Data\\n(Auth service only)\" as UserDB\n",
      "    usecase \"Text chunks and\\nvector embeddings\" as VectorDB\n",
      "}\n",
      "\n",
      "package \"LLM\" {\n",
      "    usecase \"Embed Query and Search Vector DB\" as EmbedSearch\n",
      "    usecase \"Format prompt with query\\nand context\" as PromptLLM\n",
      "    usecase \"LLM engine\" as LLMengine\n",
      "}\n",
      "\n",
      "package \"Backend (FastAPI)\" {\n",
      "    usecase \"Fetch Current Prices\" as FetchPrices\n",
      "    usecase \"Fetch Data\" as FetchData\n",
      "    usecase \"Check Cache\" as CheckCache\n",
      "    usecase \"Retrieve Data from External APIs\\nand chache\" as RetrieveExternal\n",
      "}\n",
      "\n",
      "package \"External APIs\" {\n",
      "    usecase \"Fetch Data from External APIs\" as ExternalAPIs\n",
      "}\n",
      "\n",
      "Visitor --> ViewPrices\n",
      "User --> ViewPrices\n",
      "User --> RequestHistorical\n",
      "User --> ChatWithLLM\n",
      "\n",
      "ViewPrices --> FetchPrices\n",
      "RequestHistorical --> FetchData\n",
      "FetchPrices --> CheckCache\n",
      "FetchData --> CheckCache\n",
      "CheckCache --> CacheDB\n",
      "CheckCache --> RetrieveExternal\n",
      "RetrieveExternal --> CacheDB\n",
      "RetrieveExternal --> ExternalAPIs\n",
      "\n",
      "ChatWithLLM --> EmbedSearch\n",
      "EmbedSearch --> VectorDB\n",
      "EmbedSearch --> PromptLLM\n",
      "PromptLLM --> LLMengine\n",
      "\n",
      "@enduml...\n",
      "\n",
      "File: ./Documents/diagrams/sources/authentication_use_case.wsd\n",
      "Content: @startuml\n",
      "title Use Case Diagram for authentication in Electricity Market App\n",
      "\n",
      "actor \"Signed-in User\" as User\n",
      "actor \"Visitor\" as Visitor\n",
      "\n",
      "package \"Frontend (Svelte)\" {\n",
      "    usecase \"Register\" as Register\n",
      "    usecase \"Login\" as Login\n",
      "    usecase \"Logout\" as Logout\n",
      "    usecase \"Verify Email\" as VerifyEmail\n",
      "    usecase \"Resend Verification Code\" as ResendCode\n",
      "}\n",
      "\n",
      "package \"Backend (FastAPI)\" {\n",
      "    package \"User Authentication\" as Auth  {\n",
      "        usecase \"Signal front to\\n remove JWT\" as RemoveJWT\n",
      "        usecase \"Hash Password and\\n send with email to DB\\n(un-verified)\" as HashPassword\n",
      "        usecase \"Verify Status & Password\\nand pack JWT\" as VerifyPassword\n",
      "        usecase \"Generate Verification Code\\nand send email\" as GenCode\n",
      "        usecase \"Verify User Code\" as VerifyUserCode\n",
      "        usecase \"Update Verification Code\\nand send email\" as UpdateCode\n",
      "    }\n",
      "    usecase \"Manage route access\\nwith JWT's\" as ManageJWT\n",
      "}\n",
      "\n",
      "package \"Database\" {\n",
      "    usecase \"Store and Retrieve User Data\" as UserDB\n",
      "}\n",
      "\n",
      "User --> Login\n",
      "User --> Logout\n",
      "User --> Register\n",
      "User --> VerifyEmail\n",
      "User --> ResendCode\n",
      "Visitor --> Register\n",
      "Visitor --> Login\n",
      "\n",
      "Register --> HashPassword\n",
      "Register --> GenCode\n",
      "\n",
      "Login --> VerifyPassword\n",
      "VerifyPassword <--> UserDB\n",
      "VerifyPassword <--> ManageJWT\n",
      "Logout <--> RemoveJWT\n",
      "\n",
      "HashPassword --> UserDB\n",
      "GenCode --> UserDB\n",
      "\n",
      "VerifyEmail --> VerifyUserCode\n",
      "VerifyUserCode --> UserDB\n",
      "\n",
      "ResendCode --> UpdateCode\n",
      "UpdateCode --> UserDB\n",
      "\n",
      "@enduml...\n",
      "\n",
      "File: ./App/README.md\n",
      "Content: - **Electricity Price Viewer**: View current and historical electricity prices in Finland.\n",
      "- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\n",
      "- **FastAPI Backend**: A Python-based backend for handling API requests and business logic.\n",
      "- **PostgreSQL Database**: A robust database for storing electricity price data.\n",
      "- **Flyway Migrations**: Manage database schema changes with ease.\n",
      "- **Testing**: End-to-end tests with Playwright and backend API tests with Pytest.\n",
      "- **Chat Engine**: A chat-based interface for interacting with the app.\n",
      "- **Data Loading**: Load and update electricity price data into the database using scripts in a dedicated container.  \n",
      "---...\n",
      "\n",
      "File: ./Documents/diagrams/sources/llm_retrieval.wsd\n",
      "Content: @startuml\n",
      "title LLM engine with retrieval (pre-processed data)\n",
      "User -> LLM_Client: User initiates a query\n",
      "note over LLM_Client: Client embeds the query\n",
      "note over Database: DB has text chunks\\nand embeddings\n",
      "LLM_Client -> Database: Send embedded query\n",
      "note over Database: similarity search\\nfor relevant texts\n",
      "Database -> LLM_Client: Return relevant text chunks\n",
      "note over LLM_Client: Combine query with\\nretrieved text chunks\n",
      "LLM_Client -> LLM_Model: formatted prompt with query and context\n",
      "LLM_Model -> LLM_Client: Return generated response\n",
      "LLM_Client -> User: return generated response\n",
      "@enduml...\n",
      "\n",
      "File: ./App/README.md\n",
      "Content: The Eprice App is a containerized application that allows users to view the market price of electricity in Finland, both current and historical. The app is built with a modern tech stack, including a Svelte frontend, a FastAPI backend, a PostgreSQL database, and various tools for testing and data management....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_by_vector(query_embedding, k=5)\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"File: {doc.metadata['file']}\")\n",
    "    if \"Heading\" in doc.metadata:\n",
    "        print(f\"Heading: {doc.metadata['heading']}\")\n",
    "    print(f\"Content: {doc.page_content}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21b98909",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})  # retrieve top 20 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamingHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)  # or handle token as you want\n",
    "\n",
    "class LLMReranker(BaseDocumentCompressor):\n",
    "    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents\")\n",
    "    document_variable_name: str = \"document\"\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        *,\n",
    "        callbacks: Optional[list] = None,\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        scored_docs = []\n",
    "        for doc in documents:\n",
    "            inputs = {\n",
    "                \"query\": query,\n",
    "                self.document_variable_name: doc.page_content,\n",
    "            }\n",
    "            output = self.llm_chain.run(inputs)\n",
    "            try:\n",
    "                score = int(output.strip())\n",
    "            except Exception:\n",
    "                score = 0\n",
    "            scored_docs.append((doc, score))\n",
    "        scored_docs.sort(key=lambda x: x[[1]], reverse=True)\n",
    "        return [doc for doc, score in scored_docs]\n",
    "    \n",
    "class LLMRerankerBatched(BaseDocumentCompressor):\n",
    "    \"\"\" LLM Reranker that uses LLMChain to rerank documents in batches.\n",
    "    It passes the documents to the LLM in a single call and expects the LLM to return a list of scores.\n",
    "    \"\"\"\n",
    "    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents\")\n",
    "    document_variable_name: str = \"document\"\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        *,\n",
    "        callbacks: Optional[list] = None,\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        scored_docs = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI chat model -- this is used for reranking\n",
    "llm_reranker = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define a template for reranking\n",
    "rerank_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"document\"],\n",
    "    template=(\n",
    "        \"Given the query:\\n{query}\\n\\n\"\n",
    "        \"Rate the relevance of the following document to the query on a scale from 1 to 10:\\n\"\n",
    "        \"{document}\\n\\n\"\n",
    "        \"Only output the score as an integer.\"\n",
    "    ),\n",
    ")\n",
    "rerank_batch_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"documents\"],\n",
    "    template=(\n",
    "        \"Given the query:\\n{query}\\n\\n\"\n",
    "        \"Rate the relevance of the following documents to the query on a scale from 1 to 10:\\n\"\n",
    "        \"{documents}\\n\\n\"\n",
    "        \"Only output the scores as a list of integers.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm_streaming = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callbacks=[MyStreamingHandler()],\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4f14e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ContextualCompressionRetriever\nbase_compressor\n  Input should be a valid dictionary or instance of BaseDocumentCompressor [type=model_type, input_value=<function <lambda> at 0x70e8e02b1620>, input_type=function]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m reranker = LLMReranker(llm_chain=rerank_chain)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Wrap your base retriever with ContextualCompressionRetriever using the reranker\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m reranking_retriever = \u001b[43mContextualCompressionRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_retriever\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_compressor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#reranker,\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/Eprice/Notebooks/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/Eprice/Notebooks/.venv/lib/python3.11/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ContextualCompressionRetriever\nbase_compressor\n  Input should be a valid dictionary or instance of BaseDocumentCompressor [type=model_type, input_value=<function <lambda> at 0x70e8e02b1620>, input_type=function]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "# Create an LLMChain for reranking\n",
    "#rerank_chain = LLMChain(llm=llm_reranker, prompt=rerank_prompt)\n",
    "# Instantiate your reranker\n",
    "#reranker = LLMReranker(llm_chain=rerank_chain)\n",
    "\n",
    "rerank_chain = LLMChain(llm=llm_streaming, prompt=rerank_batch_prompt)\n",
    "reranker = LLMRerankerBatched(llm_chain=rerank_chain)\n",
    "\n",
    "# Wrap your base retriever with ContextualCompressionRetriever using the reranker\n",
    "reranking_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=reranker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80d55c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the purpose of the App?',\n",
       " 'document': 'This is a sample document content.',\n",
       " 'text': '1'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_chain.invoke({\n",
    "    \"query\": \"What is the purpose of the App?\",\n",
    "    \"document\": \"This is a sample document content.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a88e773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert assistant. Use the following project documents to answer the user's question.\"\n",
    "    \"If the answer is in the documents, provide it and reference the document(s) used.\"\n",
    "    \"If the answer is not in the documents, provide a general answer based on your knowledge,\"\n",
    "    \"and state that the documents do not contain the answer.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    ")\n",
    "def full_chain(query: str):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "    response = llm.invoke(prompt_template.format(context=context, question=query))\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "884ab624",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert assistant. Use the following project documents to answer the user's question.\"\n",
    "    \"If the answer is in the documents, provide it and reference the document(s) used.\"\n",
    "    \"If the answer is not in the documents, provide a general answer based on your knowledge,\"\n",
    "    \"and state that the documents do not contain the answer.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    ")\n",
    "async def full_chain_stream(query: str):\n",
    "    # Embed query and retrieve reranked docs as before\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "\n",
    "    # Prepare prompt input\n",
    "    prompt_text = prompt_template.format(context=context, question=query)\n",
    "\n",
    "    # Stream tokens asynchronously from the LLM\n",
    "    async for token in llm.astream(prompt_text):\n",
    "        yield token.content  # yield each token as it arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b311ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = InMemoryChatMessageHistory()\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an expert assistant. Use the following project documents to answer the user's question. \"\n",
    "                \"If the answer is in the documents, provide it and reference the document(s) used. \"\n",
    "                \"If the answer is not in the documents, provide a general answer based on your knowledge, \"\n",
    "                \"and state that the documents do not contain the answer.\"\n",
    "            )\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "def full_chain_with_history(query: str):\n",
    "    # Embed and retrieve documents as before\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "\n",
    "    # Add the context as a system message or part of the prompt\n",
    "    # Here we prepend context as a system message for clarity\n",
    "    system_msg = SystemMessage(content=f\"Context:\\n{context}\")\n",
    "\n",
    "    # Get current history messages and append the system context message\n",
    "    history_msgs = message_history.messages + [system_msg]\n",
    "\n",
    "    # Add the current user query as a HumanMessage\n",
    "    user_msg = HumanMessage(content=query)\n",
    "\n",
    "    # Prepare messages for the LLM: history + current user message\n",
    "    messages = history_msgs + [user_msg]\n",
    "\n",
    "    # Invoke the LLM with the messages\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Add user and AI messages to history\n",
    "    message_history.add_user_message(query)\n",
    "    message_history.add_ai_message(response.content)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "async def full_chain_with_history_stream(query: str):\n",
    "    # Embed and retrieve documents as before\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "\n",
    "    # Add the context as a system message or part of the prompt\n",
    "    system_msg = SystemMessage(content=f\"Context:\\n{context}\")\n",
    "\n",
    "    # Get current history messages and append the system context message\n",
    "    history_msgs = message_history.messages + [system_msg]\n",
    "\n",
    "    # Add the current user query as a HumanMessage\n",
    "    user_msg = HumanMessage(content=query)\n",
    "\n",
    "    # Prepare messages for the LLM: history + current user message\n",
    "    messages = history_msgs + [user_msg]\n",
    "\n",
    "    # Stream tokens asynchronously from the LLM\n",
    "    async for token in llm.astream(messages):\n",
    "        yield token.content  # yield each token as it arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21fdf729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Eprice App is a containerized application designed to allow users to view both current and historical market prices of electricity in Finland. It features a modern tech stack that includes:\\n\\n- **Frontend**: Built with Svelte and Vite, providing a responsive user interface.\\n- **Backend**: Developed using FastAPI, which handles API requests and business logic.\\n- **Database**: Utilizes PostgreSQL for robust data storage of electricity price information.\\n- **Flyway Migrations**: Facilitates easy management of database schema changes.\\n- **Testing**: Incorporates end-to-end testing using Playwright and backend API testing with Pytest.\\n- **Chat Engine**: Offers a chat-based interface for user interaction.\\n- **Data Loading**: Includes scripts for loading and updating electricity price data into the database.\\n\\nOverall, the app combines various components to deliver a comprehensive platform for monitoring electricity prices in Finland. \\n\\nThis information is derived from the README document of the app.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain(\"Can you describe the app?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94213df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eprice App is a containerized application designed to allow users to view both current and historical market prices of electricity in Finland. It utilizes a modern tech stack that includes:\n",
      "\n",
      "- **Frontend**: Built with Svelte and Vite, providing a modern and responsive user interface.\n",
      "- **Backend**: Developed using FastAPI, which handles API requests and business logic.\n",
      "- **Database**: Utilizes PostgreSQL for robust storage of electricity price data.\n",
      "- **Data Management**: Employs Flyway migrations to manage database schema changes easily.\n",
      "- **Testing**: Incorporates end-to-end testing with Playwright and backend API testing with Pytest.\n",
      "- **Chat Engine**: Features a chat-based interface for user interaction.\n",
      "- **Data Loading**: Includes scripts for loading and updating electricity price data into the database using a dedicated container.\n",
      "\n",
      "Overall, the app provides a comprehensive solution for monitoring electricity prices, leveraging modern technologies for a seamless user experience. \n",
      "\n",
      "This information is derived from the README.md document for the Eprice App."
     ]
    }
   ],
   "source": [
    "response = \"\"\n",
    "async for resp in full_chain_stream(\"Can you describe the App?\"):\n",
    "    response += resp\n",
    "    print(resp, end=\"\", flush=True)  # Print each token as it arrives    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "133705fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of the App, as outlined in the project documents, includes several functionalities:\\n\\n1. It can be used independently to retrieve or update data, saving data to a location that is available to migrations and the database.\\n2. It runs end-to-end tests for the system.\\n3. It handles database schema migrations.\\n4. It stores application data, including user information and embeddings for retrieval.\\n\\nThese purposes are detailed in the document titled \"Application Overview\" found in the project description.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35afa35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
