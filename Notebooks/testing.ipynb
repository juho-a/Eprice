{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d5c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parse_code import *\n",
    "from parse_files import *\n",
    "from parse_contents import *\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "from langchain.chains.base import Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "from pydantic import Field\n",
    "from typing import List, Optional, Sequence\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "from helpers import *\n",
    "import dotenv\n",
    "#torch.cuda.is_available()\n",
    "dotenv.load_dotenv(\".env.private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22556c5f",
   "metadata": {},
   "source": [
    "### Parsing the source material for retrieval\n",
    "\n",
    "These are for parsing the source code and the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "3ce85555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system calls -- I wrote some cmdline scripts to do the parsing\n",
    "call_1 = \"uv run parse_code.py ../App/python-server/ --replace-source ../App --replace-target ./App -o ./data/backend_code.txt\"\n",
    "call_2 = \"uv run print_contents.py -d ../ -r --exclude-dirs exclude_dirs.txt --exclude-files exclude_files.txt -o ./data/project_structure.txt\"\n",
    "call_3 = \"uv run parse_files.py --exclude-dirs exclude_dirs.txt --exclude-files exclude_files.txt -o ./data/project_files.txt ../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "37d294d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command(call_1)\n",
    "run_command(call_2)\n",
    "run_command(call_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d0165",
   "metadata": {},
   "source": [
    "Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ea95c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read project files, and only keep md, txt, wsd files\n",
    "# the file is in key-value format, where the key is the file name and the value is the content\n",
    "with open(\"./data/project_files.txt\", \"r\") as f:\n",
    "    file_dict = json.load(f)\n",
    "\n",
    "for key in list(file_dict.keys()):\n",
    "    if key.endswith(('requirements.txt', 'ohjeistusta.md')):\n",
    "        del file_dict[key]\n",
    "    if not key.endswith(('.md', '.txt', '.wsd')):\n",
    "        del file_dict[key]\n",
    "\n",
    "with open(\"./data/raw_project_documents.json\", \"w\") as f:\n",
    "    json.dump(file_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9eb149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires that raw_project_documents.json already exists\n",
    "with open(\"./data/raw_project_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    file_dict = json.load(f)\n",
    "\n",
    "documents_json = []\n",
    "for file_path, content in file_dict.items():\n",
    "    doc_type = \"markdown document\" if file_path.endswith(\".md\") else \"document\"\n",
    "    documents_json.append({\n",
    "        \"file\": file_path,\n",
    "        \"type\": doc_type,\n",
    "        \"content\": content\n",
    "    })\n",
    "\n",
    "with open(\"./data/project_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents_json, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "612dd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents\n",
    "with open(\"./data/project_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b7b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to documents and markdown documents\n",
    "documents_dicts = []\n",
    "markdown_documents_dicts = []\n",
    "for doc in documents_json:\n",
    "    if doc[\"type\"] == \"document\":\n",
    "        documents_dicts.append(doc)\n",
    "    elif doc[\"type\"] == \"markdown document\":\n",
    "        markdown_documents_dicts.append(doc)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown document type: {doc['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1c38b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/backend_code.txt', 'r', encoding='utf-8') as f:\n",
    "    code_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6865ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_code_entries = [format_code_entry(entry) for entry in code_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e3c1c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_dicts), len(markdown_documents_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "12e0f0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': './App/README.md',\n",
       " 'type': 'markdown document',\n",
       " 'content': '# Eprice App\\n\\nThe Eprice App is a containerized application that allows users to view the market price of electricity in Finland, both current and historical. The app is built with a modern tech stack, including a Svelte frontend, a FastAPI backend, a PostgreSQL database, and various tools for testing and data management.\\n\\n## Features\\n\\n- **Electricity Price Viewer**: View current and historical electricity prices in Finland.\\n- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\\n- **FastAPI Backend**: A Python-based backend for handling API requests and business logic.\\n- **PostgreSQL Database**: A robust database for storing electricity price data.\\n- **Flyway Migrations**: Manage database schema changes with ease.\\n- **Testing**: End-to-end tests with Playwright and backend API tests with Pytest.\\n- **Chat Engine**: A chat-based interface for interacting with the app.\\n- **Data Loading**: Load and update electricity price data into the database using scripts in a dedicated container.\\n\\n---\\n\\n## Project Structure\\n.\\n\\n ├── README.md # Root README file\\n\\n ├── compose.yaml # Docker Compose configuration \\n \\n ├── chat-engine/ # Chat engine service \\n \\n ├── client/ # Svelte frontend service\\n \\n ├── data-preparation/ # Scripts and data for populating the database\\n \\n ├── database-migrations/ # Flyway migration scripts \\n \\n ├── e2e-tests/ # Playwright end-to-end tests\\n \\n ├── python-server/ # FastAPI backend service\\n \\n └── project.env # Environment variables for the project\\n\\n\\n---\\n\\n## Getting Started\\n\\n### Prerequisites\\n\\n- **Docker** and **Docker Compose**: Install Docker Desktop or Docker CLI.\\n- **Deno**: Required for local development of the frontend (see `client/README.md`).\\n\\n---\\n\\n### Running the App\\n\\n1. Clone the repository:\\n    ```bash\\n    git clone <repository-url>\\n    cd Eprice\\n    ```\\n\\n2. Build and start the containers:\\n    ```bash\\n    docker compose up --build\\n    ```\\n\\n3. Access the services:\\n\\n* Frontend: http://localhost:5173\\n* Backend: http://localhost:8000\\n\\n4. To stop the containers:\\n    ```bash\\n    docker compose down\\n    ```\\n\\n### Testing\\n\\n\\n1. Run Playwright tests:\\n    ```bash\\n    docker compose run --rm --entrypoint=npx e2e-tests playwright test\\n    ```\\n\\n2. Run Pytest for backend API (using uv inside the container):\\n    ```bash\\n    docker compose run backend-tests [uv run pytest]\\n    ```\\n\\n### Environment Variables\\n\\n* Use `.env.local` for local development (gitignored)\\n\\n* Use `project.env` for containerrized development\\n\\n* To inspect environment variables inside a container:\\n    ```bash\\n    docker exec -it <container_name> bash\\n    printenv\\n    ```\\n\\n### Services overview\\n\\n1. Frontend (Client)\\nBuilt with Svelte and Vite.\\nLocated in the client/ directory.\\nSee client/README.md for more details.\\n\\n2. Backend (Python Server)\\nBuilt with FastAPI.\\nLocated in the python-server/ directory.\\nSee python-server/README.md for more details.\\n\\n3. Database\\nPostgreSQL database for storing electricity price data.\\nFlyway is used for managing schema migrations (database-migrations/).\\n\\n4. Chat Engine\\nA chat-based interface for interacting with the app.\\nLocated in the chat-engine/ directory.\\n\\n5. Data Preparation\\nScripts for loading and updating electricity price data.\\nLocated in the data-preparation/ directory.\\n\\n### Contributing\\n\\n1. Fork the repository.\\n\\n2. Create a feature branch:\\n\\n    ```bash\\n    git checkout -b feature-name\\n    ```\\n\\n3. Commit your changes:\\n\\n    ```\\n    git commit -m \"Add feature-name\"\\n    ```\\n\\n4. Push to your branch:\\n\\n    ```bash\\n    git push origin feature-name\\n    ```\\n\\n5. Open pull request.\\n\\n\\n## License\\n\\nThis project us under MIT license: https://mit-license.org/\\n\\n## Acknowledgments\\n\\nElectricity price data is sourced from Pörssisähkö API.\\n\\n## Additional notes\\n\\n**Install docker and docker compose**. Maybe easiest to just install docker desktop, especially on windows.\\n\\n**Postgres is not needed on local machine** (unless you want to run outside containers).\\n\\nThe compose.yaml and the individual Dockerfiles are sufficient to run the App. Docker does the installing for the containers. But you can still run `deno install --allow-scripts`, if you want to run on local host. On windows, after running deno install, `node_modules/` that are loaded into client should not be copied into the container -- the container is using arch-linux as base image. You can either remove those, or add your own `.dockerignore` file.\\n\\nRun using docker compose:\\n\\n`docker compose up --build` (no need to build everytime)\\n\\nYou can also simply `ctrl+C` to shut down the containers, or\\n\\n`docker compose down` to tear down.\\n\\n\\n### **About environment variables**\\n\\nKeep private information private, preferably :-). You can use `.env.local` convention, and keep them gitignored. For public api keys, while developing, we can all get our own api keys.\\n\\n**If you are unsure what environment variables are loaded on your container launch -- either by docker from project.env, or by services using other tools, like dotenv -- you can always go inside the container to check:**\\n\\n```\\ndocker compose up -d <service_name> # launch the container\\n\\ndocker exec -it <container_name> bash # go into cmdline inside\\n\\n(container): printenv # or echo etc.\\n```\\n\\n**For developing client:** There is a sort of a bug in the denolands alpine image, which prevents us from installing with optional flags -- in our case `deno install --allow-scripts`. This means that the node modules need to be copied from local. This is not an issue if you are using linux (or wsl2 on Windows). Later, there might be a change in the client\\'s base image later on to fix this issue.\\n\\nAnd note that the container names are not necessarily same as the service name (they are derived from it though); you can check running cont\\'s with `docker <container> ps`.\\n\\n### Running without Docker\\n\\nCan be done, but needlessly cumbersome. Ask Paavo for the how.\\n\\n\\n### Starting a new client build from scratch\\n\\nIf you want to start the client build from scratch, for example with typescript checking enabled, run:\\n\\n```bash\\ndeno run -A npm:sv@latest create client\\n```\\n\\nfrom the root directory, and choose from the given options. For this project, we have used the most minimal build setup (SveleteKit minimal, no TS typechecking, nothing added, with deno itself for dependency management).\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_documents_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "0585064c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': './App/python-server/serving_over_net.txt',\n",
       " 'type': 'document',\n",
       " 'content': 'PUBLIC_API_URL=http://80.221.17.169:8000\\n\\n\"http://192.168.10.46:5173\",\\n\"http://80.221.17.169:5173\",'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10544aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name = \"BAAI/bge-small-en\" #\"BAAI/bge-large-en-v1.5\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Create the embeddings object\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87562db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's maximum sequence length: 512\n",
      "Model's embedding dimensionality: 384\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's maximum sequence length: {SentenceTransformer(model_name).max_seq_length}\")\n",
    "print(f\"Model's embedding dimensionality: {len(embedding_model.embed_query('some random query'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68359b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which headers to split on and their metadata keys\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Heading 1\"),\n",
    "    (\"##\", \"Sub heading\"),\n",
    "    (\"###\", \"Sub-sub heading\"),\n",
    "]\n",
    "\n",
    "# Initialize the Markdown splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b778dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = documents_dicts + markdown_documents_dicts\n",
    "data = []\n",
    "for doc in all_documents:\n",
    "    if doc[\"type\"] == \"document\":\n",
    "        chunks = text_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            embedding = embedding_model.embed_query(chunk)\n",
    "            data.append({\n",
    "                \"file\": doc[\"file\"],\n",
    "                \"type\": doc[\"type\"],\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": None,\n",
    "                \"embedding\": embedding\n",
    "            })\n",
    "    elif doc[\"type\"] == \"markdown document\":\n",
    "        chunks = markdown_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            subchunks = text_splitter.split_text(chunk.page_content)\n",
    "            for subchunk in subchunks:\n",
    "                embedding = embedding_model.embed_query(subchunk)\n",
    "                data.append({\n",
    "                    \"file\": doc[\"file\"],\n",
    "                    \"type\": doc[\"type\"],\n",
    "                    \"content\": subchunk,\n",
    "                    \"metadata\": join_metadata(chunk.metadata),\n",
    "                    \"embedding\": embedding_model.embed_query(subchunk)\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "325195f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Documents\n",
    "documents = []\n",
    "embeddings_list = []\n",
    "for item in data:\n",
    "    doc = Document(\n",
    "        page_content=item['content'],\n",
    "        metadata={\n",
    "            'file': item['file'],\n",
    "            'type': item['type'],\n",
    "            'heading': item['metadata'],\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    embeddings_list.append(np.array(item['embedding'], dtype=np.float32))\n",
    "\n",
    "embeddings_matrix = np.vstack(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8e02413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and embed code files, matching doc structure\n",
    "code_chunks = []\n",
    "for entry in code_data:\n",
    "    # Combine docstring and code for context, or just use code\n",
    "    docstring = entry.get(\"docstring\", \"\")\n",
    "    code_text = entry.get(\"code\", \"\")\n",
    "    full_text = f\"{docstring}\\n{code_text}\" if docstring else code_text\n",
    "\n",
    "    # Chunk the code\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    for chunk in chunks:\n",
    "        # Prepare metadata: include all keys except file, type, code, docstring, and start_line\n",
    "        metadata = {k: v for k, v in entry.items() if k not in [\"file\", \"type\", \"code\", \"docstring\", \"start_line\"]}\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"file\": entry.get(\"file\", \"\"),\n",
    "                \"type\": entry.get(\"type\", \"\"),\n",
    "                \"metadata\": metadata if metadata else None\n",
    "            }\n",
    "        )\n",
    "        embedding = embedding_model.embed_query(chunk)\n",
    "        code_chunks.append((doc, embedding))\n",
    "\n",
    "# Unpack for later use\n",
    "code_documents = [doc for doc, _ in code_chunks]\n",
    "code_embeddings_list = [np.array(emb, dtype=np.float32) for _, emb in code_chunks]\n",
    "code_embeddings_matrix = np.vstack(code_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "839ac83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine code and other documents\n",
    "documents.extend(code_documents)\n",
    "embeddings_matrix = np.vstack((embeddings_matrix, code_embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bc94858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "dimension = 1024#embeddings_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # or another index type\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings_matrix)\n",
    "\n",
    "# Create docstore dict mapping index IDs to Document objects\n",
    "# Create InMemoryDocstore wrapping your documents dict\n",
    "docstore = InMemoryDocstore({i: doc for i, doc in enumerate(documents)})\n",
    "index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "\n",
    "# Correct FAISS vector store initialization\n",
    "vector_store = FAISS(\n",
    "    embedding_function=None,  # embeddings already computed\n",
    "    index=index,              # FAISS index object here\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d57cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql+psycopg://username:password@localhost:5432/database\"\n",
    "collection_name = \"documents\"\n",
    "vector_store = PGVector(\n",
    "    embeddings=embedding_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection_string,\n",
    ")\n",
    "#vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c5fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you describe the App\"\n",
    "query_embedding = embedding_model.embed_query(query)  # get embedding for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35eb50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ./App/python-server/controllers/auth_controller.py\n",
      "Content: return {\"message\": \"Welcome!\"}...\n",
      "\n",
      "File: ./Documents/diagrams/sources/use_case.wsd\n",
      "Content: @startuml\n",
      "title Use Case Diagram for Electricity Market App\n",
      "\n",
      "actor \"Signed-in User\" as User\n",
      "actor \"Visitor\" as Visitor\n",
      "\n",
      "package \"Frontend (Svelte)\" {\n",
      "    usecase \"View Current Electricity Prices\" as ViewPrices\n",
      "    usecase \"Request Historical/predicted Data\" as RequestHistorical\n",
      "    usecase \"Chat with LLM\" as ChatWithLLM\n",
      "}\n",
      "\n",
      "package \"Database\" {\n",
      "    usecase \"Store and Retrieve Cached Data\" as CacheDB\n",
      "    usecase \"Store and Retrieve User Data\\n(Auth service only)\" as UserDB\n",
      "    usecase \"Text chunks and\\nvector embeddings\" as VectorDB\n",
      "}\n",
      "\n",
      "package \"LLM\" {\n",
      "    usecase \"Embed Query and Search Vector DB\" as EmbedSearch\n",
      "    usecase \"Format prompt with query\\nand context\" as PromptLLM\n",
      "    usecase \"LLM engine\" as LLMengine\n",
      "}\n",
      "\n",
      "package \"Backend (FastAPI)\" {\n",
      "    usecase \"Fetch Current Prices\" as FetchPrices\n",
      "    usecase \"Fetch Data\" as FetchData\n",
      "    usecase \"Check Cache\" as CheckCache\n",
      "    usecase \"Retrieve Data from External APIs\\nand chache\" as RetrieveExternal\n",
      "}\n",
      "\n",
      "package \"External APIs\" {\n",
      "    usecase \"Fetch Data from External APIs\" as ExternalAPIs\n",
      "}\n",
      "\n",
      "Visitor --> ViewPrices\n",
      "User --> ViewPrices\n",
      "User --> RequestHistorical\n",
      "User --> ChatWithLLM\n",
      "\n",
      "ViewPrices --> FetchPrices\n",
      "RequestHistorical --> FetchData\n",
      "FetchPrices --> CheckCache\n",
      "FetchData --> CheckCache\n",
      "CheckCache --> CacheDB\n",
      "CheckCache --> RetrieveExternal\n",
      "RetrieveExternal --> CacheDB\n",
      "RetrieveExternal --> ExternalAPIs\n",
      "\n",
      "ChatWithLLM --> EmbedSearch\n",
      "EmbedSearch --> VectorDB\n",
      "EmbedSearch --> PromptLLM\n",
      "PromptLLM --> LLMengine\n",
      "\n",
      "@enduml...\n",
      "\n",
      "File: ./App/python-server/serving_over_net.txt\n",
      "Content: PUBLIC_API_URL=http://80.221.17.169:8000\n",
      "\n",
      "\"http://192.168.10.46:5173\",\n",
      "\"http://80.221.17.169:5173\",...\n",
      "\n",
      "File: ./Documents/diagrams/sources/services_diagram.wsd\n",
      "Content: @startuml\n",
      "title Main services and their interactions\n",
      "skinparam rectangle {\n",
      "    BackgroundColor #FDF6E3\n",
      "    BorderColor #586e75\n",
      "}\n",
      "skinparam cloud {\n",
      "    BackgroundColor #FDF6E3\n",
      "    BorderColor #586e75\n",
      "}\n",
      "\n",
      "cloud \"Docker Network\\nmanages networking\" as DockerNetwork\n",
      "\n",
      "package \"Database Layer\" {\n",
      "    [Database] <<container>> \n",
      "    [Database Migrations] <<container>> \n",
      "}\n",
      "\n",
      "package \"Backend Layer\" {\n",
      "    [Server] <<container>> \n",
      "    [Chat Engine] <<container>> \n",
      "}\n",
      "\n",
      "package \"Frontend Layer\" {\n",
      "    [Client] <<container>> \n",
      "    [E2E Tests] <<container>> \n",
      "}\n",
      "\n",
      "cloud \"External APIs\" as ExternalAPIs\n",
      "\n",
      "[Database Migrations] --> [Database] : Applies migrations\n",
      "[Server] --> [Database] : Reads/Writes data\n",
      "[Client] --> [Server] : API calls (port 8000)\n",
      "[Client] --> [Chat Engine] : LLM service (port 7860)\n",
      "[Chat Engine] --> [Database] : Reads/Writes embeddings\\nand retrieves texts\n",
      "[E2E Tests] --> [Client] : Tests frontend\n",
      "[Server] --> ExternalAPIs : Makes external API calls\n",
      "\n",
      "note right of Client\n",
      "Runs on port 5173\n",
      "end note\n",
      "\n",
      "@enduml...\n",
      "\n",
      "File: ./App/README.md\n",
      "Content: - **Electricity Price Viewer**: View current and historical electricity prices in Finland.\n",
      "- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\n",
      "- **FastAPI Backend**: A Python-based backend for handling API requests and business logic.\n",
      "- **PostgreSQL Database**: A robust database for storing electricity price data.\n",
      "- **Flyway Migrations**: Manage database schema changes with ease.\n",
      "- **Testing**: End-to-end tests with Playwright and backend API tests with Pytest.\n",
      "- **Chat Engine**: A chat-based interface for interacting with the app.\n",
      "- **Data Loading**: Load and update electricity price data into the database using scripts in a dedicated container.  \n",
      "---...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_by_vector(query_embedding, k=5)\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"File: {doc.metadata['file']}\")\n",
    "    if \"Heading\" in doc.metadata:\n",
    "        print(f\"Heading: {doc.metadata['heading']}\")\n",
    "    print(f\"Content: {doc.page_content}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21b98909",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})  # retrieve top 20 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54c461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamingHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)  # or handle token as you want\n",
    "\n",
    "class LLMReranker(BaseDocumentCompressor):\n",
    "    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents\")\n",
    "    document_variable_name: str = \"document\"\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        *,\n",
    "        callbacks: Optional[list] = None,\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        scored_docs = []\n",
    "        for doc in documents:\n",
    "            inputs = {\n",
    "                \"query\": query,\n",
    "                self.document_variable_name: doc.page_content,\n",
    "            }\n",
    "            output = self.llm_chain.run(inputs)\n",
    "            try:\n",
    "                score = int(output.strip())\n",
    "            except Exception:\n",
    "                score = 0\n",
    "            scored_docs.append((doc, score))\n",
    "        scored_docs.sort(key=lambda x: x[[1]], reverse=True)\n",
    "        return [doc for doc, score in scored_docs]\n",
    "    \n",
    "class LLMRerankerBatched(BaseDocumentCompressor):\n",
    "    \"\"\" LLM Reranker that uses LLMChain to rerank documents in batches.\n",
    "    It passes the documents to the LLM in a single call and expects the LLM to return a list of scores.\n",
    "    \"\"\"\n",
    "    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents\")\n",
    "    document_variable_name: str = \"documents\"\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        *,\n",
    "        callbacks: Optional[list] = None,\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        scored_docs = []\n",
    "        \n",
    "        inputs = {\n",
    "            \"query\": query,\n",
    "            self.document_variable_name: [doc.page_content for doc in documents],\n",
    "        }\n",
    "        output = self.llm_chain.run(inputs)\n",
    "        try:\n",
    "            scores = [int(score.strip()) for score in output.split(\",\")]\n",
    "        except Exception:\n",
    "            scores = [0] * len(documents)\n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, score in scored_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bb5e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI chat model -- this is used for reranking\n",
    "llm_reranker = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define a template for reranking\n",
    "rerank_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"document\"],\n",
    "    template=(\n",
    "        \"Given the query:\\n{query}\\n\\n\"\n",
    "        \"Rate the relevance of the following document to the query on a scale from 1 to 10:\\n\"\n",
    "        \"{document}\\n\\n\"\n",
    "        \"Only output the score as an integer.\"\n",
    "    ),\n",
    ")\n",
    "rerank_batch_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"documents\"],\n",
    "    template=(\n",
    "        \"Given the query:\\n{query}\\n\\n\"\n",
    "        \"Rate the relevance of the following documents to the query on a scale from 1 to 10:\\n\"\n",
    "        \"{documents}\\n\\n\"\n",
    "        \"Only output the scores as a list of integers.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm_streaming = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callbacks=[MyStreamingHandler()],\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f4f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_537358/2086793062.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  rerank_chain = LLMChain(llm=llm_reranker, prompt=rerank_batch_prompt)\n"
     ]
    }
   ],
   "source": [
    "# Create an LLMChain for reranking\n",
    "#rerank_chain = LLMChain(llm=llm_reranker, prompt=rerank_prompt)\n",
    "# Instantiate your reranker\n",
    "#reranker = LLMReranker(llm_chain=rerank_chain)\n",
    "\n",
    "rerank_chain = LLMChain(llm=llm_reranker, prompt=rerank_batch_prompt)\n",
    "reranker = LLMRerankerBatched(llm_chain=rerank_chain)\n",
    "\n",
    "# Wrap your base retriever with ContextualCompressionRetriever using the reranker\n",
    "reranking_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=reranker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a88e773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert assistant. Use the following project documents to answer the user's question.\"\n",
    "    \"If the answer is in the documents, provide it and reference the document(s) used.\"\n",
    "    \"If the answer is not in the documents, provide a general answer based on your knowledge,\"\n",
    "    \"and state that the documents do not contain the answer.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    ")\n",
    "def full_chain(query: str):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "    response = llm.invoke(prompt_template.format(context=context, question=query))\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "884ab624",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert assistant. Use the following project documents to answer the user's question.\"\n",
    "    \"If the answer is in the documents, provide it and reference the document(s) used.\"\n",
    "    \"If the answer is not in the documents, provide a general answer based on your knowledge,\"\n",
    "    \"and state that the documents do not contain the answer.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    ")\n",
    "async def full_chain_stream(query: str):\n",
    "    # Embed query and retrieve reranked docs as before\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "\n",
    "    # Prepare prompt input\n",
    "    prompt_text = prompt_template.format(context=context, question=query)\n",
    "\n",
    "    # Stream tokens asynchronously from the LLM\n",
    "    async for token in llm.astream(prompt_text):\n",
    "        yield token.content  # yield each token as it arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b311ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = InMemoryChatMessageHistory()\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an expert assistant. Use the following project documents to answer the user's question. \"\n",
    "                \"If the answer is in the documents, provide it and reference the document(s) used. \"\n",
    "                \"If the answer is not in the documents, provide a general answer based on your knowledge, \"\n",
    "                \"and state that the documents do not contain the answer.\"\n",
    "            )\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "def full_chain_with_history(query: str):\n",
    "    # Embed and retrieve documents as before\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "\n",
    "    # Add the context as a system message or part of the prompt\n",
    "    # Here we prepend context as a system message for clarity\n",
    "    system_msg = SystemMessage(content=f\"Context:\\n{context}\")\n",
    "\n",
    "    # Get current history messages and append the system context message\n",
    "    history_msgs = message_history.messages + [system_msg]\n",
    "\n",
    "    # Add the current user query as a HumanMessage\n",
    "    user_msg = HumanMessage(content=query)\n",
    "\n",
    "    # Prepare messages for the LLM: history + current user message\n",
    "    messages = history_msgs + [user_msg]\n",
    "\n",
    "    # Invoke the LLM with the messages\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Add user and AI messages to history\n",
    "    message_history.add_user_message(query)\n",
    "    message_history.add_ai_message(response.content)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "async def full_chain_with_history_stream(query: str):\n",
    "    # Embed and retrieve documents as before\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "\n",
    "    # Add the context as a system message or part of the prompt\n",
    "    system_msg = SystemMessage(content=f\"Context:\\n{context}\")\n",
    "\n",
    "    # Get current history messages and append the system context message\n",
    "    history_msgs = message_history.messages + [system_msg]\n",
    "\n",
    "    # Add the current user query as a HumanMessage\n",
    "    user_msg = HumanMessage(content=query)\n",
    "\n",
    "    # Prepare messages for the LLM: history + current user message\n",
    "    messages = history_msgs + [user_msg]\n",
    "\n",
    "    # Stream tokens asynchronously from the LLM\n",
    "    async for token in llm.astream(messages):\n",
    "        yield token.content  # yield each token as it arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21fdf729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The app, known as the **Eprice App**, is designed for viewing current and historical electricity prices in Finland. Here are the key components and features:\\n\\n- **Electricity Price Viewer**: Users can view both current and historical electricity prices.\\n- **Svelte Frontend**: The user interface is built using Svelte and Vite, providing a modern and responsive design.\\n- **FastAPI Backend**: The backend is developed with Python using FastAPI, which handles API requests and business logic.\\n- **PostgreSQL Database**: A PostgreSQL database is utilized for storing electricity price data.\\n- **Flyway Migrations**: The app manages database schema changes efficiently using Flyway migrations.\\n- **Testing**: It includes end-to-end testing with Playwright and backend API testing with Pytest.\\n- **Chat Engine**: There is a chat-based interface for interacting with the app.\\n- **Data Loading**: The app features a mechanism for loading and updating electricity price data into the database using scripts in a dedicated container.\\n\\nThis information is sourced from the `./App/README.md` document.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain(\"Can you describe the app?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94213df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The App is called the **Eprice App** and serves as an **Electricity Price Viewer** that allows users to view current and historical electricity prices in Finland. It features a modern, responsive user interface built with **Svelte** and **Vite** for the frontend, while the backend is powered by **FastAPI**, which handles API requests and business logic. \n",
      "\n",
      "The app utilizes a **PostgreSQL Database** for storing electricity price data and employs **Flyway Migrations** to manage database schema changes efficiently. Testing is conducted using **end-to-end tests** with **Playwright** and backend API tests with **Pytest**. Additionally, the app includes a **Chat Engine**, providing a chat-based interface for user interaction. Data loading and updating of electricity price data into the database are managed through scripts in a dedicated container.\n",
      "\n",
      "This information can be found in the **App README.md** file."
     ]
    }
   ],
   "source": [
    "response = \"\"\n",
    "async for resp in full_chain_stream(\"Can you describe the App?\"):\n",
    "    response += resp\n",
    "    print(resp, end=\"\", flush=True)  # Print each token as it arrives    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "133705fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of the App, as outlined in the project documents, includes several functionalities:\\n\\n1. It can be used independently to retrieve or update data, saving data to a location that is available to migrations and the database.\\n2. It runs end-to-end tests for the system.\\n3. It handles database schema migrations.\\n4. It stores application data, including user information and embeddings for retrieval.\\n\\nThese purposes are detailed in the document titled \"Application Overview\" found in the project description.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5256a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_project_directory_structure_tool(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the project directory structure as a string.\n",
    "    The input argument is ignored.\n",
    "    \n",
    "    Returns:\n",
    "        str: The directory structure as a formatted string.\n",
    "    \"\"\"\n",
    "    # Get the project directory\n",
    "    fname = \"./data/project_structure.txt\"\n",
    "    # Run the command to get the directory structure\n",
    "    with open(fname, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    # Format the output\n",
    "    output = \"\"\"\n",
    "    Project Directory Structure:\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        output += f\"{line.strip()}\\n\"\n",
    "    return output\n",
    "@tool\n",
    "def get_code_by_file_name_tool(file_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve code entries from the database by file name.\n",
    "    Args:\n",
    "        file_name (str): The full path of the file to search for.\n",
    "    Returns:\n",
    "        str: The code entries as a formatted string.\n",
    "    \"\"\"\n",
    "    # check if the filename doesn't start with ./\n",
    "    # if it does not, add it\n",
    "    if not file_name.startswith(\"./\"):\n",
    "        file_name = \"./\" + file_name\n",
    "    results = get_code_by_file_name(file_name)\n",
    "    if not results:\n",
    "        return f\"No code found for file: {file_name}\"\n",
    "    # Format results for LLM output\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"File: {r['file']}\\nType: {r['type']}\\nName: {r['name']}\\nStart line: {r['start_line']}\\nDocstring: {r['docstring']}\\nCode:\\n{r['code']}\"\n",
    "        for r in results\n",
    "    )\n",
    "\n",
    "\n",
    "project_structure = get_project_directory_structure_tool(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b18b9697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File: ./Documents/openapi_endpoint_descriptions.md\\nType: \\nName: \\nStart line: 0\\nDocstring: not available\\nCode:\\n# not available'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_code_by_file_name_tool(\"./Documents/openapi_endpoint_descriptions.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35afa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_code_by_file_name_tool]\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    retriever=retriever,\n",
    "    agent_kwargs={\"prefix\": \"You are an expert assistant. Use the retriever to get relevant project documents to answer the user's question. \"\n",
    "                            \"If the answer is in the retrieved context, provide it and reference the document(s) used. \"\n",
    "                            \"If the answer is not in the documents, you use the tools to get more information about the project. \"\n",
    "                            \"The project directory structure and files can be retrieved using a tool, and every document can be retrieved by full file name. \"\n",
    "                            \"If the answer is not in the documents, try tools. If the answer is not in the documents, indicate that the documents do not contain the answer. \"\n",
    "                            f\"\\n\\nThis is the project directory structure:\\n{project_structure}\\n\\n\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7eab07ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to retrieve the content of the file `openapi_endpoint_descriptions.md` to answer the question about its contents. \n",
      "\n",
      "Action: get_code_by_file_name_tool  \n",
      "Action Input: ./Documents/openapi_endpoint_descriptions.md  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFile: ./Documents/openapi_endpoint_descriptions.md\n",
      "Type: \n",
      "Name: \n",
      "Start line: 0\n",
      "Docstring: not available\n",
      "Code:\n",
      "# not available\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe file `openapi_endpoint_descriptions.md` does not contain any available content. It appears to be empty or not properly formatted. \n",
      "\n",
      "Final Answer: The file `openapi_endpoint_descriptions.md` is empty or does not contain any content.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The file `openapi_endpoint_descriptions.md` is empty or does not contain any content.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"what is in the ./Documents/openapi_endpoint_descriptions.md?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f184a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
