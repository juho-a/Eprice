{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "3e0d5c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parse_code import *\n",
    "from parse_files import *\n",
    "from print_contents import *\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "from langchain.chains.base import Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "\n",
    "from pydantic import Field\n",
    "from typing import List, Optional, Sequence\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "#torch.cuda.is_available()\n",
    "dotenv.load_dotenv(\".env.private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "9123f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some helper functions\n",
    "# count tokens in the results\n",
    "def count_tokens(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def count_all_tokens(texts, tokenizer):\n",
    "    total_tokens = 0\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "def join_metadata(metadata):\n",
    "    key, value = list(metadata.items())[0]\n",
    "    return f\"{key}: {value}\"\n",
    "\n",
    "def run_command(command):\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    assert result.returncode == 0, f\"Command '{command}' failed with error: {result.stderr}\"\n",
    "\n",
    "def format_document(doc):\n",
    "    # Join all metadata key-value pairs as \"key: value\"\n",
    "    meta_str = \"\\n\".join(f\"{k}: {v}\" for k, v in doc.metadata.items())\n",
    "    # Combine metadata and content for the prompt\n",
    "    return f\"{meta_str}\\n{doc.page_content}\"\n",
    "def format_documents(docs):\n",
    "    return \"\\n\\n\".join(format_document(doc) for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22556c5f",
   "metadata": {},
   "source": [
    "### Parsing the source material for retrieval\n",
    "\n",
    "These are for parsing the source code and the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce85555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system calls -- I wrote some cmdline scripts to do the parsing\n",
    "call_1 = \"uv run parse_code.py ../App/python-server/ --replace-source ../App --replace-target ./App -o ./data/backend_code.txt\"\n",
    "call_2 = \"uv run print_contents.py -d ../ -r --exclude-dirs exclude_dirs.txt --exclude-files exclude_files.txt -o ./data/project_structure.txt\"\n",
    "call_3 = \"uv run parse_files.py --exclude-dirs exclude_dirs.txt --exclude-files exclude_files.txt -o ./data/project_files.txt ../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37d294d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command(call_1)\n",
    "run_command(call_2)\n",
    "run_command(call_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d0165",
   "metadata": {},
   "source": [
    "Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d9eb149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires that raw_project_documents.json already exists\n",
    "with open(\"./data/raw_project_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    file_dict = json.load(f)\n",
    "\n",
    "documents_json = []\n",
    "for file_path, content in file_dict.items():\n",
    "    doc_type = \"markdown document\" if file_path.endswith(\".md\") else \"document\"\n",
    "    documents_json.append({\n",
    "        \"file\": file_path,\n",
    "        \"type\": doc_type,\n",
    "        \"content\": content\n",
    "    })\n",
    "\n",
    "with open(\"./data/project_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents_json, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "612dd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents\n",
    "with open(\"./data/project_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    documents_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f3b7b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to documents and markdown documents\n",
    "documents_dicts = []\n",
    "markdown_documents_dicts = []\n",
    "for doc in documents_json:\n",
    "    if doc[\"type\"] == \"document\":\n",
    "        documents_dicts.append(doc)\n",
    "    elif doc[\"type\"] == \"markdown document\":\n",
    "        markdown_documents_dicts.append(doc)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown document type: {doc['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "c1c38b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/backend_code.txt', 'r', encoding='utf-8') as f:\n",
    "    code_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "65c39c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_code_entry(entry):\n",
    "    meta = [\n",
    "        f\"file: {entry.get('file', '')}\",\n",
    "        f\"type: {entry.get('type', '')}\",\n",
    "        f\"name: {entry.get('name', '')}\",\n",
    "        f\"start_line: {entry.get('start_line', '')}\"\n",
    "    ]\n",
    "    docstring = entry.get('docstring', '')\n",
    "    code = entry.get('code', '')\n",
    "    meta_str = \", \".join(meta)\n",
    "    docstring_str = f\"\\nDocstring:\\n{docstring}\" if docstring else \"\"\n",
    "    return f\"{meta_str}\\n{docstring_str}\\ncontent:\\n{code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6865ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_code_entries = [format_code_entry(entry) for entry in code_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9e3c1c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 12)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_dicts), len(markdown_documents_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "04e58b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "375d7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's maximum sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's maximum sequence length: {SentenceTransformer('BAAI/bge-large-en-v1.5').max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2f23281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding dimensionality: 1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's embedding dimensionality: {len(bge_embeddings.embed_query('some random query'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d4d2e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which headers to split on and their metadata keys\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Heading 1\"),\n",
    "    (\"##\", \"Sub heading\"),\n",
    "    (\"###\", \"Sub-sub heading\"),\n",
    "]\n",
    "\n",
    "# Initialize the Markdown splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "00126042",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "12e0f0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': './App/README.md',\n",
       " 'type': 'markdown document',\n",
       " 'content': '# Eprice App\\n\\nThe Eprice App is a containerized application that allows users to view the market price of electricity in Finland, both current and historical. The app is built with a modern tech stack, including a Svelte frontend, a FastAPI backend, a PostgreSQL database, and various tools for testing and data management.\\n\\n## Features\\n\\n- **Electricity Price Viewer**: View current and historical electricity prices in Finland.\\n- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\\n- **FastAPI Backend**: A Python-based backend for handling API requests and business logic.\\n- **PostgreSQL Database**: A robust database for storing electricity price data.\\n- **Flyway Migrations**: Manage database schema changes with ease.\\n- **Testing**: End-to-end tests with Playwright and backend API tests with Pytest.\\n- **Chat Engine**: A chat-based interface for interacting with the app.\\n- **Data Loading**: Load and update electricity price data into the database using scripts in a dedicated container.\\n\\n---\\n\\n## Project Structure\\n.\\n\\n ├── README.md # Root README file\\n\\n ├── compose.yaml # Docker Compose configuration \\n \\n ├── chat-engine/ # Chat engine service \\n \\n ├── client/ # Svelte frontend service\\n \\n ├── data-preparation/ # Scripts and data for populating the database\\n \\n ├── database-migrations/ # Flyway migration scripts \\n \\n ├── e2e-tests/ # Playwright end-to-end tests\\n \\n ├── python-server/ # FastAPI backend service\\n \\n └── project.env # Environment variables for the project\\n\\n\\n---\\n\\n## Getting Started\\n\\n### Prerequisites\\n\\n- **Docker** and **Docker Compose**: Install Docker Desktop or Docker CLI.\\n- **Deno**: Required for local development of the frontend (see `client/README.md`).\\n\\n---\\n\\n### Running the App\\n\\n1. Clone the repository:\\n    ```bash\\n    git clone <repository-url>\\n    cd Eprice\\n    ```\\n\\n2. Build and start the containers:\\n    ```bash\\n    docker compose up --build\\n    ```\\n\\n3. Access the services:\\n\\n* Frontend: http://localhost:5173\\n* Backend: http://localhost:8000\\n\\n4. To stop the containers:\\n    ```bash\\n    docker compose down\\n    ```\\n\\n### Testing\\n\\n\\n1. Run Playwright tests:\\n    ```bash\\n    docker compose run --rm --entrypoint=npx e2e-tests playwright test\\n    ```\\n\\n2. Run Pytest for backend API (using uv inside the container):\\n    ```bash\\n    docker compose run backend-tests [uv run pytest]\\n    ```\\n\\n### Environment Variables\\n\\n* Use `.env.local` for local development (gitignored)\\n\\n* Use `project.env` for containerrized development\\n\\n* To inspect environment variables inside a container:\\n    ```bash\\n    docker exec -it <container_name> bash\\n    printenv\\n    ```\\n\\n### Services overview\\n\\n1. Frontend (Client)\\nBuilt with Svelte and Vite.\\nLocated in the client/ directory.\\nSee client/README.md for more details.\\n\\n2. Backend (Python Server)\\nBuilt with FastAPI.\\nLocated in the python-server/ directory.\\nSee python-server/README.md for more details.\\n\\n3. Database\\nPostgreSQL database for storing electricity price data.\\nFlyway is used for managing schema migrations (database-migrations/).\\n\\n4. Chat Engine\\nA chat-based interface for interacting with the app.\\nLocated in the chat-engine/ directory.\\n\\n5. Data Preparation\\nScripts for loading and updating electricity price data.\\nLocated in the data-preparation/ directory.\\n\\n### Contributing\\n\\n1. Fork the repository.\\n\\n2. Create a feature branch:\\n\\n    ```bash\\n    git checkout -b feature-name\\n    ```\\n\\n3. Commit your changes:\\n\\n    ```\\n    git commit -m \"Add feature-name\"\\n    ```\\n\\n4. Push to your branch:\\n\\n    ```bash\\n    git push origin feature-name\\n    ```\\n\\n5. Open pull request.\\n\\n\\n## License\\n\\nThis project us under MIT license: https://mit-license.org/\\n\\n## Acknowledgments\\n\\nElectricity price data is sourced from Pörssisähkö API.\\n\\n## Additional notes\\n\\n**Install docker and docker compose**. Maybe easiest to just install docker desktop, especially on windows.\\n\\n**Postgres is not needed on local machine** (unless you want to run outside containers).\\n\\nThe compose.yaml and the individual Dockerfiles are sufficient to run the App. Docker does the installing for the containers. But you can still run `deno install --allow-scripts`, if you want to run on local host. On windows, after running deno install, `node_modules/` that are loaded into client should not be copied into the container -- the container is using arch-linux as base image. You can either remove those, or add your own `.dockerignore` file.\\n\\nRun using docker compose:\\n\\n`docker compose up --build` (no need to build everytime)\\n\\nYou can also simply `ctrl+C` to shut down the containers, or\\n\\n`docker compose down` to tear down.\\n\\n\\n### **About environment variables**\\n\\nKeep private information private, preferably :-). You can use `.env.local` convention, and keep them gitignored. For public api keys, while developing, we can all get our own api keys.\\n\\n**If you are unsure what environment variables are loaded on your container launch -- either by docker from project.env, or by services using other tools, like dotenv -- you can always go inside the container to check:**\\n\\n```\\ndocker compose up -d <service_name> # launch the container\\n\\ndocker exec -it <container_name> bash # go into cmdline inside\\n\\n(container): printenv # or echo etc.\\n```\\n\\n**For developing client:** There is a sort of a bug in the denolands alpine image, which prevents us from installing with optional flags -- in our case `deno install --allow-scripts`. This means that the node modules need to be copied from local. This is not an issue if you are using linux (or wsl2 on Windows). Later, there might be a change in the client\\'s base image later on to fix this issue.\\n\\nAnd note that the container names are not necessarily same as the service name (they are derived from it though); you can check running cont\\'s with `docker <container> ps`.\\n\\n### Running without Docker\\n\\nCan be done, but needlessly cumbersome. Ask Paavo for the how.\\n\\n\\n### Starting a new client build from scratch\\n\\nIf you want to start the client build from scratch, for example with typescript checking enabled, run:\\n\\n```bash\\ndeno run -A npm:sv@latest create client\\n```\\n\\nfrom the root directory, and choose from the given options. For this project, we have used the most minimal build setup (SveleteKit minimal, no TS typechecking, nothing added, with deno itself for dependency management).\\n'}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_documents_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0585064c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': './App/python-server/requirements.txt',\n",
       " 'type': 'document',\n",
       " 'content': 'fastapi==0.115.12\\nasyncpg==0.30.0\\npasslib[bcrypt]==1.7.4\\npython-jose==3.4.0\\nuvicorn==0.34.2\\npydantic[email]==2.11.4\\ndotenv==0.9.9\\nhttpx==0.28.1\\nasyncio==3.4.3\\napscheduler==3.11.0\\nrequests==2.32.3\\nfastapi-mail==1.4.2\\n'}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "10544aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "# Create the embeddings object\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b778dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = documents_dicts + markdown_documents_dicts\n",
    "data = []\n",
    "for doc in all_documents:\n",
    "    if doc[\"type\"] == \"document\":\n",
    "        chunks = text_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            embedding = embedding_model.embed_query(chunk)\n",
    "            data.append({\n",
    "                \"file\": doc[\"file\"],\n",
    "                \"type\": doc[\"type\"],\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": None,\n",
    "                \"embedding\": embedding\n",
    "            })\n",
    "    elif doc[\"type\"] == \"markdown document\":\n",
    "        chunks = markdown_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            subchunks = text_splitter.split_text(chunk.page_content)\n",
    "            for subchunk in subchunks:\n",
    "                embedding = embedding_model.embed_query(subchunk)\n",
    "                data.append({\n",
    "                    \"file\": doc[\"file\"],\n",
    "                    \"type\": doc[\"type\"],\n",
    "                    \"content\": subchunk,\n",
    "                    \"metadata\": join_metadata(chunk.metadata),\n",
    "                    \"embedding\": embedding_model.embed_query(subchunk)\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e02413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and embed code files, matching doc structure\n",
    "code_chunks = []\n",
    "for entry in code_data:\n",
    "    # Combine docstring and code for context, or just use code\n",
    "    docstring = entry.get(\"docstring\", \"\")\n",
    "    code_text = entry.get(\"code\", \"\")\n",
    "    full_text = f\"{docstring}\\n{code_text}\" if docstring else code_text\n",
    "\n",
    "    # Chunk the code\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    for chunk in chunks:\n",
    "        # Prepare metadata: include all keys except file, type, code, docstring, and start_line\n",
    "        metadata = {k: v for k, v in entry.items() if k not in [\"file\", \"type\", \"code\", \"docstring\", \"start_line\"]}\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"file\": entry.get(\"file\", \"\"),\n",
    "                \"type\": entry.get(\"type\", \"\"),\n",
    "                \"metadata\": metadata if metadata else None\n",
    "            }\n",
    "        )\n",
    "        embedding = embedding_model.embed_query(chunk)\n",
    "        code_chunks.append((doc, embedding))\n",
    "\n",
    "# Unpack for later use\n",
    "code_documents = [doc for doc, _ in code_chunks]\n",
    "code_embeddings_list = [np.array(emb, dtype=np.float32) for _, emb in code_chunks]\n",
    "code_embeddings_matrix = np.vstack(code_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "55216e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Documents\n",
    "documents = []\n",
    "embeddings_list = []\n",
    "for item in data:\n",
    "    doc = Document(\n",
    "        page_content=item['content'],\n",
    "        metadata={\n",
    "            'file': item['file'],\n",
    "            'type': item['type'],\n",
    "            'heading': item['metadata'],\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    embeddings_list.append(np.array(item['embedding'], dtype=np.float32))\n",
    "\n",
    "embeddings_matrix = np.vstack(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ac83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine code and other documents\n",
    "documents.extend(code_documents)\n",
    "embeddings_matrix = np.vstack((embeddings_matrix, code_embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "7bc94858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "dimension = 1024#embeddings_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # or another index type\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings_matrix)\n",
    "\n",
    "# Create docstore dict mapping index IDs to Document objects\n",
    "# Create InMemoryDocstore wrapping your documents dict\n",
    "docstore = InMemoryDocstore({i: doc for i, doc in enumerate(documents)})\n",
    "index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "\n",
    "# Correct FAISS vector store initialization\n",
    "vector_store = FAISS(\n",
    "    embedding_function=None,  # embeddings already computed\n",
    "    index=index,              # FAISS index object here\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql+psycopg://username:password@localhost:5432/database\"\n",
    "collection_name = \"document_collection\"\n",
    "vector_store = PGVector(\n",
    "    embeddings=None,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection_string,\n",
    "    use_jsonb=True,  # optional, for metadata storage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964d870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['64937226-3a4a-4c6d-bc66-6954d4691630',\n",
       " 'abd1bcbd-a5bb-4a8a-be1d-165c84458e59',\n",
       " 'a2f8fd40-146d-4405-a8b7-1992dd3bf339',\n",
       " 'ecad18ff-23d9-47a0-8f27-7fda01fa0327',\n",
       " 'b7cbe391-3fe8-486e-a5a8-22f0531eae3f',\n",
       " '1f3d045a-705a-4572-9e1e-e7e136fc513b',\n",
       " '281a3a14-77b0-40b3-a3da-b5a312e06677',\n",
       " 'd9953411-133f-4a5f-aedc-bee5d7cec853',\n",
       " 'a47dd161-8dde-41c9-8ff7-736925d5cf40',\n",
       " '2c1c9d8a-6535-4bbb-8f43-d28bff7b530a',\n",
       " '5840af5b-b2cf-4a40-be44-91b4e2e810bd',\n",
       " 'b585513c-c515-4bb4-9d88-2df5a1404e01',\n",
       " 'e226328c-f5fa-4e10-8a65-ac294a964020',\n",
       " '0b97a414-cf46-4d7b-a874-d4639c096681',\n",
       " 'b2dc8a08-2585-40fc-be6e-50243bf9e991',\n",
       " 'fe085651-51a1-4d01-942c-c79460418409',\n",
       " '01f5b98a-fec9-4e60-b1a9-2e7a24b0dd05',\n",
       " '40a25806-4bd6-4dab-954a-26d7f2e2ddb3',\n",
       " '14964b9b-7bfb-4e21-a839-368ba44cff51',\n",
       " '2aa0ced8-5635-4a17-9dfb-6758e887ec6e',\n",
       " 'a062f75b-8651-4d72-95df-62e47bf5cacf',\n",
       " '7e9880cd-9e5c-460b-97bb-0344b0da2bd7',\n",
       " '0f436ce7-d891-4cc7-8269-b1bf7d810842',\n",
       " '0ae2d888-a678-4d75-9503-f196088a527a',\n",
       " 'b283ce30-a78f-4b58-8d20-22e062c1c649',\n",
       " '211c9802-5a35-431e-a55e-a08266bc3e46',\n",
       " '594332ba-81ec-41ed-a8b0-91932ccd894a',\n",
       " '84929992-c1b1-467b-8e64-a275dbbfc18a',\n",
       " '2e42a86a-92f5-421c-b3b0-a1c085ea2f16',\n",
       " 'df87ff3a-dc50-4a4f-a883-ad4d4e8b68db',\n",
       " 'bd26f5a3-ec41-4f82-8545-171c1f661dd5',\n",
       " '7f1539af-239c-4dcc-a58e-44f95dda79a6',\n",
       " '19b72bf8-0534-446a-bc3e-aa81f02fd32a',\n",
       " 'bc409fec-0ea3-45ad-b1e3-27fbb2cf284b',\n",
       " 'b8500d36-fb68-47dc-b3c6-438b076bd840',\n",
       " 'b7a39d66-8c26-465d-a0ed-b7c2932b41be',\n",
       " '60415d7e-4f28-4888-bfae-8489855a50c3',\n",
       " 'd25a9cd5-38c8-49a9-bea6-7c4c58e202a8',\n",
       " 'd5189bc2-1f23-4ebc-bd0e-377cf639237c',\n",
       " 'b8b2503f-ad24-4990-a5b6-064acae0f2f4',\n",
       " '6c4ac2da-1c8a-4d65-919b-cd797d654bfe',\n",
       " '02daec0d-915d-4708-b34c-901470da4d28',\n",
       " 'd0730cbd-b922-4f5b-a79b-79b004bd5dc6',\n",
       " 'cafbdd6d-0dec-40fb-8657-3cfa8e60a44c',\n",
       " 'b1d24c01-005a-46d1-8a60-d97dbd0ae394',\n",
       " 'e7185712-2dd5-4826-8abc-02b6e4e0c917',\n",
       " '50b98d7c-f0c6-4d20-98ee-42a330d486bf',\n",
       " 'ee8eb88b-b9ea-4303-920c-3b620bb5c5a7',\n",
       " 'b106c46d-ef41-4a79-bdbb-5d7d0ac7923e',\n",
       " 'e8f38d26-6758-41d2-8ace-ce36c3da2070',\n",
       " '8dd225c5-02f5-4522-94af-f030dc971ffa',\n",
       " '736730bb-d36f-401b-9d4a-d3ee5a4aa215',\n",
       " 'cdb95324-dad3-44fc-b2b9-2e6bb735c965',\n",
       " '025b8c0b-1a03-4966-8d4c-79b1d7494087',\n",
       " '58d2e8f6-3bda-4d34-abe3-e61c774cef5d',\n",
       " '22302fcf-1318-45a3-993c-cefa21455f9f',\n",
       " '70c95687-40c0-4c29-ae1b-ac40b88bc9fe',\n",
       " '9f69b5d6-0115-4af2-9cdd-a30363e823ea',\n",
       " 'd3610116-a845-47f7-ac84-1e5f2c32b15d',\n",
       " '7aa9d37d-35c1-4ef1-8859-9734fe9ae4f5',\n",
       " '680f3b21-1f56-46e3-8407-2dbf68f8fd7b',\n",
       " '8d0558b1-1681-48bd-9314-9ad19529fd3c',\n",
       " '8fce76de-1d5c-4cf7-a893-77f825a1abd8',\n",
       " 'ea04bb59-3c5b-4b26-b0fe-d8b5293e0913',\n",
       " 'f55f6d4a-91bb-459a-aee0-d0d8f693ccc6',\n",
       " 'e6e35d41-7ea4-49e6-97c2-1e1c41a5a003',\n",
       " '80f44f98-cae1-4e5c-a907-66ce81596f35',\n",
       " 'f2b903fe-d232-4084-bfd2-51712eb5b6bd',\n",
       " '40aa050e-cd49-4151-b44a-c217a828043d',\n",
       " '18b84b32-9cae-4b52-af73-8bae686c0e75',\n",
       " '67e4ed99-a373-4a75-8566-cf0b65626071',\n",
       " 'c5d0b13e-1439-47fd-ad50-c665851cbe87',\n",
       " '8f8786b2-97dc-4ef5-86c2-5e1c784bf50b',\n",
       " '05c91e0b-900d-4dbd-9258-e376deb4111b',\n",
       " '46a30f3a-3467-4010-909d-ff7b8b9fb2cf',\n",
       " 'eb857b30-646b-4a3e-befc-70d56567dafd',\n",
       " 'c139c60d-64cb-4b6f-874d-e09049c51d49',\n",
       " 'c4a7cf9e-7071-40e6-b97e-0c0bc4a2618d',\n",
       " '673fc694-cfb7-4439-8059-81664ae137e3',\n",
       " '3bb585c6-a40a-467e-ba24-e1fd4d0610f1',\n",
       " 'c8226d9e-9486-4107-9907-35f4cd91ac2b',\n",
       " '7ed33ac0-c727-440d-8bee-99c95048a904',\n",
       " '3ab7ef03-107d-499a-b69a-bd832c225461',\n",
       " 'c33dfdfc-e0cf-4778-a91f-c6833aa17f6e',\n",
       " '5db1fa74-6cfa-44df-83b3-59d35d4777ed',\n",
       " 'c35020ee-af85-464f-beb7-05baa8de6722',\n",
       " '924c420a-dc25-4a1f-a0ba-8a403bb98610',\n",
       " '26e783b1-b5ea-43f4-af74-32ec5c2c057a',\n",
       " '3def485c-663e-445a-8661-2727474f783b',\n",
       " 'c818a2e1-53e2-4d7f-af8b-0a8dd2231b3a',\n",
       " '0a0513d0-66f6-4b0c-9bef-ca97689b12b7',\n",
       " 'a95b9859-fade-4f13-80ff-3c497f074a34',\n",
       " '2b80e92a-68b9-4e90-988f-355a5f6c2ecb',\n",
       " '1dfa37f8-40ba-41a6-8c23-d534152016c1',\n",
       " 'f1b64a34-8bad-4863-be11-5e895339775d',\n",
       " 'ddb766e0-0a62-412e-bd47-27a5b850b66f',\n",
       " '420bf986-c95a-4924-bc69-24b8f5e14181',\n",
       " '6c32d6f8-3a62-4b8f-9925-7beece7e4a25',\n",
       " '73b080f0-2a48-42f6-b030-17daa60c4b8b',\n",
       " '840b496c-fd28-4610-bd27-92ad1655e4a5',\n",
       " '4351968f-c546-407e-ae6c-ba15fc9866fe',\n",
       " 'ce121abd-6b3a-4069-893e-f4989e5ef919',\n",
       " '64334783-7ec0-40a2-84b3-f0755608a89d',\n",
       " 'a9582cd9-0907-4995-bcd2-b594ac8c3258',\n",
       " 'c9a47473-23bc-4feb-9c1f-a8a9b4531910',\n",
       " 'b5b9fd22-9b95-44b5-b407-ab1239ab86da',\n",
       " 'd93bed8d-da35-40ac-b707-cc378e099855',\n",
       " '3b76bdc5-1820-4dda-8342-fd54ed220c33',\n",
       " 'e3dbf244-f7fe-4eb4-9bfe-622e78068d42',\n",
       " '51dd30a3-d11a-4f69-9ece-d6c56fb4602b',\n",
       " 'da7400c5-ba72-4e07-accf-54a6b075eac1',\n",
       " '67afeb03-8bce-412e-b48c-47bd0299a7f9',\n",
       " '7bde193a-269e-4b35-b664-41863f0b5cc8',\n",
       " 'c86d346e-ec64-43c4-a3d8-cdf9e91585e4',\n",
       " '08fd4f37-f419-4887-9c21-89120e8a5395',\n",
       " '7b821475-936d-45b7-b4d0-fb4d5773eff7',\n",
       " 'bfa34152-2121-40be-9268-7ab0f9775b8d',\n",
       " '91f7adaf-6b46-4deb-aa7c-c12b56f30380',\n",
       " '765c0f43-9324-4e79-b200-f6e09898954b',\n",
       " '0c00d64a-cf08-48d5-97ae-bf8a3736c282',\n",
       " '2a94e52b-dc1f-4def-96bf-f1a46d41190e',\n",
       " 'c858ef59-84e3-4ff6-8798-765170b1cb24',\n",
       " 'c0711429-9d2b-49b7-bc30-f8b9ca0f9102',\n",
       " '79c40579-e8bf-4fd7-8354-e7dc1ff5e248',\n",
       " 'f4d1edc7-f71b-4a20-af7e-7e76f855095c',\n",
       " '9d824040-f32d-4138-b0c4-632236727eaa',\n",
       " '3fe92c91-712c-462f-897a-cb6b212f6833',\n",
       " '603c1ca2-8077-4b6d-8244-3c25beae80eb',\n",
       " 'f94a60dd-aa88-4546-9819-b39382124058',\n",
       " '14ecd5b5-1fe0-4a01-9f71-92b38ad46727',\n",
       " 'b82587e9-44ac-4be2-8664-1e9cea6bdcc2',\n",
       " 'd09a6ad2-2d8f-4865-ad46-d25b571e9be1',\n",
       " '79d6606d-7f7b-456d-92d7-52551775893e',\n",
       " '26bdebea-32a5-4a79-9963-761a39813430',\n",
       " 'b94681c2-e8a2-4633-965a-3d73cbbab053',\n",
       " 'a5c474cb-ffae-4dd9-8a4b-cdf9269b9978',\n",
       " '35898fa7-a97b-4567-ace8-9ec95459c0ed',\n",
       " 'b6eac3b8-cea5-4bce-8780-955da482ed45',\n",
       " 'b7bf5bf8-e0ca-4852-b83e-978b3a718dc9',\n",
       " '76a96b13-a02d-4367-be12-6060a0c0fe38',\n",
       " 'cd7aff94-d5e3-42ba-9b94-fb497ab65001',\n",
       " 'aa6ba5c2-02fe-4930-9158-4a76c16c335f',\n",
       " 'c8be52e5-fb94-4b69-903a-47708568129d',\n",
       " 'ded6cc1c-92aa-4221-b3ca-087f01601c09',\n",
       " '0c1ad6fb-ecb4-4a06-af8d-c3bf1cc89bca',\n",
       " '01ee95ba-1c18-445a-b896-32c7dc083ed2',\n",
       " 'f8389c0a-decf-4cb5-a817-0540a9cc69d3',\n",
       " '3193d847-57c0-49c1-987f-e8a31f4ac095',\n",
       " 'fb92c7c6-5a06-4b51-aeed-7667a94e244a',\n",
       " '00435db4-96a7-4e4f-8080-14aba6dc18da',\n",
       " 'db102822-4b2b-4867-852d-5de652f4f1b9',\n",
       " '52fb3e90-4c2e-4aee-8e55-9e357b3f6868',\n",
       " 'aba11fb2-3bce-48c2-95da-7f7074b37b0b',\n",
       " '9e18fefa-ab92-4515-a2ac-a9e23d655624',\n",
       " '0ecd8295-e2ed-41ff-baf8-68cbbda73ac6',\n",
       " '6580c298-f789-44c1-a80d-f597ea10cf4f',\n",
       " '8d557b8e-ad6d-400a-a4ab-7e78dbe4171f',\n",
       " '05d843b5-17dc-4427-bba8-86b9161d3a1a',\n",
       " '50f450a0-3c7a-4249-87a5-a02a804f0e9a',\n",
       " '099f4bed-48a2-432b-ab0e-2d177d40a670',\n",
       " 'f964c8b9-7a50-43f9-8b35-3195711a3f77',\n",
       " '9ac4b722-1b2f-4e56-a238-76851e7dfee5',\n",
       " 'cfc936da-8e8b-4f7e-a8f6-42169279326c',\n",
       " 'b4f33629-ebd9-4d26-a5dd-83109a986843',\n",
       " 'e9d4a518-3fdd-4f85-b54d-81306c0b87b0',\n",
       " 'f692feab-fabb-40d8-918f-9a5b4ac7d901',\n",
       " '3f3be3ac-48a8-4f4c-b88f-2caed9dde8b1',\n",
       " '51cf76b2-d672-46f9-947a-e3193cb20de7',\n",
       " '18ba7183-8f89-489d-9153-3d73cbf3734f',\n",
       " 'e0a3890c-7099-47be-8f7e-859de9108df8',\n",
       " '5d1ac31c-2772-4e18-b253-bcd6a9ea6046',\n",
       " '01aef37e-7698-4610-8539-25b128b7a6c4',\n",
       " 'e011537e-63a0-463c-9086-a65926014fba',\n",
       " 'c2697b5a-d3ac-4064-8f53-cb64a6540c27',\n",
       " '7ab2fd8e-e5ae-46df-9f35-23eee7a8d17c',\n",
       " 'ebf3d852-19a9-46d7-87ba-7e655174b37c',\n",
       " '15c12a1a-04a7-40f7-b1e1-e331afbc8f10',\n",
       " '4b58a192-2e12-4d70-8fc1-c61db4753e56',\n",
       " 'c38766d3-5b6f-41d7-b350-bf18c9429146',\n",
       " 'a4308428-d485-4125-b10c-cc4eeba7f03b',\n",
       " '71e1cc5f-a809-41d8-881f-3d01c4db5e97',\n",
       " '274429a9-8a07-4065-9120-2847992e0069']"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings_matrix.tolist(),  # convert to list for JSON serialization\n",
    "    metadata=[doc.metadata for doc in documents],\n",
    "    ids=[str(i) for i in range(len(documents))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b6c5fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you describe the project\"\n",
    "query_embedding = embedding_model.embed_query(query)  # get embedding for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "35eb50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ./App/README.md\n",
      "Heading: Heading 1: Eprice App\n",
      "Content: This project us under MIT license: https://mit-license.org/...\n",
      "\n",
      "File: ./Documents/diagrams/sources/use_case.wsd\n",
      "Heading: None\n",
      "Content: @startuml\n",
      "title Use Case Diagram for Electricity Market App\n",
      "\n",
      "actor \"Signed-in User\" as User\n",
      "actor \"Visitor\" as Visitor\n",
      "\n",
      "package \"Frontend (Svelte)\" {\n",
      "    usecase \"View Current Electricity Prices\" as V...\n",
      "\n",
      "File: ./Documents/diagrams/sources/authentication_use_case.wsd\n",
      "Heading: None\n",
      "Content: @startuml\n",
      "title Use Case Diagram for authentication in Electricity Market App\n",
      "\n",
      "actor \"Signed-in User\" as User\n",
      "actor \"Visitor\" as Visitor\n",
      "\n",
      "package \"Frontend (Svelte)\" {\n",
      "    usecase \"Register\" as Regist...\n",
      "\n",
      "File: ./Documents/project_description.md\n",
      "Heading: Heading 1: System description\n",
      "Content: This document describes the basic topology of the system (as defined in the compose.yaml file). The system consists of multiple services that work together to provide functionality, including a databa...\n",
      "\n",
      "File: ./App/README.md\n",
      "Heading: Heading 1: Eprice App\n",
      "Content: - **Electricity Price Viewer**: View current and historical electricity prices in Finland.\n",
      "- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\n",
      "- **FastAPI Backend**: A Python-ba...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_by_vector(query_embedding, k=5)\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"File: {doc.metadata['file']}\")\n",
    "    print(f\"Heading: {doc.metadata['heading']}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "21b98909",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})  # retrieve top 20 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "f54c461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMReranker(BaseDocumentCompressor):\n",
    "    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents\")\n",
    "    document_variable_name: str = \"document\"\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        *,\n",
    "        callbacks: Optional[list] = None,\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        scored_docs = []\n",
    "        for doc in documents:\n",
    "            inputs = {\n",
    "                \"query\": query,\n",
    "                self.document_variable_name: doc.page_content,\n",
    "            }\n",
    "            output = self.llm_chain.run(inputs)\n",
    "            try:\n",
    "                score = int(output.strip())\n",
    "            except Exception:\n",
    "                score = 0\n",
    "            scored_docs.append((doc, score))\n",
    "        scored_docs.sort(key=lambda x: x[[1]], reverse=True)\n",
    "        return [doc for doc, score in scored_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "7bb5e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI chat model -- this is used for reranking\n",
    "llm_reranker = ChatOpenAI(model=\"gpt-4o-nano\", temperature=0)\n",
    "\n",
    "# Define a template for reranking\n",
    "rerank_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"document\"],\n",
    "    template=(\n",
    "        \"Given the query:\\n{query}\\n\\n\"\n",
    "        \"Rate the relevance of the following document to the query on a scale from 1 to 10:\\n\"\n",
    "        \"{document}\\n\\n\"\n",
    "        \"Only output the score as an integer.\"\n",
    "    ),\n",
    ")\n",
    "class MyStreamingHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)  # or handle token as you want\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callbacks=[MyStreamingHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "b8f4f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain for reranking\n",
    "rerank_chain = LLMChain(llm=llm_reranker, prompt=rerank_prompt)\n",
    "# Instantiate your reranker\n",
    "reranker = LLMReranker(llm_chain=rerank_chain)\n",
    "\n",
    "# Wrap your base retriever with ContextualCompressionRetriever using the reranker\n",
    "reranking_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=reranker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert assistant. Use the following project documents to answer the user's question.\"\n",
    "    \"If the answer is in the documents, provide it and reference the document(s) used.\"\n",
    "    \"If the answer is not in the documents, provide a general answer based on your knowledge,\"\n",
    "    \"and state that the documents do not contain the answer.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    ")\n",
    "def full_chain(query: str):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "    response = llm.invoke(prompt_template.format(context=context, question=query))\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "884ab624",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def full_chain_stream(query: str):\n",
    "    # Embed query and retrieve reranked docs as before\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    reranked_docs = reranking_retriever.base_retriever.vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "    context = format_documents(reranked_docs)\n",
    "\n",
    "    # Prepare prompt input\n",
    "    prompt_text = prompt_template.format(context=context, question=query)\n",
    "\n",
    "    # Stream tokens asynchronously from the LLM\n",
    "    async for token in llm.astream(prompt_text):\n",
    "        yield token.content  # yield each token as it arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "94213df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptions in the backend are primarily handled at the **Controller** level. The controller is responsible for catching exceptions and returning valid JSON responses to the frontend, including appropriate error messages while ensuring that sensitive information is not leaked. This is part of the overall design pattern that emphasizes separation of concerns, where each layer has a single responsibility.\n",
      "\n",
      "This information is referenced from the document titled \"backend_design.md.\""
     ]
    }
   ],
   "source": [
    "resp = \"\"\n",
    "async for token in full_chain_stream(\"at what layer or level are exceptions handled in the backend?\"):\n",
    "    respo += token\n",
    "    print(resp, end=\"\", flush=True)  # Print each token as it arrives    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133705fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
