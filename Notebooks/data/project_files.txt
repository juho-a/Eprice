{
  "./App/README.md": "# Eprice App\n\nThe Eprice App is a containerized application that allows users to view the market price of electricity in Finland, both current and historical. The app is built with a modern tech stack, including a Svelte frontend, a FastAPI backend, a PostgreSQL database, and various tools for testing and data management.\n\n## Features\n\n- **Electricity Price Viewer**: View current and historical electricity prices in Finland.\n- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\n- **FastAPI Backend**: A Python-based backend for handling API requests and business logic.\n- **PostgreSQL Database**: A robust database for storing electricity price data.\n- **Flyway Migrations**: Manage database schema changes with ease.\n- **Testing**: End-to-end tests with Playwright and backend API tests with Pytest.\n- **Chat Engine**: A chat-based interface for interacting with the app.\n- **Data Loading**: Load and update electricity price data into the database using scripts in a dedicated container.\n\n---\n\n## Project Structure\n.\n\n ├── README.md # Root README file\n\n ├── compose.yaml # Docker Compose configuration \n \n ├── chat-engine/ # Chat engine service \n \n ├── client/ # Svelte frontend service\n \n ├── data-preparation/ # Scripts and data for populating the database\n \n ├── database-migrations/ # Flyway migration scripts \n \n ├── e2e-tests/ # Playwright end-to-end tests\n \n ├── python-server/ # FastAPI backend service\n \n └── project.env # Environment variables for the project\n\n\n---\n\n## Getting Started\n\n### Prerequisites\n\n- **Docker** and **Docker Compose**: Install Docker Desktop or Docker CLI.\n- **Deno**: Required for local development of the frontend (see `client/README.md`), and package management.\n\nAny libraries needed in the containers are installed automatically using requirements or other configuration files. `uv` is preferable package manager also inside containers, but pip is used as fallback in some cases.\n\n### Database Persistence\n\nTo ensure your PostgreSQL database data is persisted locally (so it is not lost when containers are stopped or removed), you need to create a directory for the database files and set the correct permissions:\n\n```bash\nmkdir -p ./pgdata\nchmod 700 ./pgdata\nsudo chown -R 999:999 ./pgdata\n```\n\n- `mkdir -p ./pgdata` creates the directory if it doesn't exist.\n- `chmod 700 ./pgdata` sets secure permissions (Postgres default).\n- `sudo chown -R 999:999 ./pgdata` sets ownership to the default Postgres user inside the container (UID 999).\n\n**Note:**  \nIf you change to a custom Postgres image or user, check the UID/GID with:\n\n```bash\ndocker run --rm <root/image:version> id postgres\n```\n(here we have root=reinikp2 and image=pgvector-database, and version=v1)\n\nand adjust the `chown` command accordingly.\n\n### Running the App\n\n1. Clone the repository:\n    ```bash\n    git clone <repository-url>\n    cd Eprice\n    ```\n\n2. Build and start the containers:\n    ```bash\n    docker compose up --build\n    ```\n\n3. Access the services:\n\n* Frontend: http://localhost:5173\n* Backend: http://localhost:8000\n\n4. To stop the containers:\n    ```bash\n    docker compose down\n    ```\n\n### Testing\n\n\n1. Run Playwright tests:\n    ```bash\n    docker compose run --rm --entrypoint=npx e2e-tests playwright test\n    ```\n\n2. Run Pytest for backend API (using uv inside the container):\n    ```bash\n    docker compose run backend-tests [uv run pytest]\n    ```\n\n### Environment Variables\n\n* Use `.env.local` for local development (gitignored)\n\n* Use `project.env` for containerrized development\n\n* To inspect environment variables inside a container:\n    ```bash\n    docker exec -it <container_name> bash\n    printenv\n    ```\n\n### Services overview\n\n1. Frontend (Client)\nBuilt with Svelte and Vite.\nLocated in the client/ directory.\nSee client/README.md for more details.\n\n2. Backend (Python Server)\nBuilt with FastAPI.\nLocated in the python-server/ directory.\nSee python-server/README.md for more details.\n\n3. Database\nPostgreSQL database for storing electricity price data.\nFlyway is used for managing schema migrations (database-migrations/).\n\n4. Chat Engine\nA chat-based interface for interacting with the app.\nLocated in the chat-engine/ directory.\n\n5. Data Preparation\nScripts for loading and updating electricity price data.\nLocated in the data-preparation/ directory.\n\n### Contributing\n\n1. Fork the repository.\n\n2. Create a feature branch:\n\n    ```bash\n    git checkout -b feature-name\n    ```\n\n3. Commit your changes:\n\n    ```\n    git commit -m \"Add feature-name\"\n    ```\n\n4. Push to your branch:\n\n    ```bash\n    git push origin feature-name\n    ```\n\n5. Open pull request.\n\n\n## License\n\nThis project us under MIT license: https://mit-license.org/\n\n## Acknowledgments\n\nElectricity price data is sourced from Pörssisähkö API.\n\n## Additional notes\n\n**Install docker and docker compose**. Maybe easiest to just install docker desktop, especially on windows.\n\n**Postgres is not needed on local machine** (unless you want to run outside containers).\n\nThe compose.yaml and the individual Dockerfiles are sufficient to run the App. Docker does the installing for the containers. But you can still run `deno install --allow-scripts`, if you want to run on local host. On windows, after running deno install, `node_modules/` that are loaded into client should not be copied into the container -- the container is using arch-linux as base image. You can either remove those, or add your own `.dockerignore` file.\n\nRun using docker compose:\n\n`docker compose up --build` (no need to build everytime)\n\nYou can also simply `ctrl+C` to shut down the containers, or\n\n`docker compose down` to tear down.\n\n\n### **About environment variables**\n\nKeep private information private, preferably :-). You can use `.env.local` convention, and keep them gitignored. For public api keys, while developing, we can all get our own api keys.\n\n**If you are unsure what environment variables are loaded on your container launch -- either by docker from project.env, or by services using other tools, like dotenv -- you can always go inside the container to check:**\n\n```\ndocker compose up -d <service_name> # launch the container\n\ndocker exec -it <container_name> bash # go into cmdline inside\n\n(container): printenv # or echo etc.\n```\n\n**For developing client:** There is a sort of a bug in the denolands alpine image, which prevents us from installing with optional flags -- in our case `deno install --allow-scripts`. This means that the node modules need to be copied from local. This is not an issue if you are using linux (or wsl2 on Windows). Later, there might be a change in the client's base image later on to fix this issue.\n\nAnd note that the container names are not necessarily same as the service name (they are derived from it though); you can check running cont's with `docker <container> ps`.\n\n### Running without Docker\n\nCan be done, but needlessly cumbersome. Ask Paavo for the how.\n\n\n### Starting a new client build from scratch\n\nIf you want to start the client build from scratch, for example with typescript checking enabled, run:\n\n```bash\ndeno run -A npm:sv@latest create client\n```\n\nfrom the root directory, and choose from the given options. For this project, we have used the most minimal build setup (SveleteKit minimal, no TS typechecking, nothing added, with deno itself for dependency management).\n",
  "./App/backend-tests/Dockerfile": "FROM python:3.13-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the test files into the container\nCOPY ./tests /app/tests\n\nCOPY ./pyproject.toml /app/pyproject.toml\nCOPY ./pytest.ini /app/pytest.ini\n\n# Add python-server to PYTHONPATH\nENV PYTHONPATH=\"/app/python-server:$PYTHONPATH\"\n\n# Install dependencies and curl\nRUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates\n# Download the latest uv installer\nADD https://astral.sh/uv/install.sh /uv-installer.sh\n# Run the installer then remove it\nRUN sh /uv-installer.sh && rm /uv-installer.sh\n# Ensure the installed binary is on the `PATH`\nENV PATH=\"/root/.local/bin/:$PATH\"\n\nRUN uv sync\n\nRUN uv add pip\nRUN uv run pip install --upgrade httpx\n\n# Set the default command to run pytest\nCMD [\"uv\", \"run\", \"pytest\", \"tests\"]",
  "./App/backend-tests/README.md": "### Endpoint tests\n\nYou need to have the Eprice -app running if you want to run the tests. So call `docker compose up` to run the server (or with `-d server` flag, since front is not needed for these tests). If you run `docker compose up --build`, the backend tests **will not be started** as they are defined as a \"profile\" in compose file.\n\nTo run the tests, first build the container with `docker compose build` inside the backend-tests, or just by adding the `--build` and `-d` flags:\n\n```\ndocker compose up --build -d backend-tests\n```\n\nYou can also run them in various other ways, e.g., jointly with every other container:\n\n```\ndocker compose up --profile backend-tests\n```\n\nIf you do it this way, and we add tests that depend on the loaded data in the database, this can cause issues -- we need to specify in the compose file that the backend-tests depends on `database: service_healthy`.\n\nYou can also run the tests with\n\n```\ndocker compose run backend-tests\n```\n\nbut these will leave a \"orphan\" containers behind -- these can be removed with `--remove-orphans` flag. Docker will warn you is this is happening, and in any case it is good practice to keep an eye on dockers resource usage and dangling containers:\n\n```\ndocker ps # running containers\ndocker ps -a # all containers\ndocker inspect <container_name_or_id>\ndocker stats # resource usage\ndocker container prune # remove stopped containers (it prompts you)\ndocker system prune # remove unused images, networks (and volumes)\n```\n",
  "./App/backend-tests/tests/test_auth_controller.py": "import pytest\nimport pytest_asyncio\nfrom httpx import AsyncClient, ASGITransport\nimport random\n\n# Create a pytest-asyncio fixture for the test client\n@pytest_asyncio.fixture\nasync def client():\n    async with AsyncClient(base_url=\"http://localhost:8000\") as client:\n        yield client\n\n# Test successful user registration\n@pytest.mark.asyncio\nasync def test_register_user(client):\n    # create a random email for testing\n    random_number = random.randint(0, 99999)\n    random_email = f\"email{random_number}@user.com\"\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": random_email, \"password\": \"newpassword\"},\n        headers={\"Content-Type\": \"application/json\"}\n    )\n    assert response.status_code == 200\n    assert \"message\" in response.json()\n    assert \"Confirmation email sent\" in response.json()[\"message\"]\n\n@pytest.mark.asyncio\nasync def test_register_user_invalid_email(client):\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": \"invalid-email\", \"password\": \"newpassword\"},\n        headers={\"Content-Type\": \"application/json\"}\n    )\n    assert response.status_code == 422\n    assert \"value is not a valid email address: An email address must have an @-sign.\" in response.json()[\"message\"]\n\n@pytest.mark.asyncio\nasync def test_register_user_invalid_email_end(client):\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": \"invalid-email@\", \"password\": \"newpassword\"},\n        headers={\"Content-Type\": \"application/json\"}\n    )\n    assert response.status_code == 422\n    assert \"value is not a valid email address: There must be something after the @-sign.\" in response.json()[\"message\"]\n\n\n@pytest.mark.asyncio\nasync def test_register_user_short_password(client):\n    # Generate a random email for testing\n    random_number = random.randint(0, 99999)\n    random_email = f\"another{random_number}@user.com\"\n\n    # Send a request with a short password\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": random_email, \"password\": \"3\"},  # Password is too short\n        headers={\"Content-Type\": \"application/json\"}\n    )\n\n    # Assert the response status code\n    assert response.status_code == 422\n    assert \"Password must be at least 4 characters long\" in response.json()[\"message\"]\n\n",
  "./App/chat-engine/Dockerfile": "FROM ollama/ollama\n\n# I will clean this up later (Paavo)\n# Set working directory\nWORKDIR /app\n\n# Copy the script to the docker image\nCOPY ./chat-engine/run.sh /run.sh\n\nCOPY ./chat-engine/gradio_dashboard.py /app/gradio_dashboard.py\n\nCOPY ./chat-engine/.env /app/.env\n\nCOPY ./chat-engine/pyproject.toml /app/pyproject.toml\n\n# Install dependencies and curl\nRUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates python3\n\n# Download the latest uv installer\nADD https://astral.sh/uv/install.sh /uv-installer.sh\n\n# Run the installer (then remove it)\nRUN sh /uv-installer.sh && rm /uv-installer.sh\n\n# Ensure the installed binary is on the `PATH`\nENV PATH=\"/root/.local/bin/:$PATH\"\n\n# Set environment variables for Gradio server\nENV GRADIO_SERVER_NAME=\"0.0.0.0\"\n\n#RUN pip install --no-cache-dir -r requirements.txt\nRUN uv sync\n\n# Ensure the script is executable\nRUN chmod +x /run.sh\n\nENTRYPOINT [\"/bin/bash\", \"/run.sh\"]",
  "./App/chat-engine/README.md": "## Chat engine for Eprice app\n\nThis folder has the dockerfile for ollama llm engine, and a minimal gradio dashboard implementation. In order to run the engine, docker needs to load the *llama3.2* model first. It will be downloaded into `chat-engine/.ollama/` folder, which you need to create. However, do not include it in git. \n\nThe chat-engine uses uv as the default package manager, and it's included in the container also. The dependencies are in `pyproject.toml`, and if you prefer pip, you can always you that. However, remember to include you local virtualenvironment into .gitignore and .dockerignore.\n\n**This container is not run by default when you run the project using docker-compose**. To include the chat-engine, you need use a specific profile:\n\n```\ndocker compose --profile chat-engine up\n```\n\nYou should have `chat-engine/.env.development` with `OPENAI_API_KEY` defined -- it does not need to be a valid api key, any styring will do. This is due to llm client/interface compatibility (and only for now). Eventually, we will collect all env files into a single project env (at compose level).\n\nThe bash script `run.sh` is likely going to change before long, but for now it's used to manage the Ollama server inside the container. You can connect directly into the container with `docker exec -it chat-engine bash` or something of that nature.\n",
  "./App/chat-engine/gradio_dashboard.py": "import gradio as gr\nfrom ollama import chat, ChatResponse\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv(\".env.development\")\n\n'''\nForm of ollama response (key, value pairs):\n('model', 'llama3.2')\n('created_at', '2025-04-09T11:00:25.809762663Z')\n('done', True)\n('done_reason', 'stop')\n('total_duration', 6226280507)\n('load_duration', 1044162763)\n('prompt_eval_count', 32)\n('prompt_eval_duration', 437000000)\n('eval_count', 89)\n('eval_duration', 4743000000)\n('message', Message(role='assistant', content=\"I'm doing well, ...\", images=None, tool_calls=None))\n'''\n\nclass ChatManager:\n    def __init__(self):\n        self.history = []\n        self.self_model = \"llama3.2\"\n\n    def send_message(self, message: str, history: list = None):\n        # append the message to the history and send it to the model\n        # TODO: check history length and truncate if necessary\n        # TODO: history condensation\n        self.append_message(message)\n        response: ChatResponse = chat(\n            model=self.self_model,\n            messages=self.history,\n            stream=True,\n        )\n        output = \"\"\n        for chunk in response:\n            output += chunk.message.content\n            yield output\n            \n\n        self.history.append({\"role\": \"user\", \"content\": message})\n        self.history.append({\"role\": \"assistant\", \"content\": output})\n        #self.history.append({\"role\": \"assistant\", \"content\": response.message.content})\n        #return response.message.content\n    \n    def clear_history(self):\n        # Clear the history of messages\n        self.history = []\n        return \"History cleared.\"\n    \n    def append_message(self, message: str):\n        # Append a message to the history\n        self.history.append({\"role\": \"user\", \"content\": message})\n        \n\n    def get_history(self):\n        # Return the history of messages\n        return self.history\n    \n\n# Initialize the chat manager\nchat_manager = ChatManager()\napp = gr.ChatInterface(\n    title=\"Eprice knowledge base\",\n    description=\"Open source language model with a knowledge base.\",\n    fn=chat_manager.send_message,\n    type=\"messages\",\n    autoscroll=True,\n)\napp.launch(pwa=True, share=False)",
  "./App/chat-engine/run.sh": "#!/bin/bash\n\necho \"🔴 Starting Ollama...\"\nollama serve &\necho \"🟢 Ollama started!\"\n\n# Wait for Ollama to be ready\necho \"🔴 Waiting for Ollama to be ready...\"\nsleep 5\necho \"🟢 Ollama is ready!\"\n\necho \"🔴 Retrieving model...\"\nollama pull llama3.2\necho \"🟢 Done!\"\n\n\n# Start the Gradio app and log output\necho \"🔴 Starting Gradio app...\"\nuv run gradio_dashboard.py\n\n",
  "./App/client/Dockerfile": "FROM denoland/deno:alpine-2.0.2\n\nWORKDIR /app\n\nCOPY package.json .\n\nRUN DENO_FUTURE=1 deno install\n\nCOPY . .\n\nCMD [ \"run\", \"dev\", \"--host\" ]\n",
  "./App/client/README.md": "### Client/front template for Eprice app\n\nYou need to have deno installed: https://docs.deno.com/runtime/\n\n* missing project.env -- ask Paavo for this.\n\n* run this with docker compose.\n\n* first run `deno install --allow-scripts` in `client/` and `e2e-tests/` directories. This is likely to change in the future, but for now the denoland's alpine base image does not allow installing with optional flags (why this is the case beats me).\n\n",
  "./App/client/src/app.css": "@import \"tailwindcss\";\n@plugin \"@tailwindcss/forms\";\n\n@import \"@skeletonlabs/skeleton\";\n@import \"@skeletonlabs/skeleton/optional/presets\";\n@import \"@skeletonlabs/skeleton/themes/cerberus\";\n@import \"@skeletonlabs/skeleton/themes/mint\";\n@import \"@skeletonlabs/skeleton/themes/pine\";\n@import \"@skeletonlabs/skeleton/themes/seafoam\";\n@import \"@skeletonlabs/skeleton/themes/rocket\";\n\n@source \"../node_modules/@skeletonlabs/skeleton-svelte/dist\";\n/*@tailwind base;\n@tailwind components;\n@tailwind utilities;\n*/\n\nhtml,\nbody {\n  @apply h-full;\n}\n\n#mainheading {\n  @apply mt-8 mb-4 text-3xl font-extrabold text-gray-900 dark:text-white md:text-4xl lg:text-6xl\n}\n\n#minorheading {\n  @apply mt-8 mb-4 font-extrabold text-gray-900 dark:text-white md:text-3xl lg:text-4xl\n}\n\n/* ChatBot styles */\n.chat-toggle {\n  position: fixed;\n  display: flex;\n  z-index: 10000; /* Ensure it appears above the chat window */\n  right: 0;\n  bottom: 750px; /* Default position when the chat is visible */\n  padding: 3.5rem 2rem; /* Increase padding for a larger button */\n  font-size: 1.25rem; /* Increase font size */\n}\n\n.chat-toggle.hidden {\n  right: fixed; /* Reset right alignment */\n  right: 0; /* Move to the left lower corner */\n  bottom: 0; /* Position at the bottom of the screen */\n}\n",
  "./App/client/src/app.html": "<!doctype html>\n<html lang=\"en\">\n\t<head>\n\t\t<meta charset=\"utf-8\" />\n\t\t<link rel=\"icon\" href=\"%sveltekit.assets%/favicon.png\" />\n\t\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n\t\t%sveltekit.head%\n\t</head>\n\t<body data-sveltekit-preload-data=\"hover\" data-theme=\"rocket\">\n\t\t<div style=\"display: contents\">%sveltekit.body%</div>\n\t</body>\n</html>\n",
  "./App/client/src/hooks.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { decodeJwt } from \"jose\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\n// handle with decoding only\nexport const handle = async ({ event, resolve }) => {\n  const authCookie = event.cookies.get(COOKIE_KEY);\n  if (authCookie) {\n    try {\n      const payload = decodeJwt(authCookie);\n      event.locals.user = payload;\n    } catch (e) {\n      console.log(e);\n    }\n  }\n\n  return await resolve(event);\n};\n\n// handle using verify endpoint\nconst OLDhandle = async ({ event, resolve }) => {\n  const authCookie = event.cookies.get(COOKIE_KEY);\n  if (authCookie) {\n    const response = await fetch(`${PUBLIC_INTERNAL_API_URL}/api/auth/verify`, {\n      method: \"POST\",\n      headers: {\n        \"cookie\": `${COOKIE_KEY}=${authCookie}`,\n      },\n    });\n\n    // if response not ok, clear cookie and resolve\n    if (!response.ok) {\n      event.cookies.delete(COOKIE_KEY, { path: \"/\" });\n      return await resolve(event);\n    }\n\n    const responseCookies = response.headers.getSetCookie();\n    const cookie = responseCookies.find((cookie) =>\n      cookie.startsWith(COOKIE_KEY)\n    );\n\n    // if no cookie, resolve\n    if (!cookie) {\n      return await resolve(event);\n    }\n\n    const cookieValue = cookie.split(\"=\")[1].split(\";\")[0];\n    event.cookies.set(COOKIE_KEY, cookieValue, { path: \"/\", secure: false });\n\n    try {\n      const payload = decodeJwt(authCookie);\n      event.locals.user = payload;\n    } catch (e) {\n      console.log(e);\n    }\n  }\n\n  return await resolve(event);\n};\n",
  "./App/client/src/lib/apis/data-api.js": "import { PUBLIC_API_URL } from \"$env/static/public\";\n\nconst readData = async () => {\n    const response = await fetch(`${PUBLIC_API_URL}/api/data`, {\n        headers: {\n        \"Content-Type\": \"application/json\",\n        },\n        method: \"GET\",\n        credentials: \"include\", // Include cookies in the request\n    });\n    return await response.json();\n};\n\nconst readPublicData = async () => {\n    const response = await fetch(`${PUBLIC_API_URL}/api/public/data`, {\n        headers: {\n        \"Content-Type\": \"application/json\",\n        },\n        method: \"GET\",\n    });\n    return await response.json();\n};\n\nexport { readData, readPublicData };\n",
  "./App/client/src/lib/components/ChatBot.svelte": "<script>\n    import { PUBLIC_CHAT_URL } from \"$env/static/public\";\n    \n    let chatOnState = $state(false);\n    const toggleChat = () => {\n        chatOnState = !chatOnState;\n    };\n    let user = $props();\n\n</script>\n\n<!-- Styles like chat-toggle are in app.css (mix with tailwind) -->\n<button\n    class=\"text-white font-bold py-2 px-8 rounded chat-toggle {chatOnState ? '' : 'hidden'}\"\n    onclick={toggleChat}>\n    {chatOnState ? 'Hide Chat' : 'Open Chat'}\n</button>\n\n{#if chatOnState}\n    <div class=\"gradio-embed\" style=\"position: fixed; bottom: 0; right: 0; width: 400px; height: 800px; z-index: 9999;\">\n        <iframe title=\"Gradio App\"\n            src=\"{PUBLIC_CHAT_URL}\"\n            width=\"100%\"\n            height=\"100%\"\n            frameborder=\"20\"\n            allowfullscreen\n            name=\"gradio-iframe\"\n            sandbox=\"allow-same-origin allow-scripts allow-popups allow-forms\"\n        ></iframe>\n    </div>\n{/if}\n",
  "./App/client/src/lib/components/MainChart.svelte": "<script>\n    import chartjs from 'chart.js/auto';\n    import { onMount } from 'svelte';\n    import { usePricesState } from '$lib/states/usePricesState.svelte';\n\n    let prices = usePricesState();\n    let ctx;\n    let chartCanvas;\n\n   \n   onMount(async () => {\n    // Get the raw API data (array of objects)\n    const priceData = prices.data;\n\n    // Find the most recent startDate\n    const mostRecent = priceData.reduce((latest, item) =>\n        new Date(item.startDate) > new Date(latest.startDate) ? item : latest\n    , priceData[0]);\n\n    // Filter data to include only entries up to the most recent hour\n    const filteredData = priceData.filter(item =>\n        new Date(item.startDate) <= new Date(mostRecent.startDate)\n    );\n\n    // Sort by startDate ascending (optional, for chart order)\n    filteredData.sort((a, b) => new Date(a.startDate) - new Date(b.startDate));\n\n    // Transform data for the chart\n    const chartLabels = filteredData.map(item => {\n        const date = new Date(item.startDate);\n        const hour = String(date.getUTCHours()).padStart(2, '0');\n        const minute = String(date.getUTCMinutes()).padStart(2, '0');\n        return `${hour}:${minute}`;\n    });\n    // Get the price values for the chart\n    const chartValues = filteredData.map(item => item.price);\n\n    ctx = chartCanvas.getContext('2d');\n    new chartjs(ctx, {\n        type: 'line',\n        data: {\n            labels: chartLabels,\n            datasets: [{\n                label: 'Sähkön hinta',\n                backgroundColor: 'rgb(70, 50, 255)',\n                borderColor: 'rgb(255, 255, 255)',\n                data: chartValues,\n            }]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: true,\n            scales: {\n                x: {\n                    ticks: {\n                        maxRotation: 60,\n                        minRotation: 60\n                    }\n                }\n            },\n\n\t\t\tplugins: {\n        \ttooltip: {\n            callbacks: {\n                title: function(context) {\n                    const idx = context[0].dataIndex;\n                    const item = filteredData[idx];\n                    const date = new Date(item.startDate);\n                    const day = String(date.getUTCDate()).padStart(2, '0');\n                    const month = String(date.getUTCMonth() + 1).padStart(2, '0');\n                    const year = date.getUTCFullYear();\n                    const hour = String(date.getUTCHours()).padStart(2, '0');\n                    const minute = String(date.getUTCMinutes()).padStart(2, '0');\n                    return `${day}.${month}.${year} ${hour}:${minute}`;\n                }\n            }\n        }\n    }\n        },\n\n    });\n});\n</script>\n\n<canvas bind:this={chartCanvas} id=\"myChart\" class=\"\n\tbottom-10\n\ttop-10\n\tm-auto\n\trounded\"\n style=\"width: 100%;background-color: #1e1e2f;\"\n></canvas>\n\n",
  "./App/client/src/lib/components/layout/Footer.svelte": "<script>\n    let data = $props();\n</script>\n\n\n<footer class=\"p-4 bg-black border-t-1\">\n<!--<footer class=\"bg-primary-300 p-4 border-t-1\">-->\n    <p class=\"text-center text-white font-bold\">\n        Dogs seem to dislike cats, sadly... © 2025\n    </p>\n</footer>\n",
  "./App/client/src/lib/components/layout/Header.svelte": "<script>\n  import { page } from '$app/stores';\n  let { user } = $props();\n</script>\n\n<header class=\"flex items-center justify-between bg-black p-4 mb-6 border-b-1\">\n<!--<header class=\"flex items-center bg-primary-300 p-4 mb-6\">-->\n  <h1 class=\"text-2xl text-white\">Electricity prices</h1>\n    <nav>\n      <ul class=\"ml-4 flex space-x-4 text-white\">\n        <li>\n          <a href=\"/\" class=\"\">Home</a>\n        </li>\n        {#if user}\n        <li>\n          <p>Placeholder</p>\n        </li>\n        {:else}\n            <li>\n              <a href=\"/auth/login\" class=\"\">Login</a>\n            </li>\n            <li>\n              <a href=\"/auth/register\" class=\"\">Register</a>\n            </li>\n        {/if}\n      </ul>\n    </nav>\n    {#if user && $page.url.pathname !== '/logout'}\n      <div class=\"ml-auto\">\n        <a href=\"/logout\" class=\"underline text-white\">Logout</a>\n      </div>\n    {:else if $page.url.pathname === '/logout'}\n      <div class=\"ml-auto\">\n        <!--This is a hack to force the placement of other elements-->\n      </div>\n    {/if}\n</header>",
  "./App/client/src/lib/components/layout/PriceBall.svelte": "<script lang=\"ts\">\n    let { heading, price } = $props();  \n</script>\n\n<div class=\"flex flex-col justify-center items-center rounded-full bg-blue-500 w-50 h-50\">\n    <p class=\"font-bold\">{heading}</p>\n    <span>{price}(snt / kWh)</span>\n</div>",
  "./App/client/src/lib/states/usePricesState.svelte.js": "import { browser } from \"$app/environment\";\nimport * as dataApi from \"$lib/apis/data-api.js\";\n\nlet pricesState = $state({});\n\nif (browser) {\n  pricesState = await dataApi.readPublicData();\n}\n\nconst usePricesState = () => {\n  return {\n    get data() {\n      return pricesState;\n    },\n    update: async () => {\n      pricesState = await dataApi.readPublicData();\n    },\n    // this is a trick for chart.js to get the data\n    // it needs to be a plain object\n    // otherwise it will not work\n    state2js: () => {\n      const temp = JSON.stringify(pricesState);\n      return JSON.parse(temp);\n    },\n  }\n};\n\nexport { usePricesState };",
  "./App/client/src/lib/states/userState.svelte.js": "let user = $state({ });\n\nconst useUserState = () => {\n  return {\n    get user() {\n      return user;\n    },\n    set user(u) {\n      user = u;\n    },\n  }; // you can also define other methods/properties here\n};\n\nexport { useUserState };",
  "./App/client/src/routes/+layout.js": "//export const prerender = true;\n",
  "./App/client/src/routes/+layout.server.js": "//export const ssr = true;\nexport const load = async ({ locals }) => {\n    return locals;\n  };",
  "./App/client/src/routes/+layout.svelte": "<script>\n    import \"../app.css\";\n    import bg from \"$lib/assets/image.png\";\n    import { useUserState } from \"$lib/states/userState.svelte.js\";\n    import Header from \"$lib/components/layout/Header.svelte\";\n    import Footer from \"$lib/components/layout/Footer.svelte\";\n    import ChatBot from \"$lib/components/ChatBot.svelte\";\n\n    \n    let { children, data } = $props();\n    const userState = useUserState();\n    if (data.user) {\n      userState.user = data.user;\n    }\n\n  </script>\n\n<div class=\"flex flex-col h-full\">\n  \n  \n  <Header user={data.user} />\n\n  {#if data.user?.email}\n    <p class=\"\">\n      Logged in as:<br/> <b>{data.user?.email}</b>\n    </p>\n\n    <ChatBot user={data.user} />\n  {/if}\n  \n  <main class=\"container mx-auto max-w-2xl grow\">\n    {@render children()}\n  </main>\n\n  <Footer />\n  \n</div>\n\n<!--\n<style>\n  :global(body){\n  background-image: url(\"https://images.pexels.com/photos/956981/milky-way-starry-sky-night-sky-star-956981.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: center;\n  font-family: 'Poppins', sans-serif;\n  font-size: 1.2rem\n  }\n</style>\n-->",
  "./App/client/src/routes/+page.svelte": "<script>\n    import PriceBall from \"$lib/components/layout/PriceBall.svelte\";\n    import MainChart from \"$lib/components/MainChart.svelte\";\n\n    import { onMount } from \"svelte\";\n    import { usePricesState } from \"$lib/states/usePricesState.svelte\";\n\n    const { data: pricesState } = usePricesState();\n\n    // Function to calculate the average price for the day\n    const averageDayPrice = array => array.reduce((a, b) => a + b) / array.length;\n\n    let currentPrice = null; // Default value for current price\n    let prices = null; // Store the fetched prices\n    let loading = true; // Loading state\n    let error = null; // Error state\n    let dailyAverage = null; // Store the calculated daily average\n\n    // Calculate the current price and daily average\n    onMount(() => {\n        if (pricesState) {\n            console.log(\"Chart Values:\", pricesState);\n            const now = new Date();\n            const currentHour = now.getHours();\n            const currentDate = now.toISOString().split(\"T\")[0];\n\n            // Find the current price\n            const matchingPrice = pricesState.find(price => {\n                const priceDate = new Date(price.startDate);\n                return (\n                    priceDate.getUTCHours() === currentHour &&\n                    priceDate.toISOString().startsWith(currentDate)\n                );\n            });\n\n            currentPrice = matchingPrice ? matchingPrice.price : \"N/A\";\n\n            // Calculate the daily average\n            dailyAverage = averageDayPrice(pricesState.map(p => p.price));\n            dailyAverage = dailyAverage.toFixed(3); // Format to 2 decimal places\n        }\n    }\n    );\n\n\n    //let { data, form } = $props();\n</script>\n\n<h1 id=\"mainheading\" class=\"text-center\">\n    Welcome!<br>\n    <span class=\"text-2xl\">Market Electricity Prices<br> {new Date().toLocaleDateString()}</span>\n    <br>\n</h1>\n<main class=\"flex gap-20\">\n    <div class=\"text-center gap-5 flex flex-col\">\n       <PriceBall heading=\"Sähkönhinta NYT\" price={currentPrice}/>\n       <PriceBall heading=\"Sähkö 24h\" price={dailyAverage}/>\n       <PriceBall heading=\"Sähkö viikko\" />\n        \n    </div>\n    <MainChart />   \n</main>",
  "./App/client/src/routes/auth/[action]/+page.js": "import { error } from \"@sveltejs/kit\";\n\nexport const load = ({ params, url }) => {\n  if (params.action !== \"login\" && params.action !== \"register\" && params.action !== \"verify\") {\n    throw error(404, \"Page not found.\");\n  }\n\n  if (url.searchParams.has(\"registered\")) {\n    params.registered = true;\n  }\n\n  if (url.searchParams.has(\"email\")) {\n    params.email = url.searchParams.get(\"email\");\n  } else {\n    params.email = null;\n  }\n\n  if (url.searchParams.has(\"code\")) {\n    params.code = url.searchParams.get(\"code\");\n  } else {\n    params.code = null;\n  }\n  if (url.searchParams.has(\"is_verified\")) {\n    params.is_verified = true;\n  } else {\n    params.is_verified = null;\n  }\n\n  return params;\n};",
  "./App/client/src/routes/auth/[action]/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { redirect } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\nconst apiRequest = async (url, data) => {\n  return await fetch(`${PUBLIC_INTERNAL_API_URL}${url}`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify(data),\n  });\n};\n\nexport const actions = {\n\n  login: async ({ request, cookies }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/login\",\n      Object.fromEntries(data),\n    );\n\n    if (response.ok) {\n      const responseCookies = response.headers.getSetCookie();\n      const cookie = responseCookies.find((cookie) =>\n        cookie.startsWith(COOKIE_KEY),\n      );\n      const cookieValue = cookie.split(\"=\")[1].split(\";\")[0];\n      cookies.set(COOKIE_KEY, cookieValue, { path: \"/\", secure: false });\n      throw redirect(302, \"/\");\n    }\n    \n    return response.json();\n  },\n  \n  register: async ({ request }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/register\",\n      Object.fromEntries(data),\n    );\n\n    if (response.ok) {\n      throw redirect(302, \"/auth/verify?registered=true\");\n    }\n\n    return await response.json();\n  },\n\n  verify: async ({ request }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/verify\",\n      Object.fromEntries(data),\n    );\n\n    if (response.ok) {\n      throw redirect(302, \"/auth/login?is_verified=true\");\n    }\n\n    return await response.json();\n  },\n\n};\n",
  "./App/client/src/routes/auth/[action]/+page.svelte": "<script>\n    let { data, form } = $props();\n    let title = $state(\"\");\n    let email = $state(\"\");\n    let code = $state(\"\");\n\n    $effect(() => {\n        if (data.action === \"login\") {\n            title = \"Login\";\n        } else if (data.action === \"register\") {\n            title = \"Register\";\n        } else if (data.action === \"verify\") {\n            title = \"Verify\";\n        }\n        if (data.code && data.action === \"verify\") {\n            code = data.code;\n        }\n        if (data.email) {\n            email = data.email;\n        }\n    });\n\n</script>\n  \n  <h1 id=\"minorheading\" class=\"text-center\"><!--class=\"text-xl pb-4\"> h2-->\n    {title} Form<!--{data.action === \"login\" ? \"Login\" : \"Register\"} form-->\n  </h1>\n  \n  {#if form?.message}\n    <p class=\"text-xl\">{form.message}</p>\n  {/if}\n\n  {#if form?.error}\n    <p class=\"text-xl text-red-500\">{form.error}</p>\n  {/if}\n\n  {#if data.registered}\n    <p class=\"text-xl\">\n      Verification has been sent to your email. Please verify your account to continue.\n    </p><br/>\n  {/if}\n\n  {#if data.is_verified}\n    <p class=\"text-xl\">\n      Your email has been verified. You can now login.\n    </p><br/>\n  {/if}\n\n  {#if form?.message===\"Email not verified.\"}\n    <p class=\"text-xl\">\n      Please check your email for the verification code.\n    </p><br/>\n  {/if}\n\n\n\n  <form class=\"space-y-4\" method=\"POST\" action=\"?/{data.action}\" enctype=\"application/json\">\n    <label class=\"label\" for=\"email\">\n      <span class=\"label-text\">Email</span>\n      <input\n        class=\"input\"\n        id=\"email\"\n        name=\"email\"\n        type=\"email\"\n        placeholder=\"Email\"\n        bind:value={email}\n        required\n      />\n    </label>\n    {#if data.action === \"verify\"}\n      <label class=\"label\" for=\"code\">\n        <span class=\"label-text\">Verification Code</span>\n        <input\n          class=\"input\"\n          id=\"code\"\n          name=\"code\"\n          type=\"text\"\n          bind:value={code}\n          required\n        />\n      </label>\n    <!--else if login or register, show password-->\n    {:else}\n      <label class=\"label\" for=\"password\">\n        <span class=\"label-text\">Password</span>\n        <input \n          class=\"input\"\n          id=\"password\"\n          name=\"password\"\n          type=\"password\"\n          required />\n      </label>\n    {/if}\n    <button class=\"w-full btn preset-filled-primary-500\" type=\"submit\">\n      {title}<!-- === \"login\" ? \"Login\" : \"Register\"}-->\n    </button>\n  </form>\n\n\n  {#if form?.message===\"Email not verified.\"}\n  <br/>\n  <div class=\"flex gap-2\">\n    <a href=\"/send\" class=\"btn preset-filled-primary-500\">\n      Resend Verification Email\n    </a>\n    <a href=\"/auth/verify\" class=\"btn preset-filled-primary-500 w-full\">\n      Verify Email\n    </a>\n  </div>\n  <br/>\n{/if}",
  "./App/client/src/routes/logout/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { redirect } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\nconst apiRequest = async (url) => {\n  return await fetch(`${PUBLIC_INTERNAL_API_URL}${url}`, {\n    method: \"GET\"\n  });\n};\n\nexport const actions = {\n\n  logout: async ({ cookies }) => {\n    const response = await apiRequest(\"/api/auth/logout\");\n    if (response.ok) {\n      cookies.delete(COOKIE_KEY, {path: \"/\"});\n      throw redirect(302, \"/\");\n    } else {\n      return { error: \"Logout failed\" };\n    }\n  }\n\n};\n",
  "./App/client/src/routes/logout/+page.svelte": "<script>\n    let { data, form } = $props();\n</script>\n\n\n<h1 id=\"minorheading\" class=\"text-center\"><!--class=\"text-xl pb-4\"> h2-->\n    Confirm your logout\n</h1>\n\n{#if form?.error}\n    <div class=\"alert alert-error\">\n        {form.error}\n    </div>\n{/if}\n\n{#if form?.message}\n    <div class=\"alert alert-success\">\n        {form.message}\n    </div>\n{/if}\n\n\n<form method=\"POST\" action=\"?/logout\">\n    <button class=\"w-full btn preset-filled-primary-500\" type=\"submit\">Logout</button>\n</form>\n\n",
  "./App/client/src/routes/send/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { redirect } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\nconst apiRequest = async (url, data) => {\n  return await fetch(`${PUBLIC_INTERNAL_API_URL}${url}`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify(data),\n  });\n};\n\nexport const actions = {\n\n  resend: async ({ request, cookies }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/resend\",\n      Object.fromEntries(data),\n    );\n    return response.json();\n  }\n};",
  "./App/client/src/routes/send/+page.svelte": "<script>\n    let { data, form } = $props();\n</script>\n\n<h1 id=\"minorheading\" class=\"text-center\">\n    Resend Verification Email\n</h1>\n  \n{#if form?.message}\n<p class=\"text-xl\">{form.message}</p>\n{/if}\n\n{#if form?.error}\n<p class=\"text-xl text-red-500\">{form.error}</p>\n{/if}\n\n<form class=\"space-y-4\" method=\"POST\" action=\"?/resend\" enctype=\"application/json\">\n    <label class=\"label\" for=\"email\">\n        <span class=\"label-text\">Email</span>\n        <input\n            class=\"input\"\n            id=\"email\"\n            name=\"email\"\n            type=\"email\"\n            placeholder=\"Email\"\n            required />\n    </label>\n    <button class=\"w-full btn preset-filled-primary-500\" type=\"submit\">\n      Send\n    </button>\n</form>\n",
  "./App/compose.yaml": "services:\n  database:\n    container_name: postgresql_database\n    image: reinikp2/pgvector-database:v1 #postgres:17.0\n    restart: unless-stopped\n    env_file:\n      - project.env\n    ports: # TODO: comment out later\n      - 5432:5432 # Only for development/debugging\n    environment:\n      - TZ=Europe/Helsinki  # Set the timezone\n    volumes:\n      - ./data-preparation/data:/data\n      - ./pgdata:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  database-migrations:\n    image: flyway/flyway:10\n    env_file:\n      - project.env\n    depends_on:\n      database:\n        condition: service_started\n    volumes:\n      - ./data-preparation/data:/data\n      - ./database-migrations:/flyway/sql\n    command: -connectRetries=60 -baselineOnMigrate=true migrate\n      \n  client:\n    build: client\n    restart: unless-stopped\n    volumes:\n      - ./client:/app\n    ports:\n      - 5173:5173\n    depends_on:\n      - server\n\n  e2e-tests:\n    entrypoint: \"/bin/true\"\n    build: e2e-tests\n    network_mode: host\n    depends_on:\n      - client\n    volumes:\n      - ./e2e-tests/tests:/app.tests\n\n  server:\n    build:\n      context: ./python-server  # Updated to point to the python-server directory\n    restart: unless-stopped\n    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload\n    volumes:\n      - ./python-server:/app\n    ports:\n      - 8000:8000\n    env_file:\n      - project.env\n    depends_on:\n      database:\n        condition: service_healthy\n      #- database\n    environment:\n      - TZ=Europe/Helsinki  # Set the timezone\n\n  chat-engine:\n    container_name: chat-engine\n    build:\n      context: .\n      dockerfile: chat-engine/Dockerfile\n    volumes:\n      - type: bind\n        source: ./chat-engine/.ollama\n        target: /root/.ollama\n    ports:\n      - 7860:7860\n    restart: always\n    profiles:\n      - chat-engine\n\n  # This service is for updating the database with data from external sources\n  # It will be run manually when needed. Any known and constant data should be\n  # added to the database using migrations. Only fresh data that is not\n  # available in the database should be added using this service.\n  data-preparation:\n    build:\n      context: ./data-preparation/scripts  # Here be Dockerfile\n    volumes:\n      - ./data-preparation/data:/data\n      - ./data-preparation/scripts:/scripts\n    working_dir: /scripts # this is already done in the Dockerfile (remove for production)\n    env_file:\n      - ./project.env\n    entrypoint: >\n      bash -c \"\n      ./retrieve_porssisahko_update.sh\n      #tail -f /dev/null # for debugging -- keeps the container running after the script finishes\n      \"\n    profiles:\n      - data-preparation\n\n  backend-tests:\n    build:\n      context: ./backend-tests\n    network_mode: host\n    depends_on:\n      - server\n      - database\n    env_file:\n      - ./project.env\n    volumes:\n      - ./backend-tests/tests:/app/tests\n      - ./python-server:/app/python-server\n    environment:\n      - PYTHONPATH=/app/python-server\n    profiles:\n      - backend-tests\n",
  "./App/data-preparation/scripts/Dockerfile": "# python:3.10-slim has security issues, so we changed to python:3.13-slim\nFROM python:3.13-slim \n\n# Install required system packages (libpq-dev for psycopg2, vim for Q&D edits)\n# and gnumeric for Excel file handling\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    gnumeric \\\n    libpq-dev \\\n    vim \\\n    && apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies using pip\nRUN pip install --no-cache-dir \\\n    pandas \\\n    psycopg2-binary\n\n# Set the working directory\nWORKDIR /scripts\n\n# Copy the scripts into the container\nCOPY . /scripts\n\n# Ensure all .sh scripts are executable\nRUN chmod +x /scripts/*.sh\n\n# Default command (can be overridden in docker-compose)\nCMD [\"bash\"]",
  "./App/data-preparation/scripts/README.md": "### Scipts for populating the database\n\nThe historical data stays constant but as new datapoints arrive they are being fetched daily. If for some reason, the service has been down for a long time it might be best to run data-preparation service -- it get's the new historical data and inserts it into the database in bulk. This way the backend server is not needlessly burdened with dataloading on launch.\n\nThe service can be run using profile `data-preparation` (depends on database and migrations).\n\n**If dangling/orphan containers cause problems:** You can remove them by adding the `--remove-orphans` flag for compose. If you want to populate the database with your own scripts, you should start by tearing down (with the above flag), and then re-build. You can use a profile, or target the service with `-d data-preparation` argument for compose up.",
  "./App/data-preparation/scripts/clean_porssisahko.py": "import pandas as pd\nimport datetime\nimport sys\n\ndef clean_data(filename):\n    # Read the CSV file\n    df = pd.read_csv(filename, sep=\",\", encoding=\"utf-8\")\n\n    # Extract times and prices\n    times = df[\"Aika\"].tolist()\n    prices = df[df.columns[1]].tolist()\n\n    # Convert prices to float, handle errors by setting to average of adjacent values\n    for i in range(len(prices)):\n        try:\n            #prices[i] = float(prices[i].replace(\",\", \".\"))\n            prices[i] = float(prices[i])\n        except ValueError:\n            print(f\"Invalid price at index {i}: {prices[i]}\")\n            if i == 0:\n                prices[i] = prices[i + 1]\n            elif i == len(prices) - 1:\n                prices[i] = prices[i - 1]\n            else:\n                prices[i] = (prices[i - 1] + prices[i + 1]) / 2\n\n    # Extract year, month, day, hour, and weekday\n    years = []\n    months = []\n    days = []\n    hours = []\n    weekdays = []\n    dates = []\n    datetimes = []\n\n    for line in times:\n        #datetimes.append(line)\n        date = line.split(\" \")[0]\n        time = line.split(\" \")[1]\n        year, month, day = date.split(\"/\")\n        hour = time.split(\":\")[0]\n        \n        # datetime as \"YYYY-MM-DD HH:00:00\",always beginning hours\n        datetime_str = f\"{year}-{month.zfill(2)}-{day.zfill(2)} {hour.zfill(2)}:00:00\"\n        datetimes.append(datetime_str)\n\n        dates.append(date)\n        years.append(int(year))\n        months.append(int(month))\n        days.append(int(day))\n        hours.append(int(hour))\n        weekday = datetime.datetime(int(year), int(month), int(day)).weekday()\n        weekdays.append(weekday)        \n\n    # Create a new DataFrame\n    df2 = pd.DataFrame({\n        \"datetime\": datetimes,\n        \"date\": dates,\n        \"year\": years,\n        \"month\": months,\n        \"day\": days,\n        \"hour\": hours,\n        \"weekday\": weekdays,\n        \"price\": prices\n    })\n\n    # Save the cleaned data to a new CSV file\n    output_file = filename.replace(\".csv\", \"_cleaned.csv\")\n    df2.to_csv(output_file, index=False, sep=\";\")\n    print(f\"Cleaned data saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python clean_data.py <filename>\")\n        sys.exit(1)\n\n    input_file = sys.argv[1]\n    clean_data(input_file)",
  "./App/data-preparation/scripts/populate_porssisahko.py": "# this script populates the porssisahko table in the database\n# NOTE: environment variables are loaded by docker compose\n\n# whatever dependencies you need, add them\n# to Dockerfile or create a requirements.txt file\nimport psycopg2 # pip install psycopg2-binary\nimport pandas as pd\nimport sys\n\n\ndef populate_db(df):\n\n    conn = psycopg2.connect() # env is already loaded by docker compose\n    cursor = conn.cursor()\n\n    # Juho, Markus:\n    # the table creation is handled by migrations (let's not do it in scripts)\n\n    # Insert data into the table (on conflict do nothing)\n    for _, row in df.iterrows():\n        cursor.execute(\"\"\"\n            INSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (Datetime) DO NOTHING\n        \"\"\", (row['datetime'], row['date'], row['year'], row['month'], row['day'], row['hour'], row['weekday'], row['price']))\n    # Commit the changes and close the connection\n    conn.commit()\n    cursor.close()\n    conn.close()\n    \n    print(\"Data populated successfully.\")\n    print(f\"Inserted {len(df)} rows into the database.\")\n\nif __name__ == \"__main__\":\n    # csv file has been cleaned and is in the correct format\n    filename = sys.argv[1]\n    df = pd.read_csv(filename, sep=\";\", encoding=\"utf-8\")\n    # Populate the database\n    populate_db(df)\n    ",
  "./App/data-preparation/scripts/retrieve_porssisahko_update.sh": "#!/bin/bash\n\n# downloads the xlsx file from the porssisahko API\ncurl https://porssisahko.net/api/internal/excel-export --output ../data/updated_porssisahko.xlsx\n\n# checks if the file was downloaded successfully\nif [ $? -ne 0 ]; then\n    echo \"Failed to download the file\"\n    exit 1\nfi\n\ninput_file=\"../data/updated_porssisahko.xlsx\"\n# output filename is same as input file with .csv extension\noutput_file=\"${input_file%.*}.csv\"\n\n# ssconvert transforms the file to csv format in temp_file.csv\nssconvert \"$input_file\" temp_file.csv # for debugging\n\n# removes first 3 rows from temp_file.csv\nsed -i '1,3d' temp_file.csv\n\n# save the modified file as output_file\nmv temp_file.csv \"$output_file\"\n\necho \"File converted and saved as $output_file\"\n\n# python script to process the csv file (nicer format with pandas and datetime)\npython clean_porssisahko.py \"$output_file\"\n# the new name has \"_cleaned.csv\" appended to the original name\nrm -f \"$output_file\" # (comment for debugging)\nmv \"${output_file%.csv}_cleaned.csv\" \"$output_file\"\n\n# run the populate_porssisahko.py script\npython populate_porssisahko.py \"$output_file\"\n# print success message\necho \"Table porssisahko populated from file $output_file\"\n# remove the original file\n#rm -f \"$input_file\" # (comment for debugging)",
  "./App/database-migrations/V10__documents_constraint.sql": "ALTER TABLE documents\nADD CONSTRAINT documents_file_type_name_start_line_unique UNIQUE (file, type, name, start_line);\n",
  "./App/database-migrations/V1__users.sql": "CREATE TABLE users (\n  id SERIAL PRIMARY KEY,\n  email TEXT NOT NULL,\n  password_hash TEXT NOT NULL\n);\n\nCREATE UNIQUE INDEX ON users(lower(email));\n",
  "./App/database-migrations/V2__porssisahko.sql": "CREATE TABLE IF NOT EXISTS porssisahko (\n    id SERIAL PRIMARY KEY,\n    datetime TIMESTAMP NOT NULL, -- Original column for datetime\n    date DATE NOT NULL, -- New column for the date\n    year INT NOT NULL, -- Year, etc. for statistics\n    month INT NOT NULL,\n    day INT NOT NULL,\n    hour INT NOT NULL,\n    weekday INT NOT NULL,\n    price NUMERIC(10, 3) NOT NULL,\n    predicted BOOLEAN NOT NULL DEFAULT FALSE,\n    createdAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Add a unique constraint to prevent duplicate rows (date and time)\nALTER TABLE porssisahko\n    ADD CONSTRAINT unique_datetime UNIQUE (datetime);\n",
  "./App/database-migrations/V3__timezone.sql": "ALTER DATABASE database SET timezone TO 'Europe/Helsinki';",
  "./App/database-migrations/V4__users_add_role.sql": "ALTER TABLE users\nADD COLUMN role VARCHAR(10) NOT NULL DEFAULT 'user';\n\nALTER TABLE users\nADD CONSTRAINT role_check CHECK (role IN ('user', 'admin'));\n\nUPDATE users\nSET role = 'user'\nWHERE role IS NULL;\n\n-- Insert a default admin user if it doesn't already exist\nINSERT INTO users (email, password_hash, role)\nVALUES ('test@test.com', '$2b$12$eh8m1dy3N2e/P5OvSuzHeeBwoaS9RbZPMThDhGoD0EuHrKbBq9JIW', 'admin')\nON CONFLICT ((lower(email))) DO NOTHING;\n\n-- Insert a default user if it doesn't already exist\nINSERT INTO users (email, password_hash)\nVALUES ('testi@testi.fi', '$2b$12$j6.jBujeoaNAenFbA/iELeoy2.Jlt9jNV.NunCCPZHet.z/4lKJtu')\nON CONFLICT ((lower(email))) DO NOTHING;",
  "./App/database-migrations/V5__porssisahko_load_entries.sql": "-- Create a temporary table for staging the data\nCREATE TEMP TABLE porssisahko_staging (\n    id SERIAL PRIMARY KEY,\n    datetime TIMESTAMP NOT NULL, -- Original column for datetime\n    date DATE NOT NULL, -- New column for the date\n    year INT NOT NULL, -- Year, etc. for statistics\n    month INT NOT NULL,\n    day INT NOT NULL,\n    hour INT NOT NULL,\n    weekday INT NOT NULL,\n    price NUMERIC(10, 3) NOT NULL,\n    predicted BOOLEAN NOT NULL DEFAULT FALSE,\n    createdAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Load data into the temporary table\nCOPY porssisahko_staging (datetime, date, year, month, day, hour, weekday, price)\nFROM '/data/porssisahko.csv'\nWITH CSV HEADER DELIMITER ';';\n\n-- Insert unique rows into the target table\nINSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\nSELECT datetime, date, year, month, day, hour, weekday, price\nFROM porssisahko_staging\nON CONFLICT (datetime) DO NOTHING;\n\n-- Drop the temporary table\nDROP TABLE porssisahko_staging;\n",
  "./App/database-migrations/V6__users_add_isverified.sql": "ALTER TABLE users\nADD COLUMN is_verified BOOLEAN NOT NULL DEFAULT FALSE;\n\nALTER TABLE users\nADD COLUMN verification_code VARCHAR(7);\n\n-- Set is_verified to TRUE and verification_code to 'ABC-123' for existing users\nUPDATE users\nSET is_verified = TRUE,\n    verification_code = 'ABC-123'\nWHERE is_verified = FALSE OR verification_code IS NULL;\n\n-- Enforce that verification_code is always exactly 7 characters\nALTER TABLE users\nADD CONSTRAINT verification_code_length CHECK (char_length(verification_code) = 7);\n",
  "./App/database-migrations/V8__extension_vector.sql": "CREATE EXTENSION vector;\n",
  "./App/database-migrations/V9__documents.sql": "\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    file TEXT NOT NULL,\n    type TEXT NOT NULL, -- 'function', 'class', 'document', etc.\n    name TEXT,\n    docstring TEXT,\n    start_line INTEGER,\n    code TEXT,\n    embedding VECTOR(384),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);",
  "./App/e2e-tests/Dockerfile": "FROM mcr.microsoft.com/playwright:v1.48.1-jammy\n\nWORKDIR /app\n\nCOPY package*.json .\nCOPY *config.js .\n\nRUN npm install\nRUN npx playwright install chromium\n\nCOPY . .\n\nCMD [ \"npc\", \"playwright\", \"test\" ]\n",
  "./App/e2e-tests/tests/example.spec.js": "const { test, expect } = require(\"@playwright/test\");\n\ntest('Pressing \"Fetch message\" shows message.', async ({ page }) => {\n  await page.goto(\"http://localhost:5173/\");\n  // wait for a second for the page to load\n  await page.waitForTimeout(1000);\n  const canvas = page.locator(\"#myChart\");\n  await expect(canvas).toBeVisible();\n});",
  "./App/project.env": "FLYWAY_USER=username\nFLYWAY_PASSWORD=password\nFLYWAY_URL=jdbc:postgresql://postgresql_database:5432/database\n\nPOSTGRES_USER=username\nPOSTGRES_PASSWORD=password\nPOSTGRES_DB=database\n\nPGUSER=username\nPGPASSWORD=password\nPGDATABASE=database\nPGHOST=postgresql_database\nPGPORT=5432\n\n",
  "./App/python-server/Dockerfile": "# Python 3.11 has vulnerability issues\nFROM python:3.13-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the requirements file into the container\nCOPY requirements.txt .\n\nRUN pip install --upgrade pip\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n#RUN pip install --upgrade passlib\n# incompatibility issues with new bcrypt (revert to old version)\nRUN pip install bcrypt==3.2.0\n# Copy the rest of the application code into the container\nCOPY . .\n\n# Expose the port the app runs on if you want access from host\n#EXPOSE 8000\n\n# Command to run the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n",
  "./App/python-server/README.md": "\n## Python FastAPI server template for Eprice app\n\n**See parent `README.md`.**\n\nYou need to define `.env` -file for production. The defaults should work for development purposes. See config folder.\n\n* requirements.txt has all the requirements -- if you add more, upodate the requirements.\n\n### Dockerized\n\nThe server is meant to be run  from docker container, and it is using dockerized Postgres database by default. It is also possible to develop it without docker by setting up an independent Postgres database, by running the postgres container, or by ignoring the database for unrelated dev. You can also define you own SQLite database locally from the python-server -- just change how the repositories makes the connection.\n\n* .dockerignore has the basics, but if you use some other .venv naming convention, add them. The container uses pip and requirements to install dependencies -- so using managers like uv/pip/poetry might produce files/folders you must exclude from docker (it copies all contents not excluded into the container).\n\n* .gitignore, same things. No venv's.\n\n",
  "./App/python-server/config/__init__.py": "\"\"\"\nconfig package initializer\n\nLoads environment variables from .env files for the Eprice backend.\n\"\"\"\n\n\nimport dotenv\ndotenv.load_dotenv(\".env.development\")\ndotenv.load_dotenv(\".env.local\")\n",
  "./App/python-server/config/secrets.py": "\"\"\"\nsecrets.py\n\nConfiguration and secrets for the Eprice backend.\n\nThis module loads environment variables for database, JWT, and email settings.\nIt also defines the list of public routes that do not require authentication.\n\"\"\"\n\nimport os\n\n# Database configuration\n# check if environment variables for postgres are set, otherwise use default values\n# Default values are for development purposes only\n# and should not be used in production\n\nif os.getenv(\"POSTGRES_USER\") is None:\n    os.environ[\"POSTGRES_USER\"] = \"username\"\nif os.getenv(\"POSTGRES_PASSWORD\") is None:\n    os.environ[\"POSTGRES_PASSWORD\"] = \"password\"\nif os.getenv(\"PGHOST\") is None:\n    os.environ[\"PGHOST\"] = \"postgresql_database\"\nif os.getenv(\"PGPORT\") is None:\n    os.environ[\"PGPORT\"] = \"5432\"\nif os.getenv(\"POSTGRES_DB\") is None:\n    os.environ[\"POSTGRES_DB\"] = \"database\"\n\n# PostgreSQL connection string\nDATABASE_URL = f\"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('PGHOST')}:{os.getenv('PGPORT')}/{os.getenv('POSTGRES_DB')}\"\n# JWT configuration\nJWT_SECRET = os.getenv(\"JWT_SECRET\", \"wsd-project-secret\")  # Default value for development\nALGORITHM = os.getenv(\"ALGORITHM\", \"HS256\")  # Default algorithm\nCOOKIE_KEY = os.getenv(\"COOKIE_KEY\", \"token\")  # Default cookie key\nACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7  # 7 days\n\nMAIL_USERNAME=os.getenv(\"MAIL_USERNAME\", \"eprice.varmennus@gmail.com\")  # Default sender\nMAIL_PASSWORD=os.getenv(\"MAIL_PASSWORD\")  # Default password\nMAIL_FROM=os.getenv(\"MAIL_FROM\", \"eprice.varmennus@gmail.com\")  # Default sender email\nMAIL_PORT=os.getenv(\"MAIL_PORT\", 587)  # Default port for TLS\nMAIL_SERVER=os.getenv(\"MAIL_SERVER\", \"smtp.gmail.com\")  # Default SMTP server\nMAIL_FROM_NAME=os.getenv(\"MAIL_FROM_NAME\", \"Eprice-verification\")\n\n\n# Public routes that do not require authentication\n# These routes can be accessed without a valid JWT token\npublic_routes = [\n    \"/api/public/data\",\n    \"/api/auth/login\",\n    \"/api/auth/register\",\n    \"/api/auth/verify\",\n    \"/api/auth/resend\",\n    \"/api/auth/logout\",\n    \"/docs\",\n    \"/openapi.json\"\n    ]",
  "./App/python-server/controllers/auth_controller.py": "\"\"\" auth_controller.py defines the authentication controller for the Eprice backend API using FastAPI.\nIt provides endpoints for user registration, login, logout, email verification, and resending verification codes.\nThe controller manages authentication logic, JWT token handling, and cookie management for session persistence.\n\nKey Endpoints:\n\nPOST /api/auth/register: Registers a new user and sends a confirmation email. Handles duplicate email errors.\nPOST /api/auth/login: Authenticates a user, checks email verification status, and issues a JWT token as an HTTP-only cookie.\nGET /api/auth/logout: Logs out the user by deleting the authentication cookie.\nPOST /api/auth/verify: Verifies a user's email using a code sent to their email address.\nPOST /api/auth/resend: Resends the email verification code to the user.\nThe controller uses dependency-injected service and repository layers for business logic and database access.\nIt also provides a JWT middleware factory for protecting private routes by validating JWT tokens from cookies and attaching user info to the request state.\n\nError handling is performed by setting appropriate HTTP status codes and returning informative messages for frontend handling.\nAll endpoints expect and return JSON payloads.\n\nDependencies:\n\nFastAPI for API routing and response handling.\njose for JWT encoding/decoding.\nasyncpg for async PostgreSQL operations.\nCustom modules for user models, authentication services, and configuration.\nThis controller is intended to be used as part of the FastAPI application and imported into the main app router. \"\"\"\n\nfrom fastapi import APIRouter, Response, Request\nfrom fastapi.responses import JSONResponse\nfrom jose import jwt, JWTError\nfrom services.auth_service import AuthService\nfrom repositories.user_repository import UserRepository\nfrom models.user_model import User, UserCode, EmailRequest\nfrom config.secrets import DATABASE_URL, JWT_SECRET, ALGORITHM, COOKIE_KEY\nimport asyncpg\n\n\nrouter = APIRouter()\nuser_repository = UserRepository(DATABASE_URL)\nauth_service = AuthService(user_repository)\n\n@router.post(\"/api/auth/register\")\nasync def register(user: User, response: Response):\n    \"\"\"\n    Registers a new user and sends a confirmation email.\n\n    Attempts to create a new user account with the provided email and password.\n    If successful, sends a confirmation email with a verification code.\n    Handles duplicate email registration and unexpected errors.\n\n    Args:\n        user (User): The user registration data (email and password).\n        response (Response): FastAPI response object for setting status codes.\n\n    Returns:\n        dict: JSON message indicating success or the reason for failure.\n\n    NOTE: email can raise fastapi_mail.errors.ConnectionErrors for SMTP connection issues,\n          or some other errors related to email sending.\n    \"\"\"\n    try:\n        await auth_service.register_user(user.email.lower(), user.password)\n        return {\"message\": f\"Confirmation email sent to address {user.email.lower()}.\"}\n    except asyncpg.UniqueViolationError:\n        print(f\"Email already registered: {user.email.lower()}\")\n        response.status_code = 400\n        return {\"message\": \"Email already registered.\"}\n    except Exception as e:\n        print(f\"Error during registration: {str(e)}\")\n        response.status_code = 500\n        return {\"error\": \"An error occurred during registration.\"}\n\n@router.post(\"/api/auth/login\")\nasync def login(user: User, response: Response):\n    \"\"\"\n    Authenticates a user and issues a JWT token as an HTTP-only cookie.\n\n    Verifies the user's email and password. Checks if the user's email is verified.\n    If authentication is successful, sets a JWT token in a secure cookie.\n    Handles incorrect credentials and unverified email cases.\n\n    Args:\n        user (User): The user login data (email and password).\n        response (Response): FastAPI response object for setting cookies and status codes.\n\n    Returns:\n        dict: JSON message indicating the result of the login attempt.\n    \"\"\"\n    db_user = await auth_service.authenticate_user(user.email.lower(), user.password)\n    if not db_user:\n        # SUGGESTION TO JUHO:\n        # dont't raise exceptions, just return set status code and return a message\n        # and we can handle it in the front\n        #raise HTTPException(status_code=401, detail=\"Incorrect email or password.\")\n        response.status_code = 401\n        return {\"message\": \"Incorrect email or password.\"}\n    \n    if not db_user[\"is_verified\"]:\n        print(f\"Email not verified: {user.email.lower()}\")\n        response.status_code = 401\n        return {\"message\": \"Email not verified.\"}\n\n    payload = {\"email\": db_user[\"email\"], \"role\": db_user[\"role\"]}\n    token = auth_service.create_access_token(payload)\n    response.set_cookie(key=COOKIE_KEY,\n                        value=token,\n                        httponly=True, samesite=\"lax\",\n                        domain=\"localhost\", path=\"/\",\n                        secure=False)\n\n    return {\"message\": \"Welcome!\"}\n\n\n@router.get(\"/api/auth/logout\")\nasync def logout(response: Response):\n    \"\"\"\n    Logs out the current user by deleting the authentication cookie.\n\n    Removes the JWT authentication cookie from the client to end the session.\n\n    Args:\n        response (Response): FastAPI response object for deleting cookies.\n\n    Returns:\n        dict: JSON message confirming successful logout.\n    \"\"\"\n    response.delete_cookie(\n        key=COOKIE_KEY,\n        path=\"/\",\n        domain=\"localhost\",\n    )\n    return {\"message\": \"User has successfully logged out\"}\n\n@router.post(\"/api/auth/verify\")\nasync def verify(user_code: UserCode, response: Response):\n    \"\"\"\n    Verifies a user's email address using a verification code.\n\n    Checks the provided verification code against the stored code for the user.\n    If valid, marks the user's email as verified. Handles invalid or expired codes.\n\n    Args:\n        user_code (UserCode): The user's email and verification code.\n        response (Response): FastAPI response object for setting status codes.\n\n    Returns:\n        dict: JSON message indicating the result of the verification attempt.\n    \"\"\"\n    try:\n        await auth_service.verify_user(user_code.email.lower(), user_code.code)\n        return {\"message\": \"Email verified successfully.\"}\n    except Exception as e:\n        print(f\"Error during verification: {str(e)}\")\n        response.status_code = 400\n        return {\"message\": \"Verification failed.\"}\n\n\n@router.post(\"/api/auth/resend\")\nasync def resend_verification_code(request: EmailRequest, response: Response):\n    \"\"\"\n    Resends a new email verification code to the user's email address.\n\n    Used when the user did not receive or lost the original verification code.\n    Handles errors such as invalid email addresses.\n\n    Args:\n        request (EmailRequest): The user's email address.\n        response (Response): FastAPI response object for setting status codes.\n\n    Returns:\n        dict: JSON message indicating whether the code was resent successfully.\n\n    NOTE: email can raise fastapi_mail.errors.ConnectionErrors for SMTP connection issues,\n          or some other errors related to email sending.\n    \"\"\"\n    try:\n        await auth_service.update_verification_code(request.email.lower())\n        return {\"message\": \"Verification code resent successfully.\"}\n    except Exception as e:\n        print(f\"Error during resending verification code: {str(e)}\")\n        response.status_code = 400\n        return {\"message\": \"Failed to resend verification code.\"}\n\n\n                                   \ndef create_jwt_middleware(public_routes):\n    \"\"\"\n    Creates a FastAPI middleware for validating JWT tokens on protected routes.\n\n    Extracts the JWT token from cookies, decodes and verifies it, and attaches\n    the user payload to the request state. Skips validation for public routes and\n    handles preflight (OPTIONS) requests. Returns a 401 error if the token is\n    missing or invalid.\n\n    Args:\n        public_routes (list): List of route paths that do not require authentication.\n\n    Returns:\n        Callable: The JWT validation middleware function.\n    \"\"\"\n    async def jwt_middleware(request: Request, call_next):\n        # Accept preflight requests\n        if request.method == \"OPTIONS\":\n            return await call_next(request)\n\n        # Skip validation for public routes\n        if request.url.path in public_routes:\n            return await call_next(request)\n\n        # Extract token from cookies\n        token = request.cookies.get(COOKIE_KEY)\n        if not token:\n            return JSONResponse(status_code=401, content={\"message\": \"No token found!\"})\n\n        # Validate the token\n        try:\n            payload = jwt.decode(token, JWT_SECRET, algorithms=[ALGORITHM])\n            request.state.user = payload\n        except JWTError:\n            return JSONResponse(status_code=401, content={\"message\": \"Invalid token!\"})\n\n        # Proceed to the next middleware or route handler\n        return await call_next(request)\n\n    return jwt_middleware",
  "./App/python-server/controllers/data_controller.py": "\"\"\"\ndata_controller.py\n\nThis module defines the FastAPI routes for the Eprice backend service. It provides API endpoints for retrieving\nand querying electricity production, consumption, wind power, and price data. The endpoints fetch data from\nFingrid and Porssisähkö APIs, and return results as Pydantic models or error responses.\n\nRoutes:\n    - /api/windpower\n    - /api/windpower/range\n    - /api/consumption\n    - /api/consumption/range\n    - /api/production\n    - /api/production/range\n    - /api/price/range\n    - /api/public/data\n    - /api/data/today\n\"\"\"\n\nfrom fastapi import APIRouter\nfrom fastapi.responses import JSONResponse\nfrom typing import List\nfrom services.data_service import FingridDataService, PriceDataService\nfrom models.data_model import FingridDataPoint, TimeRangeRequest, PriceDataPoint, ErrorResponse\nfrom fastapi import HTTPException\n\nrouter = APIRouter()\nfingrid_data_service = FingridDataService()\nprice_data_service = PriceDataService()\n\n\n\n@router.get(\"/api/windpower\", response_model=FingridDataPoint, responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_windpower():\n    \"\"\"\n    Get wind power production forecast.\n\n    Fetches forecast data from Fingrid dataset ID 245.\n\n    Returns:\n        FingridDataPoint | JSONResponse: A wind power data point or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data(dataset_id=245)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/windpower/range\", response_model=List[FingridDataPoint],\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def post_windpower_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get wind power production data for a given time range.\n\n    Fetches data from Fingrid dataset ID 245.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time in RFC 3339 format.\n\n    Returns:\n        List[FingridDataPoint] | JSONResponse: List of wind power data points or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data_range(\n            dataset_id=245,\n            start_time=time_range.startTime,\n            end_time=time_range.endTime,\n)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.get(\"/api/consumption\",\n    response_model=FingridDataPoint,\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_consumption():\n    \"\"\"\n    Get electricity consumption forecast.\n\n    Fetches consumption data from Fingrid dataset ID 165.\n\n    Returns:\n        FingridDataPoint | JSONResponse: A consumption data point or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data(dataset_id=165)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/consumption/range\", response_model=List[FingridDataPoint],\n             responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def post_consumption_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get electricity consumption data for a given time range.\n\n    Fetches consumption data from Fingrid dataset ID 165.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time in RFC 3339 format.\n\n    Returns:\n        List[FingridDataPoint] | JSONResponse: List of consumption data points or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data_range(\n            dataset_id=165,\n            start_time=time_range.startTime,\n            end_time=time_range.endTime\n        )\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.get(\"/api/production\",\n    response_model=FingridDataPoint,\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_production():\n    \"\"\"\n    Get electricity production forecast.\n\n    Fetches production data from Fingrid dataset ID 241.\n\n    Returns:\n        FingridDataPoint | JSONResponse: A production data point or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data(dataset_id=241)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/production/range\", response_model=List[FingridDataPoint],\n            responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def post_production_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get electricity production data for a given time range.\n\n    Fetches production data from Fingrid dataset ID 241.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time in RFC 3339 format.\n\n    Returns:\n        List[FingridDataPoint] | JSONResponse: List of production data points or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data_range(\n            dataset_id=241,\n            start_time=time_range.startTime,\n            end_time=time_range.endTime\n        )\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/price/range\")\nasync def post_price_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get price data for a specific time range from the Porssisahko API.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time as datetime objects.\n\n    Returns:\n        List[PriceDataPoint] | JSONResponse: List of price data points or an error message.\n    \"\"\"\n    try:\n        return await price_data_service.price_data_range(\n            time_range.startTime, time_range.endTime\n        )\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        print(e)\n        return JSONResponse({\"error\":\"InternalServerError\", \"message\": str(e)})\n\n\n\n@router.get(\n    \"/api/public/data\",\n    response_model=List[PriceDataPoint],\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_prices():\n    \"\"\"\n    Retrieve the latest 48 hours of electricity price data.\n\n    Returns:\n        List[PriceDataPoint] | JSONResponse: List of the latest price data points or an error message.\n    \"\"\"\n    try:\n        return await price_data_service.price_data_latest()\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.get(\n    \"/api/data/today\",\n    response_model=List[PriceDataPoint],\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_prices_today():\n    \"\"\"\n    Retrieve today's electricity price data for Finland (Europe/Helsinki).\n\n    Returns:\n        List[PriceDataPoint] | JSONResponse: List of today's price data points or an error message.\n    \"\"\"\n    try:\n        return await price_data_service.price_data_today()\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})",
  "./App/python-server/ext_apis/ext_apis.py": "\"\"\"\next_apis.py\n\nThis module provides service classes for fetching electricity production, consumption, and price data\nfrom external APIs (Fingrid and Porssisähkö). It handles API requests, rate limiting, retries, error handling,\nand conversion of API responses into application models used by the backend.\n\nClasses:\n    - FetchFingridData: Fetches production and consumption data from the Fingrid API.\n    - FetchPriceData: Fetches electricity price data from the Porssisähkö API.\n\"\"\"\n\nfrom datetime import datetime, timezone, timedelta\nimport httpx\nfrom urllib.parse import urlencode\nfrom typing import List\nfrom dotenv import load_dotenv\nimport os\nfrom models.data_model import *\nfrom zoneinfo import ZoneInfo\nfrom models.data_model import PriceDataPoint\nfrom fastapi import HTTPException\nimport asyncio\n\n\nload_dotenv(dotenv_path=\"./.env.local\")\nFINGRID_API_KEY = os.getenv(\"FINGRID_API_KEY\")\n\nclass FetchFingridData:\n    \"\"\"\n    Service for fetching electricity production and consumption data from the Fingrid API.\n\n    Provides methods to fetch the latest data point or a range of data points for a given Fingrid dataset.\n    Handles API rate limiting, retries on failure, and parsing of the Fingrid API response into application models.\n    \"\"\"\n\n    base_url = \"https://data.fingrid.fi/api/datasets/\"\n\n    def __init__(self):\n        self._lock = asyncio.Lock()\n        self._last_call_time: datetime | None = None\n        self._sleep_time = 1.5\n\n    async def _rate_limiter(self):\n        async with self._lock:\n            if self._last_call_time:\n                elapsed = (datetime.now(timezone.utc)- self._last_call_time).total_seconds()\n                if elapsed < self._sleep_time:\n                    await asyncio.sleep(self._sleep_time - elapsed)\n            self._last_call_time = datetime.now(timezone.utc)\n\n    async def fetch_fingrid_data(self, dataset_id: int) -> FingridDataPoint:\n        \"\"\"\n        Fetch the latest data point for a given Fingrid dataset ID.\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n\n        Returns:\n            FingridDataPoint: The closest data point to the current time.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        await self._rate_limiter() \n        url = f\"{self.base_url}{dataset_id}/data\"\n        headers = {}\n        if FINGRID_API_KEY is not None:\n            headers[\"x-api-key\"] = FINGRID_API_KEY\n        max_retries = 3\n        retry_delay = 3\n\n        for attempt in range(max_retries):\n            try:\n                async with httpx.AsyncClient() as client:\n                    # Remove any None values from headers to satisfy type checker\n                    clean_headers = {k: v for k, v in headers.items() if v is not None}\n                    response = await client.get(url, headers=clean_headers)\n                    response.raise_for_status()\n                    full_data = response.json()\n                    data = full_data.get(\"data\", [])\n\n                    if not data:\n                        raise ValueError(\"No data available from Fingrid API\")\n\n                    now = datetime.now(timezone.utc)\n\n                    def time_diff(item):\n                        start = datetime.fromisoformat(item[\"startTime\"].replace(\"Z\", \"+00:00\"))\n                        end = datetime.fromisoformat(item[\"endTime\"].replace(\"Z\", \"+00:00\"))\n                        return min(abs(start - now), abs(end - now))\n\n                    closest_item = min(data, key=time_diff)\n                    closest_item.pop(\"datasetId\", None)\n                    return FingridDataPoint(**closest_item)\n\n            except httpx.HTTPStatusError as exc:\n                if attempt == max_retries - 1:\n                    raise HTTPException(\n                        status_code=exc.response.status_code,\n                        detail=f\"HTTP error while fetching data for dataset {dataset_id} from Fingrid API. Number of attempts: {attempt +1}\"\n                    ) from exc\n                await asyncio.sleep(retry_delay)\n\n            except Exception as e:\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Unexpected error fetching data for dataset {dataset_id} from Fingrid API.\"\n                ) from e\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to fetch data for dataset {dataset_id} from Fingrid API after {max_retries} attempts.\"\n        )\n        \n\n    async def fetch_fingrid_data_range(self, dataset_id: int, start_time: datetime, end_time: datetime) -> List[FingridDataPoint]:\n        \"\"\"\n        Fetch a list of data points for a given Fingrid dataset ID and time range.\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n            start_time (datetime): Start time in UTC.\n            end_time (datetime): End time in UTC.\n\n        Returns:\n            List[FingridDataPoint]: List of data points for the specified range.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        headers = {\"x-api-key\": FINGRID_API_KEY} if FINGRID_API_KEY is not None else {}\n        # Remove any None values from headers to satisfy type checker\n        headers = {k: v for k, v in headers.items() if v is not None}\n        url = f\"{self.base_url}{dataset_id}/data\"\n        max_retries = 3\n        retry_delay = 1\n\n            \n        query_params = {\n            \"startTime\": start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"endTime\": end_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"sortBy\": \"startTime\",\n            \"sortOrder\": \"asc\",\n            \"pageSize\": \"20000\"\n        }\n\n        url = f\"{url}?{urlencode(query_params)}\"\n        for attempt in range(max_retries):\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(url, headers=headers)\n                    response.raise_for_status()\n                    full_data = response.json()\n                    data = full_data.get(\"data\", [])\n                    for item in data:\n                        item.pop(\"datasetId\", None)\n                    return [FingridDataPoint(**item) for item in data]\n            except httpx.HTTPStatusError as exc:\n                if attempt == max_retries - 1:\n                    raise HTTPException(\n                        status_code=exc.response.status_code,\n                        detail=f\"HTTP error while fetching data for dataset {dataset_id} from Fingrid API. Number of attempts: {attempt +1}\"\n                    ) from exc\n                await asyncio.sleep(retry_delay)\n\n            except Exception as e:\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Unexpected error fetching data for dataset {dataset_id} from Fingrid API.\"\n                ) from e\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to fetch data for dataset {dataset_id} from Fingrid API after {max_retries} attempts.\"\n        )\n\nclass FetchPriceData:\n    \"\"\"\n    Service for fetching electricity price data from the Porssisähkö API.\n\n    Provides methods to fetch price data for a specific time range, the latest prices, or today's prices.\n    Handles API requests, error handling, and conversion of API responses into application models.\n    \"\"\"\n\n    base_url = \"https://api.porssisahko.net/v1/price.json\"\n\n    async def fetch_price_data_range(self, start_time: datetime, end_time: datetime):\n        \"\"\"\n        Fetch hourly electricity price data for a given time range from the Porssisähkö API.\n\n        Args:\n            start_time (datetime): Start time in UTC.\n            end_time (datetime): End time in UTC.\n\n        Returns:\n            list[dict]: A list of dictionaries with 'startDate' (ISO8601 UTC string) and 'price' (float) for each hour.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        result = []\n        current_time_utc = start_time\n        current_time_helsinki = start_time.astimezone(ZoneInfo(\"Europe/Helsinki\"))\n        end_time_helsinki = end_time.astimezone(ZoneInfo(\"Europe/Helsinki\"))\n\n        while current_time_helsinki <= end_time_helsinki:\n            date_str = current_time_helsinki.strftime(\"%Y-%m-%d\")\n            hour_str = current_time_helsinki.strftime(\"%H\")\n\n            query_params = {\n                \"date\": date_str,\n                \"hour\": hour_str\n            }\n\n            url = f\"{self.base_url}?{urlencode(query_params)}\"\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(url)\n                    response.raise_for_status()\n\n                    data = response.json()\n                    if not data:\n                        raise ValueError(f\"No price data returned for {date_str} {hour_str}\")\n\n\n                    result.append({\n                        \"startDate\": current_time_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n                        \"price\": data[\"price\"]\n                    })\n            except httpx.HTTPStatusError as exc:\n                raise HTTPException(\n                    status_code=exc.response.status_code,\n                    detail=f\"HTTP error while fetching price data from Porssisahko: {exc.response.status_code}: {exc.response.text}\"\n                ) from exc\n            except Exception as e:\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Unexpected error occurred while fetching data from Porssisahko: {str(e)}\"\n                ) from e\n\n            current_time_helsinki += timedelta(hours=1)\n            current_time_utc += timedelta(hours=1)\n\n        return result\n\n\n    async def fetch_price_data_latest(self):\n        \"\"\"\n        Fetch the latest hourly electricity prices from the Porssisähkö API.\n\n        Returns:\n            List[PriceDataPoint]: A list of PriceDataPoint instances containing hourly electricity price data.\n                The 'endDate' key is removed from each dictionary.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        url = \"https://api.porssisahko.net/v1/latest-prices.json\"\n\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()[\"prices\"]\n\n                for item in data:\n                    item.pop(\"endDate\", None)\n\n                return [PriceDataPoint(**item) for item in data]\n\n        except httpx.HTTPStatusError as exc:\n            raise HTTPException(\n                status_code=exc.response.status_code,\n                detail=f\"HTTP error while fetching latest price data from Porssisahko: {exc.response.status_code} - {exc.response.text}\"\n            ) from exc\n        except Exception as e:\n            raise HTTPException(\n                status_code=500,\n                detail=f\"Unexpected error while fetching latest price data from Porssisahko: {str(e)}\"\n            ) from e\n\n\n\n    async def fetch_price_data_today(self) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch today's electricity price data and return it as a list of PriceDataPoint models.\n\n        The data is filtered so that only prices for the current day in Europe/Helsinki timezone are returned.\n\n        Returns:\n            List[PriceDataPoint]: A list of price data points for today.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        data = await self.fetch_price_data_latest()\n\n        # Get the current date in Finland's timezone\n        now_fi = datetime.now(ZoneInfo(\"Europe/Helsinki\"))\n        today_fi = now_fi.date()\n\n        # Filter data for today's date\n        \n        filtered_data = [item for item in data if item.startDate.astimezone(ZoneInfo(\"Europe/Helsinki\")).date() == today_fi]\n\n        # Convert filtered data to PriceDataPoint models\n        return [PriceDataPoint(**item.dict()) for item in filtered_data]\n",
  "./App/python-server/main.py": "\"\"\"\nmain.py initializes and configures the FastAPI application for the Eprice backend.\n\nFeatures:\n- Sets up application lifespan events for startup and shutdown, including checking and inserting missing price data on startup.\n- Registers custom exception handlers for request validation errors.\n- Configures CORS middleware for frontend and test environments.\n- Includes routers for authentication and external API endpoints.\n- Adds JWT authentication middleware for protected routes.\n- Integrates scheduled tasks and ensures graceful shutdown of background schedulers.\n\nDependencies:\n- fastapi for API framework and routing.\n- fastapi.middleware.cors for CORS configuration.\n- controllers for API route definitions.\n- scheduled_tasks for background data synchronization.\n- config for application and secret settings.\n- models.custom_exception for custom error handling.\n\nIntended Usage:\n- Entry point for running the Eprice backend server.\n- Should be run with a compatible ASGI server (e.g., uvicorn).\n\"\"\"\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom controllers.auth_controller import router as auth_router\nfrom controllers.auth_controller import create_jwt_middleware\nfrom controllers.data_controller import router as external_api_router\n\nfrom scheduled_tasks.porssisahko_scheduler import shutdown_scheduler, fetch_and_insert_missing_porssisahko_data\n\nimport config\nfrom config.secrets import public_routes\n\nfrom models.custom_exception import custom_validation_exception_handler\nfrom fastapi.exceptions import RequestValidationError\n\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Lifespan event handler for the FastAPI application.\n    This function is called when the application starts up and shuts down.\n    It is used to perform startup tasks, such as checking for missing data.\n    On shutdown, it ensures scheduled tasks are properly terminated.\n\n    Args:\n        app (FastAPI): The FastAPI application instance.\n    \"\"\"\n\n    # Startup code\n    print(\"Server is starting... Checking for missing data.\")\n    start_datetime = \"2025-05-13T23:00:00\"\n    await fetch_and_insert_missing_porssisahko_data(start_datetime)\n    print(\"Server started and missing data checked.\")\n    yield\n    # Shutdown code\n    shutdown_scheduler()\n\napp = FastAPI(lifespan=lifespan)\napp.add_exception_handler(RequestValidationError, custom_validation_exception_handler)\napp.include_router(external_api_router)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"https://localhost:5173\",\n                   \"http://localhost:5173\",\n                   \"http://testserver\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.include_router(auth_router)\napp.middleware(\"http\")(create_jwt_middleware(public_routes))\n",
  "./App/python-server/models/custom_exception.py": "\"\"\"\ncustom_exception.py\n\nThis module defines a custom exception handler for FastAPI request validation errors.\nIt provides a user-friendly JSON error response when request data fails validation.\n\"\"\"\n\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\nfrom fastapi import Request\nfrom fastapi.exception_handlers import request_validation_exception_handler\n\n# Customized handler for RequestValidationError errors\nasync def custom_validation_exception_handler(request: Request, exc: RequestValidationError):\n    \"\"\"\n    Custom handler for FastAPI RequestValidationError.\n\n    Args:\n        request (Request): The incoming FastAPI request.\n        exc (RequestValidationError): The validation exception raised by FastAPI.\n\n    Returns:\n        JSONResponse: A JSON response with error details and HTTP status 422.\n    \"\"\"\n    status_code = 422\n    if exc.errors():\n        error_detail = exc.errors()[0]\n        # Extracting the first error detail\n        loc_list = error_detail.get(\"loc\", [])\n        str_parts = [str(loc) for loc in loc_list]\n        loc_str = \".\".join(str_parts)\n\n        msg = error_detail.get(\"msg\", \"Validation error\")\n        type_str = error_detail.get(\"type\", \"validation_error\")\n\n        error_message = (\n            f\"{msg}.\"\n        )\n\n    else:\n        error_message = f\"Validation failed for unknown reason.\"\n\n    return JSONResponse(\n        status_code=status_code,\n        content={\n            \"error\": \"RequestValidationError\",\n            \"message\": error_message\n        }\n    )",
  "./App/python-server/models/data_model.py": "\"\"\"\ndata_model.py\n\nThis module defines Pydantic data models used throughout the Eprice backend service.\nIt includes models for time ranges, Fingrid and price data points, error responses,\nand utility base classes for datetime validation.\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nfrom pydantic import BaseModel, field_validator\nfrom datetime import datetime\n\nclass DateTimeValidatedModel(BaseModel):\n    \"\"\"\n    Base model that validates datetime fields to ensure they are in ISO 8601 format.\n    Fields validated: 'startTime', 'endTime', 'timestamp', 'startDate'.\n    \"\"\"\n    @classmethod\n    @field_validator('startTime', 'endTime', 'timestamp', 'startDate', mode='before')\n    def validate_datetime(cls, v):\n        \"\"\"\n        Validates that the provided value is a valid ISO 8601 datetime string.\n        \"\"\"\n        datetime.fromisoformat(v.replace(\"Z\", \"+00:00\"))\n        return v\n\nclass StartDateModel(BaseModel):\n    \"\"\"\n    Model containing a single startDate field as a datetime object.\n    \"\"\"\n    startDate: datetime\n\nclass TimeRange(DateTimeValidatedModel):\n    \"\"\"\n    Model representing a time range with start and end times.\n\n    Attributes:\n        startTime (datetime): Start time in RFC 3339 format.\n        endTime (datetime): End time in RFC 3339 format.\n    \"\"\"\n    startTime: datetime = Field(\n        description=\"Start time in RFC 3339 format (e.g., 2024-05-01T00:00:00Z)\",\n        examples=[\"2024-05-01T00:00:00Z\"]\n    )\n    endTime: datetime = Field(\n        examples=[\"2024-05-02T00:00:00Z\"],\n        description=\"End time in RFC 3339 format (e.g., 2024-05-02T00:00:00Z)\"\n    )\n\nclass TimeRangeRequest(TimeRange):\n    \"\"\"\n    Request model for endpoints requiring a time range.\n    Provides helper methods to ensure start and end times are returned as datetime objects.\n    \"\"\"\n    def start_datetime(self) -> datetime:\n        \"\"\"\n        Returns the start time as a datetime object, parsing from string if necessary.\n\n        Returns:\n            datetime: The start time as a datetime object.\n        \"\"\"\n        if isinstance(self.startTime, str):\n            return datetime.fromisoformat(str(self.startTime).replace(\"Z\", \"+00:00\"))\n        elif isinstance(self.startTime, datetime):\n            return self.startTime\n        else:\n            raise TypeError(\"startTime must be a string or datetime object\")\n\n    def end_datetime(self) -> datetime:\n        \"\"\"\n        Returns the end time as a datetime object, parsing from string if necessary.\n\n        Returns:\n            datetime: The end time as a datetime object.\n        \"\"\"\n        if isinstance(self.endTime, str):\n            return datetime.fromisoformat(str(self.endTime).replace(\"Z\", \"+00:00\"))\n        elif isinstance(self.endTime, datetime):\n            return self.endTime\n        else:\n            raise TypeError(\"endTime must be a string or datetime object\")\n\nclass FingridDataPoint(TimeRange):\n    \"\"\"\n    Model representing a single data point from Fingrid, including a value and time range.\n\n    Attributes:\n        value (float): Value of the data point, must be non-negative.\n    \"\"\"\n    value: float = Field(\n        examples=[7883.61],\n        description=\"Value of the data point\"\n    )\n\n    @field_validator(\"value\")\n    def validate_value_positive(cls, v):\n        \"\"\"\n        Validates that the value is non-negative.\n\n        Raises:\n            ValueError: If the value is negative.\n        \"\"\"\n        if v < 0:\n            raise ValueError(\"value must be non-negative\")\n        return v\n\nclass PriceDataPoint(StartDateModel):\n    \"\"\"\n    Model representing a single electricity price data point.\n\n    Attributes:\n        startDate (datetime): Start time of the price data point in UTC.\n        price (float): Price in euro cents.\n    \"\"\"\n    startDate: datetime = Field(\n        description=\"UTC str in RFC 3339 format\",\n        examples=[\"2025-05-08T04:00:00.000Z\"]\n    )\n    price: float = Field(\n        description=\"Floating-point number representing the price in euro cents\",\n        examples=[0.61]\n    )\n\nclass ErrorResponse(BaseModel):\n    \"\"\"\n    Model for error responses returned by the API.\n\n    Attributes:\n        error (str): Error message describing the issue.\n    \"\"\"\n    error: str = Field(\n        description=\"Error message describing the issue.\",\n        examples=[\"An error occurred\"]\n    )\n\n",
  "./App/python-server/models/user_model.py": "\"\"\"\nuser_model.py defines Pydantic models for user-related data validation in the Eprice backend.\n\nModels:\n- EmailRequest: Validates and represents an email address for requests such as verification code resending.\n- UserCode: Validates an email and a verification code, ensuring the code matches the required format (ABC-123).\n- User: Validates user registration and login data, enforcing email format and password strength.\n\nAll models use Pydantic for type validation and include custom field validators for additional constraints.\n\nIntended Usage:\n- Used in FastAPI route handlers for request body validation.\n- Ensures consistent and secure data formats for authentication and user management endpoints.\n\"\"\"\nfrom pydantic import BaseModel, EmailStr, field_validator, ValidationError\nfrom pydantic_core import PydanticCustomError\nfrom typing import Optional\nimport re\n\nclass EmailRequest(BaseModel):\n    \"\"\"\n    Pydantic model for validating an email address in request bodies.\n    This can be extended with additional validation rules as needed.\n\n    Attributes:\n        email (EmailStr): The user's email address.\n    \n    Raises:\n        ValidationError: If the email address is not valid.\n        Custom exception handler is set up in the main(entrypoint) file.\n    \"\"\"\n    email: EmailStr\n\nclass UserCode(BaseModel):\n    \"\"\"\n    Pydantic model for validating an email and verification code.\n\n    Attributes:\n        email (EmailStr): The user's email address.\n        code (str): The verification code in format ABC-123.\n    \"\"\"\n    email: EmailStr\n    code: str\n\n    @field_validator('code')\n    def validate_code(cls, code: str) -> str:\n        \"\"\"\n        Validate that the verification code matches the required format (ABC-123).\n\n        Args:\n            code (str): The verification code to validate.\n\n        Returns:\n            str: The validated code.\n\n        Raises:\n            ValidationError: If the code does not match the required format.\n            Custom exception handler is set up in the main(entrypoint) file.\n        \"\"\"\n        pattern = r'^[A-Z]{3}-\\d{3}$'\n        if not re.match(pattern, code):\n            assert 0>10, \"Invalid code format. Expected format: ABC-123\"\n        return code\n\nclass User(BaseModel):\n    \"\"\"\n    Pydantic model for validating user registration and login data.\n\n    Attributes:\n        email (EmailStr): The user's email address.\n        password (str): The user's password.\n    \"\"\"\n    email: EmailStr\n    password: str\n\n    @field_validator('password')\n    def validate_password(cls, password: str) -> str:\n        \"\"\"\n        Validate that the password meets minimum strength requirements.\n\n        Args:\n            password (str): The password to validate.\n\n        Returns:\n            str: The validated password.\n\n        Raises:\n            ValidationError: If the password does not meet the minimum length requirement.\n            Custom exception handler is set up in the main(entrypoint) file.\n        \"\"\"\n        assert len(password) >= 4, \"Password must be at least 4 characters long\"\n        return password",
  "./App/python-server/repositories/porssisahko_repository.py": "\"\"\"\nporssisahko_repository.py defines the PorssisahkoRepository class for price data operations in the Eprice backend.\n\nThe repository provides asynchronous methods for:\n- Inserting single or multiple price entries into the porssisahko table.\n- Retrieving entries within a date range.\n- Finding missing hourly entries within a date range.\n\nAll operations interact directly with a PostgreSQL database using asyncpg for asynchronous access.\nThis repository is intended to be used by service and controller layers to abstract database logic\nfrom business and API logic.\n\nDependencies:\n- asyncpg for asynchronous PostgreSQL operations.\n- utils.porssisahko_tools for entry conversion utilities.\n\nIntended Usage:\n- Instantiate with a database connection URL.\n- Use in services or controllers for all price data-related database actions.\n\"\"\"\n\nimport asyncpg\nfrom utils.porssisahko_tools import convert_to_porssisahko_entry\nfrom datetime import datetime\n\nclass PorssisahkoRepository:\n    \"\"\"\n    Repository class for price data operations in the Eprice backend.\n\n    Provides asynchronous methods for inserting and retrieving price entries,\n    as well as finding missing entries. Interacts directly with the PostgreSQL\n    database using asyncpg.\n\n    Args:\n        database_url (str): The database connection URL.\n    \"\"\"\n    def __init__(self, database_url: str):\n        \"\"\"\n        Initialize the PorssisahkoRepository with a database connection URL.\n\n        Args:\n            database_url (str): The database connection URL.\n        \"\"\"\n        self.database_url = database_url\n\n    async def insert_entry(self, price: float, iso_date: str, predicted: bool = False):\n        \"\"\"\n        Insert a single entry into the porssisahko table.\n\n        Args:\n            price (float): The price value.\n            iso_date (str): The date in ISO 8601 format.\n            predicted (bool): Indicates if the price is predicted. Default is False.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Convert the entry to the correct format\n            entry = convert_to_porssisahko_entry(price, iso_date, predicted)\n\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Insert the entry into the database\n            await conn.execute(\n                \"\"\"\n                INSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\n                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n                ON CONFLICT (Datetime) DO NOTHING\n                \"\"\",\n                entry[\"datetime\"],\n                entry[\"date\"],\n                entry[\"year\"],\n                entry[\"month\"],\n                entry[\"day\"],\n                entry[\"hour\"],\n                entry[\"weekday\"],\n                entry[\"price\"]\n            )\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n    async def insert_entries(self, entries: list):\n        \"\"\"\n        Insert multiple entries into the porssisahko table.\n\n        Args:\n            entries (list[dict]): A list of dictionaries containing the data to insert.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Convert entries to the correct format\n            formatted_entries = [\n                convert_to_porssisahko_entry(entry[\"price\"], entry[\"startDate\"], predicted=entry.get(\"predicted\", False))\n                for entry in entries\n            ]\n\n            # Prepare the insert query\n            insert_query = \"\"\"\n                INSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\n                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n                ON CONFLICT (Datetime) DO NOTHING\n            \"\"\"\n            # Create a list of tuples for the entries\n            values = [\n                (\n                    entry[\"datetime\"],\n                    entry[\"date\"],\n                    entry[\"year\"],\n                    entry[\"month\"],\n                    entry[\"day\"],\n                    entry[\"hour\"],\n                    entry[\"weekday\"],\n                    entry[\"price\"]\n                )\n                for entry in formatted_entries\n            ]\n\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the insert query with the list of values\n            await conn.executemany(insert_query, values)\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n    async def get_entries(self, start_date: datetime, end_date: datetime, select_columns: str = \"*\"):\n        \"\"\"\n        Retrieve entries from the porssisahko table between two dates.\n\n        Args:\n            start_date (datetime): The start date as a datetime object.\n            end_date (datetime): The end date as a datetime object.\n            select_columns (str): The columns to select from the table. Default is \"*\".\n\n        Returns:\n            list[dict]: A list of dictionaries containing the data for each entry.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the query\n            rows = await conn.fetch(\n                f\"\"\"\n                SELECT {select_columns}\n                FROM porssisahko\n                WHERE datetime BETWEEN $1 AND $2\n                \"\"\",\n                start_date,\n                end_date\n            )\n\n            # Convert rows to a list of dictionaries\n            return [dict(row) for row in rows]\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n    async def get_missing_entries(self, start_date: datetime, end_date: datetime):\n        \"\"\"\n        Retrieve missing hourly entries from the porssisahko table between two dates.\n\n        Args:\n            start_date (datetime): The start date as a datetime object.\n            end_date (datetime): The end date as a datetime object.\n\n        Returns:\n            list[tuple]: A list of tuples where each tuple contains the date (YYYY-MM-DD) and hour (0-23).\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the query to find missing entries\n            rows = await conn.fetch(\n                \"\"\"\n                WITH date_range AS (\n                    SELECT generate_series(\n                        $1::TIMESTAMP,\n                        $2::TIMESTAMP,\n                        '1 hour'::INTERVAL\n                    ) AS datetime\n                )\n                SELECT dr.datetime\n                FROM date_range dr\n                LEFT JOIN porssisahko p ON dr.datetime = p.datetime\n                WHERE p.datetime IS NULL\n                \"\"\",\n                start_date,\n                end_date\n            )\n\n            # Return the missing entries as a list of tuples\n            return [\n                (row[\"datetime\"].strftime(\"%Y-%m-%d\"), row[\"datetime\"].hour)\n                for row in rows\n            ]\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()",
  "./App/python-server/repositories/user_repository.py": "\"\"\"\nuser_repository.py defines the UserRepository class for user-related database operations in the Eprice backend.\n\nThe repository provides asynchronous methods for:\n- Retrieving user records by email.\n- Creating new user accounts with hashed passwords and verification codes.\n- Verifying user email addresses using verification codes.\n- Updating verification codes for users.\n\nAll operations interact directly with a PostgreSQL database using asyncpg for asynchronous access.\nThis repository is intended to be used by service and controller layers to abstract database logic\nfrom business and API logic.\n\nDependencies:\n- asyncpg for asynchronous PostgreSQL operations.\n\nIntended Usage:\n- Instantiate with a database connection URL.\n- Use in authentication and user management services for all user-related database actions.\n\"\"\"\nimport asyncpg\n\nclass UserRepository:\n    \"\"\"\n    Repository class for user-related database operations in the Eprice backend.\n\n    Provides asynchronous methods for retrieving, creating, and updating user records,\n    as well as verifying user email addresses. Interacts directly with the PostgreSQL\n    database using asyncpg.\n\n    Args:\n        database_url (str): The database connection URL.\n    \"\"\"\n    def __init__(self, database_url: str):\n        \"\"\"\n        Initialize the UserRepository with a database connection URL.\n\n        Args:\n            database_url (str): The database connection URL.\n        \"\"\"\n        self.database_url = database_url\n\n    async def get_user_by_email(self, email: str):\n        \"\"\"\n        Retrieve a user record by email address.\n\n        Args:\n            email (str): The user's email address.\n\n        Returns:\n            Record or None: The user record if found, otherwise None.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            user = await conn.fetchrow(\"SELECT * FROM users WHERE email = $1\", email)\n            return user\n        finally:\n            await conn.close()\n    \n    async def create_user(self, email: str, password_hash: str, verification_code: str):\n        \"\"\"\n        Create a new user record in the database.\n\n        Args:\n            email (str): The user's email address.\n            password_hash (str): The hashed password.\n            verification_code (str): The email verification code.\n\n        Raises:\n            Exception: If the user could not be created.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            await conn.execute(\n                \"INSERT INTO users (email, password_hash, verification_code) VALUES ($1, $2, $3)\",\n                email, password_hash, verification_code\n            )\n        finally:\n            await conn.close()\n\n    async def verify_code(self, email: str, verification_code: str):\n        \"\"\"\n        Verify a user's email address using a verification code.\n\n        Args:\n            email (str): The user's email address.\n            verification_code (str): The verification code to check.\n\n        Returns:\n            str: The result of the update operation.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            result = await conn.execute(\n                \"UPDATE users SET is_verified = TRUE WHERE email = $1 AND verification_code = $2\",\n                email, verification_code\n            )\n            return result\n        finally:\n            await conn.close()\n\n    async def update_code(self, email: str, new_code: str):\n        \"\"\"\n        Update a user's verification code.\n\n        Args:\n            email (str): The user's email address.\n            new_code (str): The new verification code.\n\n        Raises:\n            Exception: If the update operation fails.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            await conn.execute(\n                \"UPDATE users SET verification_code = $1 WHERE email = $2\",\n                new_code, email\n            )\n        finally:\n            await conn.close()\n",
  "./App/python-server/requirements.txt": "fastapi==0.115.12\nasyncpg==0.30.0\npasslib[bcrypt]==1.7.4\npython-jose==3.4.0\nuvicorn==0.34.2\npydantic[email]==2.11.4\ndotenv==0.9.9\nhttpx==0.28.1\nasyncio==3.4.3\napscheduler==3.11.0\nrequests==2.32.3\nfastapi-mail==1.4.2\n",
  "./App/python-server/scheduled_tasks/porssisahko_scheduler.py": "\"\"\"\nporssisahko_scheduler.py defines scheduled tasks for fetching and inserting electricity price data into the Eprice backend database.\n\nFeatures:\n- Periodically fetches the latest price data from the Pörssisähkö API and inserts it into the database.\n- Detects and fills missing hourly price entries by querying the API for specific dates and hours.\n- Uses APScheduler to schedule tasks at specified intervals or times.\n- Provides synchronous wrappers for running asynchronous tasks in a scheduler context.\n- Handles API and database errors with logging for monitoring and debugging.\n\nDependencies:\n- requests for HTTP requests to the external API.\n- apscheduler for scheduling background tasks.\n- repositories.porssisahko_repository for database operations.\n- config.secrets for database configuration.\n- asyncio for running async functions in a synchronous context.\n\nIntended Usage:\n- Used as part of the backend service to ensure the database is kept up-to-date with the latest and complete price data.\n- Can be extended with additional scheduled tasks or triggers as needed.\n\"\"\"\n\nimport asyncio\nimport requests\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.triggers.cron import CronTrigger\n#from apscheduler.triggers.interval import IntervalTrigger\nfrom datetime import datetime, timedelta\nfrom repositories.porssisahko_repository import PorssisahkoRepository\nfrom config.secrets import DATABASE_URL\n\n# Initialize the repository with the database URL\nporssisahko_repository = PorssisahkoRepository(DATABASE_URL)\n\n# The task to fetch data and insert it into the database\nasync def fetch_and_insert_porssisahko_data():\n    \"\"\"\n    Fetch the latest price data from the Pörssisähkö API and insert it into the database.\n\n    Raises:\n        requests.RequestException: If there is an error fetching data from the API.\n        Exception: For any unexpected errors during data insertion.\n    \"\"\"\n    try:\n        # Fetch data from the API\n        response = requests.get(\"https://api.porssisahko.net/v1/latest-prices.json\")\n        response.raise_for_status() # Raise an exception for HTTP errors\n        data = response.json()\n\n        # Insert the data into the database using the repository\n        await porssisahko_repository.insert_entries(data[\"prices\"])\n\n        print(f\"Database successfully updated at {datetime.now()}\")\n    except requests.RequestException as e:\n        print(f\"Error fetching data from the API: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n\nasync def fetch_and_insert_missing_porssisahko_data(start_datetime_str: str):\n    \"\"\"\n    Detect and insert missing hourly price entries into the database.\n\n    Args:\n        start_datetime_str (str): The ISO format string representing the start datetime.\n\n    Raises:\n        requests.RequestException: If there is an error fetching data from the API.\n        Exception: For any unexpected errors during data insertion.\n    \"\"\"\n    try:\n        # Convert the start_datetime string to a datetime object (db likes ISO format)\n        start_datetime = datetime.fromisoformat(start_datetime_str)\n        \n        # Calculate the end datetime (24 hours later)\n        end_datetime = datetime.now() + timedelta(days=1)\n        end_datetime = end_datetime.replace(minute=0, second=0, microsecond=0)\n        \n        # Retrieve missing entries from the repository\n        missing_entries = await porssisahko_repository.get_missing_entries(\n            start_datetime, end_datetime\n        )\n\n        if not missing_entries:\n            print(f\"No missing entries found between {start_datetime} and {end_datetime}.\")\n            return\n\n        print(f\"Found {len(missing_entries)} missing entries. Fetching data...\")\n\n        # Fetch data for the missing entries\n        for date, hour in missing_entries:\n            # Construct the API URL for the specific date and hour\n            api_url = f\"https://api.porssisahko.net/v1/price.json?date={date}&hour={hour}\"\n            response = requests.get(api_url)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            data = response.json()  # Parse the JSON response\n\n            # Insert the data into the database -- datetime format:  \"2022-11-14THH:00:00.000Z\"\n            await porssisahko_repository.insert_entry(data[\"price\"], f\"{date}T{hour:02d}:00.000Z\")\n\n        print(f\"Missing data successfully inserted into the database.\")\n    except requests.RequestException as e:\n        print(f\"Error fetching data from the API: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n\n# we need a wrapper to run the async task in a synchronous context\ndef fetch_and_insert_porssisahko_data_sync():\n    \"\"\"\n    Synchronous wrapper to run fetch_and_insert_porssisahko_data in an event loop.\n    \"\"\"\n    asyncio.run(fetch_and_insert_porssisahko_data())\n\ndef fetch_and_insert_missing_porssisahko_data_sync(start_datetime_str: str):\n    \"\"\"\n    Synchronous wrapper to run fetch_and_insert_missing_porssisahko_data in an event loop.\n\n    Args:\n        start_datetime_str (str): The ISO format string representing the start datetime.\n    \"\"\"\n    asyncio.run(fetch_and_insert_missing_porssisahko_data(start_datetime_str))\n\n\n# Set up the scheduler\nps_scheduler = BackgroundScheduler()\n# Trigger to run the task every day at 14:15\nps_trigger = CronTrigger(hour=14, minute=15)\n\n# NOTE DEBUG: For debugging/testing purposes, you can use an interval trigger to run every 15 seconds or so\n#ps_trigger = IntervalTrigger(seconds=10)\n\nps_scheduler.add_job(fetch_and_insert_porssisahko_data_sync, ps_trigger)\nps_scheduler.start()\n\n# Ensure the scheduler shuts down properly on application exit\ndef shutdown_scheduler():\n    \"\"\"\n    Shut down the APScheduler instance gracefully on application exit.\n    \"\"\"\n    print(\"Shutting down scheduler...\")\n    ps_scheduler.shutdown()\n\n",
  "./App/python-server/services/auth_service.py": "\"\"\"\nauth_service.py defines AuthService, which provides authentication and user management logic for the Eprice backend.\n\nThis service handles password hashing and verification, JWT token creation, user authentication,\nregistration, email verification, and verification code management. It interacts with the user\nrepository for database operations and with email utilities for sending verification codes.\n\nKey Responsibilities:\n- Securely hash and verify user passwords using bcrypt.\n- Generate and validate JWT access tokens for authenticated sessions.\n- Register new users, including generating and emailing verification codes.\n- Authenticate users by verifying credentials.\n- Verify user email addresses using codes sent via email.\n- Regenerate and resend verification codes as needed.\n\nDependencies:\n- passlib for password hashing.\n- jose for JWT encoding.\n- async database repository for user data.\n- email utilities for sending verification codes.\n\nIntended Usage:\n- Instantiated with a UserRepository instance.\n- Used by FastAPI controllers to perform authentication-related operations.\n\"\"\"\n\nfrom passlib.context import CryptContext\nfrom jose import jwt\nfrom datetime import datetime, timedelta\nimport random\nimport string\nfrom repositories.user_repository import UserRepository\nfrom config.secrets import JWT_SECRET, ALGORITHM, ACCESS_TOKEN_EXPIRE_MINUTES\nfrom utils.email_tools import send_email_async\n\nclass AuthService:\n    \"\"\"\n    Service class for authentication and user management logic in the Eprice backend.\n\n    Handles password hashing and verification, JWT token creation, user authentication,\n    registration, email verification, and verification code management. Interacts with the\n    user repository for database operations and with email utilities for sending verification codes.\n\n    Args:\n        user_repository (UserRepository): The repository instance for user database operations.\n    \"\"\"\n    def __init__(self, user_repository: UserRepository):\n        \"\"\"\n        Initialize the AuthService with a user repository.\n\n        Args:\n            user_repository (UserRepository): The repository instance for user database operations.\n        \"\"\"\n        self.user_repository = user_repository\n        self.pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\n    def get_password_hash(self, password: str) -> str:\n        \"\"\"\n        Hash a plaintext password using bcrypt.\n\n        Args:\n            password (str): The plaintext password.\n\n        Returns:\n            str: The hashed password.\n        \"\"\"\n        return self.pwd_context.hash(password)\n    \n    def generate_verification_code(self):\n        \"\"\"\n        Generate a random verification code in the format ABC-123.\n\n        Returns:\n            str: The generated verification code.\n        \"\"\"\n        letters = ''.join(random.choices(string.ascii_uppercase, k=3))\n        digits = ''.join(random.choices(string.digits, k=3))\n        return f\"{letters}-{digits}\"\n\n    def verify_password(self, plain_password: str, hashed_password: str) -> bool:\n        \"\"\"\n        Verify a plaintext password against a hashed password.\n\n        Args:\n            plain_password (str): The plaintext password.\n            hashed_password (str): The hashed password.\n\n        Returns:\n            bool: True if the password matches, False otherwise.\n        \"\"\"\n        return self.pwd_context.verify(plain_password, hashed_password)\n\n    def create_access_token(self, data: dict, expires_delta: timedelta = None):\n        \"\"\"\n        Create a JWT access token for the given data.\n\n        Args:\n            data (dict): The payload data to encode in the token.\n            expires_delta (timedelta, optional): Token expiration time. Defaults to configured value.\n\n        Returns:\n            str: The encoded JWT token.\n        \"\"\"\n        to_encode = data.copy()\n        expire = datetime.utcnow() + (expires_delta or timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))\n        to_encode.update({\"exp\": expire})\n        return jwt.encode(to_encode, JWT_SECRET, algorithm=ALGORITHM)\n\n    async def authenticate_user(self, email: str, password: str):\n        \"\"\"\n        Authenticate a user by email and password.\n\n        Args:\n            email (str): The user's email address.\n            password (str): The user's plaintext password.\n\n        Returns:\n            dict or None: The user record if authentication succeeds, None otherwise.\n        \"\"\"\n        user = await self.user_repository.get_user_by_email(email)\n        if not user or not self.verify_password(password, user[\"password_hash\"]):\n            return None\n        return user\n\n    async def register_user(self, email: str, password: str):\n        \"\"\"\n        Register a new user and send a verification code via email.\n\n        Args:\n            email (str): The user's email address.\n            password (str): The user's plaintext password.\n\n        Raises:\n            Exception: If user creation fails.\n        \"\"\"\n        hashed_password = self.get_password_hash(password)\n        code = self.generate_verification_code()\n        \n        await self.user_repository.create_user(email, hashed_password, code)\n        # Only send email if user creation succeeded\n        await send_email_async(email, code)\n\n    async def verify_user(self, email: str, code: str):\n        \"\"\"\n        Verify a user's email address using a verification code.\n\n        Args:\n            email (str): The user's email address.\n            code (str): The verification code.\n\n        Raises:\n            Exception: If verification fails.\n        \"\"\"\n        result = await self.user_repository.verify_code(email, code)\n        if not result:\n            raise Exception(\"Verification failed\")\n        \n    async def update_verification_code(self, email: str):\n        \"\"\"\n        Generate and update a new verification code for the user, and send it via email.\n\n        Args:\n            email (str): The user's email address.\n\n        Raises:\n            Exception: If updating the code fails.\n        \"\"\"\n        new_code = self.generate_verification_code()\n        await self.user_repository.update_code(email, new_code)\n        # Only send email if update succeeded\n        await send_email_async(email, new_code)\n        ",
  "./App/python-server/services/data_service.py": "\"\"\"\ndata_service.py\n\nThis module provides service classes for handling data operations in the Eprice backend.\nIt includes services for fetching and processing Fingrid electricity data and price data,\ncombining information from external APIs and the database, and providing unified access\nto the application's core data models.\n\"\"\"\n\nfrom models.data_model import *\nfrom ext_apis.ext_apis import *\nfrom repositories.porssisahko_repository import *\nfrom utils.porssisahko_service_tools import *\nfrom config.secrets import DATABASE_URL\nfrom datetime import datetime\nfrom zoneinfo import ZoneInfo\n\nclass FingridDataService:\n    \"\"\"\n    Service class for fetching Fingrid data from the external API.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the FingridDataService with the external API fetcher.\n        \"\"\"\n        self.ext_api_fetcher = FetchFingridData()\n\n    async def fingrid_data(self, dataset_id: int) -> FingridDataPoint:\n        \"\"\"\n        Fetch the latest Fingrid data for a given dataset ID.\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n\n        Returns:\n            FingridDataPoint: The latest data point.\n        \n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        return await self.ext_api_fetcher.fetch_fingrid_data(dataset_id)\n    \n    async def fingrid_data_range(self, dataset_id: int, start_time: datetime, end_time: datetime) -> List[FingridDataPoint]:\n        \"\"\"\n        Fetch Fingrid data for a given dataset ID and time range.\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n            start_time (datetime): Start time in UTC.\n            end_time (datetime): End time in UTC.\n\n        Returns:\n            List[FingridDataPoint]: List of data points for the given range.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        return await self.ext_api_fetcher.fetch_fingrid_data_range(dataset_id, start_time, end_time)\n\n\nclass PriceDataService:\n    \"\"\"\n    Service class for handling price data operations, including fetching from the database and external API.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the PriceDataService with required repositories and helper services.\n        \"\"\"\n        self.ext_api_fetcher = FetchPriceData()\n        self.database_fetcher = PorssisahkoRepository(DATABASE_URL)\n        self.porssisahko_service_tools = PorssisahkoServiceTools(self.ext_api_fetcher, self.database_fetcher)\n\n    async def price_data_latest(self) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch the latest 48 hours of price data, preferring the database but falling back to the external API if needed.\n\n        Returns:\n            List[PriceDataPoint]: List of price data points for the latest 48 hours.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        start_date, end_date = self.porssisahko_service_tools.expected_time_range()\n        try:\n            result = await self.porssisahko_service_tools.fetch_and_process_data(start_date, end_date)\n            return result if result else await self.ext_api_fetcher.fetch_price_data_latest()\n        except Exception:\n            return await self.ext_api_fetcher.fetch_price_data_latest()\n\n    async def price_data_range(self, start_date: datetime, end_date: datetime) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch price data for a given time range, preferring the database but falling back to the external API if needed.\n\n        Args:\n            start_date (datetime): Start of the time range.\n            end_date (datetime): End of the time range.\n\n        Returns:\n            List[PriceDataPoint]: List of price data points for the given range.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        start_date = start_date.replace(tzinfo=None)\n        end_date = end_date.replace(tzinfo=None)\n        try:\n            result = await self.porssisahko_service_tools.fetch_and_process_data(start_date, end_date)\n            return result if result else await self.ext_api_fetcher.fetch_price_data_range(start_date, end_date)\n        except Exception:\n            return await self.ext_api_fetcher.fetch_price_data_range(start_date, end_date)\n\n    async def price_data_today(self) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch price data for the current day in Helsinki time.\n\n        Returns:\n            List[PriceDataPoint]: List of today's price data points.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        data = await self.price_data_latest()\n        if data:\n            now_fi = datetime.now(ZoneInfo(\"Europe/Helsinki\"))\n            today_fi = now_fi.date()\n            filtered_data = [item for item in data if item.startDate.astimezone(ZoneInfo(\"Europe/Helsinki\")).date() == today_fi]\n            return sorted(filtered_data, key=lambda x: x.startDate, reverse=False)\n        else:\n            return await self.ext_api_fetcher.fetch_price_data_today()\n\n\n",
  "./App/python-server/utils/email_tools.py": "\"\"\"\nemail_tools.py provides utility functions for sending emails in the Eprice backend.\n\nFeatures:\n- Asynchronous email sending using FastAPI-Mail.\n- Configures SMTP connection using environment variables from the secrets configuration.\n- Sends verification emails with a code and a direct verification link for user registration and authentication flows.\n\nDependencies:\n- fastapi_mail for asynchronous email delivery.\n- config.secrets for SMTP credentials and configuration.\n\nIntended Usage:\n- Used by authentication and user management services to send verification codes to users.\n- Can be extended for other email-related utilities as needed.\n\"\"\"\n\nfrom fastapi_mail import FastMail, MessageSchema, ConnectionConfig\nfrom config.secrets import (\n    MAIL_USERNAME,\n    MAIL_FROM,\n    MAIL_PORT,\n    MAIL_SERVER,\n    MAIL_FROM_NAME,\n    MAIL_PASSWORD\n)\n\n\nconf = ConnectionConfig(\n    MAIL_USERNAME=MAIL_USERNAME,\n    MAIL_PASSWORD=MAIL_PASSWORD,\n    MAIL_FROM=MAIL_FROM,\n    MAIL_PORT=MAIL_PORT,\n    MAIL_SERVER=MAIL_SERVER,\n    MAIL_FROM_NAME=MAIL_FROM_NAME,\n    MAIL_STARTTLS=True,      # Add this line\n    MAIL_SSL_TLS=False       # And this line (set to True if your SMTP requires SSL/TLS)\n)\n\nasync def send_email_async(email_to: str, verification_code: str):\n    '''\n    Send an email asynchronously with a verification code and a link to verify the email address.\n\n    Args:\n        email_to (str): The recipient's email address.\n        verification_code (str): The verification code to be sent in the email.\n    '''\n\n    subject = 'Verify your email address'\n    body = f'''\n    <html>\n        <body style=\"font-family: Arial, sans-serif; text-align: center;\">\n            <h1>Verify your email address</h1>\n            <p>Use the code below to verify your email address:</p>\n            <div style=\"display: inline-block; margin: 20px auto; padding: 16px 32px; background: #f3f3f3; border-radius: 8px; font-size: 2em; letter-spacing: 0.2em; font-weight: bold; color: #333;\">\n                {verification_code}\n            </div>\n            <p>Or click the link below to verify directly:</p>\n            <a href=\"http://localhost:5173/auth/verify?email={email_to}&code={verification_code}\"\n               style=\"display: inline-block; margin-top: 16px; padding: 12px 24px; background: #2563eb; color: #fff; text-decoration: none; border-radius: 6px; font-size: 1.1em;\">\n                Verify Email\n            </a>\n        </body>\n    </html>\n    '''\n    message = MessageSchema(\n        subject=subject,\n        recipients=[email_to],\n        body=body,\n        subtype='html',\n    )\n    \n    fm = FastMail(conf)\n    await fm.send_message(message, template_name='email.html')\n",
  "./App/python-server/utils/porssisahko_service_tools.py": "\"\"\"\nporssisahko_service_tools.py\n\nThis module provides helper services for handling electricity price data time ranges, \nconverting database results to application models, and filling missing data entries \nfrom external APIs. It is used to unify and process price data from both the database \nand external sources for the Eprice backend.\n\"\"\"\n\nfrom models.data_model import *\nfrom ext_apis.ext_apis import *\nfrom repositories.porssisahko_repository import *\nfrom utils.porssisahko_service_tools import *\nfrom datetime import datetime, timedelta\nfrom zoneinfo import ZoneInfo\n\n\nclass PorssisahkoServiceTools:\n    \"\"\"\n    PorssisahkoServiceTools has functions for time range calculations, data conversion, and filling missing entries.\n    \"\"\"\n\n    def __init__(self, ext_api_fetcher: FetchPriceData, database_fetcher: PorssisahkoRepository):\n        \"\"\"\n        Initialize the PorssisahkoServiceTools with external API and database fetchers.\n        \"\"\"\n        self.ext_api_fetcher = ext_api_fetcher\n        self.database_fetcher = database_fetcher\n\n    def expected_time_range(self) -> tuple[datetime, datetime]:\n        \"\"\"\n        Calculate the expected time range for the latest 48 hours based on Helsinki time.\n\n        Returns:\n            tuple[datetime, datetime]: Start and end datetimes (naive, Europe/Helsinki time).\n        \"\"\"\n        now_naive = datetime.now(ZoneInfo(\"Europe/Helsinki\")).replace(tzinfo=None)\n        if now_naive.hour >= 14:\n            end_time = (now_naive + timedelta(days=2)).replace(hour=0, minute=0, second=0, microsecond=0)\n        else:\n            end_time = (now_naive + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n        start_time = (end_time - timedelta(hours=48)).replace(tzinfo=None)\n        return start_time, end_time\n\n    def convert_to_price_data(self, data: List[dict]) -> List[PriceDataPoint]:\n        \"\"\"\n        Convert a list of database dicts to a sorted list of PriceDataPoint objects in UTC.\n\n        Args:\n            data (List[dict]): List of dicts with 'datetime' and 'price'.\n\n        Returns:\n            List[PriceDataPoint]: Sorted list of PriceDataPoint objects (startDate in UTC).\n        \"\"\"\n        if not data:\n            return []\n        return sorted([\n            PriceDataPoint(\n                startDate=item[\"datetime\"].astimezone(ZoneInfo(\"UTC\")),\n                price=item[\"price\"]\n            ) for item in data\n        ], key=lambda x: x.startDate, reverse=False)\n\n    async def fill_missing_entries(self, result: List[PriceDataPoint], missing_entries: List[PriceDataPoint]):\n        \"\"\"\n        Fetch and insert missing price data entries from the external API.\n\n        Args:\n            result (List[PriceDataPoint]): List to append new data points to (modified in place).\n            missing_entries (List[PriceDataPoint]): List of missing data points to fetch.\n\n        Side effects:\n            Updates the result list and inserts new entries into the database.\n        \"\"\"\n        for missing in missing_entries:\n            fetched = await self.ext_api_fetcher.fetch_price_data_range(missing.startDate, missing.startDate)\n            if not fetched:\n                continue\n\n            datapoint = fetched[0]\n            utc_dt = datetime.fromisoformat(datapoint[\"startDate\"])\n            iso_str = (utc_dt + timedelta(hours=3)).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n\n            result.append(PriceDataPoint(\n                startDate=utc_dt,\n                price=datapoint[\"price\"]\n            ))\n            await self.database_fetcher.insert_entry(\n                price=datapoint[\"price\"],\n                iso_date=iso_str\n            )\n\n    async def fetch_and_process_data(self, start_date: datetime, end_date: datetime) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch and process price data from the database, fill missing entries from the external API if needed.\n\n        Args:\n            start_date (datetime): Start of the time range.\n            end_date (datetime): End of the time range.\n\n        Returns:\n            List[PriceDataPoint]: Sorted list of price data points for the range, including filled-in values if needed.\n        \"\"\"\n        start_naive = start_date.replace(tzinfo=None)\n        end_naive = end_date.replace(tzinfo=None)\n\n        raw_data = await self.database_fetcher.get_entries(\n            start_date=start_naive,\n            end_date=end_naive,\n            select_columns=\"datetime, price\"\n        )\n\n\n        result = self.convert_to_price_data(raw_data)\n\n        missing_entries = self.find_missing_entries_utc(\n            start_date.astimezone(ZoneInfo(\"UTC\")),\n            end_date.astimezone(ZoneInfo(\"UTC\")),\n            result\n        )\n        if missing_entries:\n            await self.fill_missing_entries(result, missing_entries)\n\n        return sorted(result, key=lambda x: x.startDate, reverse=False)\n\n    def find_missing_entries_utc(self, start_date_utc: datetime, end_date_utc: datetime, data_utc: List[PriceDataPoint]):\n        \"\"\"\n        Find missing hourly entries in the given UTC time range.\n\n        Args:\n            start_date_utc (datetime): Start of the UTC time range.\n            end_date_utc (datetime): End of the UTC time range.\n            data_utc (List[PriceDataPoint]): List of available data points.\n\n        Returns:\n            List[StartDateModel]: List of StartDateModel objects for missing hours (all in UTC).\n        \"\"\"\n        result = []\n        current_date_utc = start_date_utc\n        while current_date_utc <= end_date_utc:\n            if not any(item.startDate == current_date_utc for item in data_utc):\n                result.append(StartDateModel(startDate=current_date_utc))\n            current_date_utc += timedelta(hours=1)\n        result.sort(key=lambda x: x.startDate, reverse=False)\n        return result\n",
  "./App/python-server/utils/porssisahko_tools.py": "\"\"\"\nporssisahko_tools.py provides utility functions for working with price data and database readiness in the Eprice backend.\n\nFeatures:\n- Asynchronous function to wait for the PostgreSQL database to become available before starting the application.\n- Conversion utility to transform price and ISO 8601 date data into a dictionary format suitable for the porssisahko table.\n\nIntended Usage:\n- Used by repository and service layers to ensure database readiness and to prepare data for insertion into the porssisahko table.\n- Can be extended with additional utilities for price data processing as needed.\n\nDependencies:\n- asyncpg for asynchronous PostgreSQL operations.\n- Python standard library modules: datetime and time.\n\"\"\"\n\nfrom datetime import datetime\nimport asyncpg\nimport time\n\nasync def wait_for_database(database_url):\n    \"\"\"\n    Wait for the database to be ready by attempting to connect to it.\n\n    Args:\n        database_url: The URL of the database to connect to.\n\n    Raises:\n        Exception: If the database is not ready after multiple attempts.\n        (logs attempts and failure)\n    \"\"\"\n    max_retries = 10\n    retry_delay = 5  # seconds\n    for attempt in range(max_retries):\n        try:\n            conn = await asyncpg.connect(database_url)\n            await conn.close()\n            print(\"Database is ready.\")\n            return\n        except Exception as e:\n            print(f\"Database not ready (attempt {attempt + 1}/{max_retries}): {e}\")\n            time.sleep(retry_delay)\n    raise Exception(\"Database is not ready after multiple attempts.\")\n\n\ndef convert_to_porssisahko_entry(price, iso_date, predicted=False):\n    \"\"\"\n    Converts a price and ISO 8601 date into a dictionary for the porssisahko table.\n\n    Args:\n        price (float): The price value.\n        iso_date (str): The date in ISO 8601 format (e.g., \"2022-11-14T22:00:00.000Z\").\n        predicted (bool): Indicates if the price is predicted. Default is False.\n    Returns:\n        dict: A dictionary with keys: Datetime, Date, Year, Month, Day, Hour, Weekday, Price, Predicted.\n    Raises:\n        ValueError: If the ISO date is not in the correct format.\n    \"\"\"\n    try:\n        # Juho: we'll later make the formats more precise and succinct\n        # Now there's some needless formatting to make the api play nice with the database\n        dt = datetime.fromisoformat(iso_date.replace(\"Z\", \"+00:00\"))  # Handle the \"Z\" for UTC\n\n        # Convert to offset-naive datetime (db doesn't like tz's)\n        dt_naive = dt.replace(tzinfo=None)\n\n        # Extract the weekday\n        weekday = dt_naive.weekday()\n\n        # Return the dictionary\n        return {\n            \"datetime\": dt_naive,  # Use offset-naive datetime\n            \"date\": dt_naive.date(),  # Extract the date part\n            \"year\": dt_naive.year,\n            \"month\": dt_naive.month,\n            \"day\": dt_naive.day,\n            \"hour\": dt_naive.hour,\n            \"weekday\": weekday,\n            \"price\": price,\n            \"predicted\": predicted\n        }\n    except ValueError as e:\n        # Handle invalid date format or parsing errors\n        raise ValueError(f\"Invalid ISO date format: {iso_date}. Error: {e}\")\n\n\n",
  "./Documents/README.md": "## Documents and materials for demo session at Taitotalo\n\nPut everything we need for the first session at Taitotalo.\n\n### Sequence diagrams\n\nInstall vscode sequance diagram -extension (see it's documentation, and https://bramp.github.io/js-sequence-diagrams/ for examples).\n\nYou can open preview in vscode, and export as png/svg.\n\n\n### class diagrams\n\nInstall PlanUML extension, and install Graphviz:\n\n`sudo apt install graphviz` (on linux)\n\nYou might also need java/jre, which you very likely already have -- if you can't render these, try googling the solution.\n",
  "./Documents/backend_design.md": "# Controller-Service-Repository Pattern & CRUD: An Overview\n\n## Introduction\n\nModern web applications often separate concerns into distinct layers to improve maintainability, testability, and scalability. One popular approach in the backend is the controller-service-repository pattern. This pattern is especially useful for applications with complex business logic and database interactions. We are aiming to comply with this design pattern in the project.\n\nA server that serves a Svelte frontend can use this pattern to handle requests—especially those involving sensitive information — securely and efficiently.\n\n## The Layers Explained\n\n1. **Controller**\n\n   - **Role:** Handles HTTP requests and responses.\n   - **Responsibility:** Receives input from the client (e.g., a Svelte form), calls the appropriate service methods, and returns the result.\n   - **Example:** The controller defines endpoints such as `/api/auth/register` and `/api/auth/login`. These endpoints receive user data, call the service layer, and return JSON responses.\n   - **Error Handling:** Exceptions are mainly caught at the controller level. The controller always returns valid JSON to the frontend, including appropriate error messages when necessary, while avoiding leaking sensitive information.\n\n2. **Service**\n\n   - **Role:** Contains business logic.\n   - **Responsibility:** Implements the core functionality of the application, such as validating credentials, generating tokens, or sending emails. It orchestrates calls to the repository and other utilities.\n   - **Example:** The service layer handles password hashing, verification code generation, and calls to the repository for user data. It may also send emails after successful registration or code updates.\n\n3. **Repository**\n\n   - **Role:** Manages data persistence.\n   - **Responsibility:** Handles all interactions with the database. It provides methods for CRUD operations (Create, Read, Update, Delete) on data models.\n   - **Example:** The repository provides methods like `get_user_by_email`, `create_user`, `verify_code`, and `update_code` to interact with the `users` table in the database.\n\n## CRUD Operations\n\nCRUD stands for Create, Read, Update, Delete—the four basic operations for persistent storage.\n\n- **Create:** Add new records (e.g., registering a new user).\n- **Read:** Retrieve records (e.g., fetching a user by email).\n- **Update:** Modify existing records (e.g., updating a verification code).\n- **Delete:** Remove records (for example, deleting a user).\n\n### Example: Authentication Flow\n\n1. **Register (Create)**\n    - Controller: Receives registration data from the Svelte frontend.\n    - Service: Hashes the password, generates a verification code, and calls the repository to create the user.\n    - Repository: Inserts the new user into the database.\n\n2. **Login (Read)**\n    - Controller: Receives login credentials.\n    - Service: Fetches the user by email and verifies the password.\n    - Repository: Retrieves the user record from the database.\n\n3. **Verify Email (Update)**\n    - Controller: Receives a verification code.\n    - Service: Calls the repository to update the user's verification status.\n    - Repository: Updates the `is_verified` field in the database.\n\n## Why Use Controller-Service-Repository Pattern?\n\n- **Separation of Concerns:** Each layer has a single responsibility, making the codebase easier to understand and maintain.\n- **Testability:** Business logic can be tested independently from HTTP and database layers.\n- **Reusability:** Services and repositories can be reused across different controllers or even applications.\n- **Security:** Sensitive operations (like authentication) are handled server-side, reducing exposure to the client.\n- **Consistent Error Handling:** Exceptions are caught at the controller level, and valid JSON responses are always returned to the frontend, including appropriate error messages without leaking sensitive details.\n\n## How It Works with Svelte\n\nA Svelte frontend can use form actions and server-side calls for sensitive or restricted operations. When a user submits a form (such as login or register), the request is sent to the FastAPI backend, which processes it through the controller-service-repository pipeline. This ensures data is validated, business rules are enforced, and database operations are performed securely.\n\n## Summary Diagram\n\n```plantuml\n@startuml\n[Svelte Form] --> [Controller (FastAPI Route)]\n[Controller (FastAPI Route)] --> [Service (Business Logic)]\n[Service (Business Logic)] --> [Repository (Database Access)]\n[Repository (Database Access)] --> [Database]\n@enduml\n```\n\nThis pattern helps build robust, maintainable, and secure web applications, especially when handling authentication and other sensitive operations.",
  "./Documents/diagrams/sources/authentication_call_sequence_diagram.wsd": "@startuml\ntitle Sequence Diagram for User Registration, Login, Verification, and Logout using form-actions and JWT\n\nClient -> ClientServer: register form-action\nClientServer -> Server: api/auth/register\nServer -> Database: insert new user into database\nnote over Server, Database: hash password and attempt to insert\\nif email not already registered\nalt registration success\n    Server -> EmailService: send verification code to email\n    EmailService --> Server: email sent\n    Server --> Client: registration success (prompt verify)\nelse registration failure\n    Server --> Client: registration failure\nend\nnote over Client: On success: prompt for verification code\n\nClient -> ClientServer: verify form-action (email, code)\nClientServer -> Server: api/auth/verify\nServer -> Database: update user is_verified if code matches\nDatabase --> Server: success/failure\nalt verification success\n    Server --> Client: verification success\nelse verification failure\n    Server --> Client: verification failure\nend\n\nClient -> ClientServer: resend verification code form-action (email)\nClientServer -> Server: api/auth/resend\nServer -> Database: update verification_code for user\nalt update success\n    Server -> EmailService: send new verification code to email\n    EmailService --> Server: email sent\n    Server --> Client: resend success\nelse update failure\n    Server --> Client: resend failure\nend\n\nClient -> ClientServer: login form-action (email, password)\nClientServer -> Server: api/auth/login\nServer -> Database: select user from database\nDatabase --> Server: user data / not found\nalt user found\n    Server -> Server: verify password hash\n    alt password correct and is_verified\n        Server -> Server: pack user info in JWT\n        Server --> ClientServer: login success and set-cookie (JWT)\n        note over Client, ClientServer: JWT available to client components\n    else not verified\n        Server --> ClientServer: login failure (email not verified)\n    else password incorrect\n        Server --> ClientServer: login failure (wrong credentials)\n    end\nelse user not found\n    Server --> ClientServer: login failure (wrong credentials)\nend\n\nClient -> ClientServer: logout form-action\nClientServer -> Server: api/auth/logout\nServer --> ClientServer: delete JWT cookie\nnote over ClientServer, Client: Client server clears JWT payload\n\n@enduml",
  "./Documents/diagrams/sources/authentication_class_diagram.wsd": "@startuml\ntitle FastAPI Server-side Authentication (Updated)\n\npackage \"Models\" {\n    class User {\n        + email: EmailStr\n        + password: str\n        + validate_password(password: str): str\n    }\n    class UserCode {\n        + email: EmailStr\n        + code: str\n        + validate_code(code: str): str\n    }\n    class EmailRequest {\n        + email: EmailStr\n    }\n}\n\npackage \"Repositories\" {\n    class UserRepository {\n        - database_url: str\n        + get_user_by_email(email: str): dict | None\n        + create_user(email: str, password_hash: str, verification_code: str): None\n        + verify_code(email: str, verification_code: str): Any\n        + update_code(email: str, new_code: str): None\n    }\n}\n\npackage \"Services\" {\n    class AuthService {\n        - user_repository: UserRepository\n        - pwd_context: CryptContext\n        + get_password_hash(password: str): str\n        + generate_verification_code(): str\n        + verify_password(plain_password: str, hashed_password: str): bool\n        + create_access_token(data: dict, expires_delta: timedelta = None): str\n        + authenticate_user(email: str, password: str): dict | None\n        + register_user(email: str, password: str): None\n        + verify_user(email: str, code: str): None\n        + update_verification_code(email: str): None\n    }\n}\n\npackage \"Controllers\" {\n    class AuthController {\n        + register(user: User, response: Response): dict\n        + login(user: User, response: Response): dict\n        + logout(response: Response): dict\n        + verify(user_code: UserCode, response: Response): dict\n        + resend_verification_code(request: EmailRequest, response: Response): dict\n        + create_jwt_middleware(public_routes: list): Callable\n    }\n}\n\n' Relationships\nAuthService --> UserRepository\nAuthController --> AuthService\nAuthController --> User\nAuthController --> UserCode\nAuthController --> EmailRequest\n\n@enduml",
  "./Documents/diagrams/sources/authentication_use_case.wsd": "@startuml\ntitle Use Case Diagram for authentication in Electricity Market App\n\nactor \"Signed-in User\" as User\nactor \"Visitor\" as Visitor\n\npackage \"Frontend (Svelte)\" {\n    usecase \"Register\" as Register\n    usecase \"Login\" as Login\n    usecase \"Logout\" as Logout\n    usecase \"Verify Email\" as VerifyEmail\n    usecase \"Resend Verification Code\" as ResendCode\n}\n\npackage \"Backend (FastAPI)\" {\n    package \"User Authentication\" as Auth  {\n        usecase \"Signal front to\\n remove JWT\" as RemoveJWT\n        usecase \"Hash Password and\\n send with email to DB\\n(un-verified)\" as HashPassword\n        usecase \"Verify Status & Password\\nand pack JWT\" as VerifyPassword\n        usecase \"Generate Verification Code\\nand send email\" as GenCode\n        usecase \"Verify User Code\" as VerifyUserCode\n        usecase \"Update Verification Code\\nand send email\" as UpdateCode\n    }\n    usecase \"Manage route access\\nwith JWT's\" as ManageJWT\n}\n\npackage \"Database\" {\n    usecase \"Store and Retrieve User Data\" as UserDB\n}\n\nUser --> Login\nUser --> Logout\nUser --> Register\nUser --> VerifyEmail\nUser --> ResendCode\nVisitor --> Register\nVisitor --> Login\n\nRegister --> HashPassword\nRegister --> GenCode\n\nLogin --> VerifyPassword\nVerifyPassword <--> UserDB\nVerifyPassword <--> ManageJWT\nLogout <--> RemoveJWT\n\nHashPassword --> UserDB\nGenCode --> UserDB\n\nVerifyEmail --> VerifyUserCode\nVerifyUserCode --> UserDB\n\nResendCode --> UpdateCode\nUpdateCode --> UserDB\n\n@enduml",
  "./Documents/diagrams/sources/data_access_call_sequence_diagram.wsd": "@startuml\ntitle FastAPI Server, access to data\n\nTitle: Sequence Diagram for data access\nParticipant Client\nParticipant Server\nParticipant Database\nParticipant ExternalAPI\nClient->Server: POST /api/data/ <identifier>\nNote over Server: Check credentials on\\nprotected routes\nServer->Database: query if cached\nDatabase->Server: Success/Failure [,data]\nNote over Server: On success: return data\\ndirectly to client\\non failure: make api call\nServer->ExternalAPI: api call\nExternalAPI->Server: success/failure [,data]\nServer->Client: Success/Failure [,data]\nServer->Database: On success: cache data\nNote over Client: On success: display data\n\n@enduml",
  "./Documents/diagrams/sources/llm_retrieval.wsd": "@startuml\ntitle LLM engine with retrieval (pre-processed data)\nUser -> LLM_Client: User initiates a query\nnote over LLM_Client: Client embeds the query\nnote over Database: DB has text chunks\\nand embeddings\nLLM_Client -> Database: Send embedded query\nnote over Database: similarity search\\nfor relevant texts\nDatabase -> LLM_Client: Return relevant text chunks\nnote over LLM_Client: Combine query with\\nretrieved text chunks\nLLM_Client -> LLM_Model: formatted prompt with query and context\nLLM_Model -> LLM_Client: Return generated response\nLLM_Client -> User: return generated response\n@enduml",
  "./Documents/diagrams/sources/services_diagram.wsd": "\n@startuml\ntitle Main services and their interactions\nskinparam rectangle {\n    BackgroundColor #FDF6E3\n    BorderColor #586e75\n}\nskinparam cloud {\n    BackgroundColor #FDF6E3\n    BorderColor #586e75\n}\n\ncloud \"Docker Network\\nmanages networking\" as DockerNetwork\n\npackage \"Database Layer\" {\n    [Database] <<container>> \n    [Database Migrations] <<container>> \n}\n\npackage \"Backend Layer\" {\n    [Server] <<container>> \n    [Chat Engine] <<container>> \n}\n\npackage \"Frontend Layer\" {\n    [Client] <<container>> \n    [E2E Tests] <<container>> \n}\n\ncloud \"External APIs\" as ExternalAPIs\n\n[Database Migrations] --> [Database] : Applies migrations\n[Server] --> [Database] : Reads/Writes data\n[Client] --> [Server] : API calls (port 8000)\n[Client] --> [Chat Engine] : LLM service (port 7860)\n[Chat Engine] --> [Database] : Reads/Writes embeddings\\nand retrieves texts\n[E2E Tests] --> [Client] : Tests frontend\n[Server] --> ExternalAPIs : Makes external API calls\n\nnote right of Client\nRuns on port 5173\nend note\n\n@enduml",
  "./Documents/diagrams/sources/tool_calling.wsd": "@startuml\ntitle LLM engine with tool calling\nUser->LLM_Client: User initiates a query\nNote over LLM_Client: Client adds system messages\\nand tool calling instructions\nLLM_Client->LLM_Model: formatted prompt\nNote over LLM_Model: LLM processes the prompt and\\nidentifies if a tool is needed\nLLM_Model->LLM_Client: return tool call instructions\nLLM_Client->Tool: Call tool with parameters\nTool->LLM_Client: return tool results\nNote over LLM_Client: Client adds system messages\\nand results to the prompt\nLLM_Client->LLM_Model: formatted prompt with tool results\nLLM_Model->LLM_Client: return generated response\nLLM_Client->User: return generated response\n@enduml",
  "./Documents/diagrams/sources/use_case.wsd": "@startuml\ntitle Use Case Diagram for Electricity Market App\n\nactor \"Signed-in User\" as User\nactor \"Visitor\" as Visitor\n\npackage \"Frontend (Svelte)\" {\n    usecase \"View Current Electricity Prices\" as ViewPrices\n    usecase \"Request Historical/predicted Data\" as RequestHistorical\n    usecase \"Chat with LLM\" as ChatWithLLM\n}\n\npackage \"Database\" {\n    usecase \"Store and Retrieve Cached Data\" as CacheDB\n    usecase \"Store and Retrieve User Data\\n(Auth service only)\" as UserDB\n    usecase \"Text chunks and\\nvector embeddings\" as VectorDB\n}\n\npackage \"LLM\" {\n    usecase \"Embed Query and Search Vector DB\" as EmbedSearch\n    usecase \"Format prompt with query\\nand context\" as PromptLLM\n    usecase \"LLM engine\" as LLMengine\n}\n\npackage \"Backend (FastAPI)\" {\n    usecase \"Fetch Current Prices\" as FetchPrices\n    usecase \"Fetch Data\" as FetchData\n    usecase \"Check Cache\" as CheckCache\n    usecase \"Retrieve Data from External APIs\\nand chache\" as RetrieveExternal\n}\n\npackage \"External APIs\" {\n    usecase \"Fetch Data from External APIs\" as ExternalAPIs\n}\n\nVisitor --> ViewPrices\nUser --> ViewPrices\nUser --> RequestHistorical\nUser --> ChatWithLLM\n\nViewPrices --> FetchPrices\nRequestHistorical --> FetchData\nFetchPrices --> CheckCache\nFetchData --> CheckCache\nCheckCache --> CacheDB\nCheckCache --> RetrieveExternal\nRetrieveExternal --> CacheDB\nRetrieveExternal --> ExternalAPIs\n\nChatWithLLM --> EmbedSearch\nEmbedSearch --> VectorDB\nEmbedSearch --> PromptLLM\nPromptLLM --> LLMengine\n\n@enduml",
  "./Documents/openapi_endpoint_descriptions.md": "# Eprice API Endpoint Overview\n\nThis document provides a concise technical overview of the main API endpoints exposed by the Eprice backend, as described in the OpenAPI specification.  \nThe API is organized into **public data endpoints** (for electricity, etc.) and **authentication endpoints**.  \nAll other endpoints require authentication via JWT.\n\nThe API is designed for both public and authenticated use. Public endpoints provide enough data for basic electricity price queries and user authentication, while authenticated endpoints allow access to more detailed or user-specific data.\n\n- The use of POST for range queries (instead of GET with query parameters) allows for more complex request bodies and easier extension in the future.\n- The API is well-structured for integration with frontend applications and external systems.\n\n---\n\n## Public Endpoints\n\nThese endpoints are accessible **without authentication** (no JWT required):\n\n### Electricity Data endpoint\n\n- **GET `/api/public/data`**  \n  Returns a list of market price data points (historical and/or current).  \n  **Response:** Array of objects with `startDate` (RFC 3339 UTC string) and `price` (euro cents).\n\n### Authentication endpoints\n\n- **POST `/api/auth/register`**  \n  Registers a new user.  \n  **Request:** JSON with `email` and `password`.  \n  **Response:** Confirmation or validation error.\n\n- **POST `/api/auth/login`**  \n  Authenticates a user and returns a JWT (usually set as a cookie).  \n  **Request:** JSON with `email` and `password`.  \n  **Response:** Confirmation or validation error.\n\n- **POST `/api/auth/verify`**  \n  Verifies a user by checking the verification code.  \n  **Request:** JSON with `email` and `code`.  \n  **Response:** Confirmation or validation error.\n\n- **POST `/api/auth/resend`**  \n  Resends the verification code to the user's email.  \n  **Request:** JSON with `email`.  \n  **Response:** Confirmation or validation error.\n\n- **GET `/api/auth/logout`**  \n  Logs out the user by clearing the JWT cookie.  \n  **Response:** Confirmation.\n\n- **GET `/docs`**  \n  API documentation (Swagger UI).\n\n- **GET `/openapi.json`**  \n  OpenAPI specification in JSON format.\n\n---\n\n## Protected Data Endpoints\n\nThe following endpoints **require authentication** (JWT):\n\n### Market Price data endpoint\n\n- **GET `/api/data/today`**  \n  Returns today's market price data points.  \n  **Response:** Array of objects with `startDate` and `price`.\n\n- **POST `/api/price/range`**  \n  Returns market price data for a specified time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of price data points.\n\n### Wind Power data endpoint\n\n- **GET `/api/windpower`**  \n  Returns the latest wind power production forecast.  \n  **Response:** Object with `startTime`, `endTime`, and `value`.\n\n- **POST `/api/windpower/range`**  \n  Returns wind power production data for a given time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of forecast data points.\n\n### Consumption data endpoint\n\n- **GET `/api/consumption`**  \n  Returns the latest electricity consumption forecast.  \n  **Response:** Object with `startTime`, `endTime`, and `value`.\n\n- **POST `/api/consumption/range`**  \n  Returns consumption data for a given time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of consumption data points.\n\n### Production data endpoint\n\n- **GET `/api/production`**  \n  Returns the latest electricity production forecast.  \n  **Response:** Object with `startTime`, `endTime`, and `value`.\n\n- **POST `/api/production/range`**  \n  Returns production data for a given time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of production data points.\n\n---\n\n## General Notes about api endpoints\n\n- **Validation:**  \n  Most endpoints validate input and return a 422 error for malformed requests.\n\n- **Error Handling:**  \n  On server errors, endpoints return a 500 status with an error message.\n\n- **Authentication:**  \n  Only the endpoints listed under \"Public Endpoints\" are accessible without a JWT.  \n  All other endpoints require authentication.\n\n- **Data Format:**  \n  All timestamps are in RFC 3339 UTC format. Numeric values are typically floats (e.g., price, temperature).\n\n---\n\n",
  "./Documents/project_description.md": "# Application Overview\n\nEprice is an application that show users market electricity price and additional related information, such as electricity consumption and production. For non-registered users, only the current 24 hour period is covered, and only the market price is shown. For registered users, also historical data is available for market price and for production/consumption. The data is represented with graphs and statistics.\n\nFor registered users there is also a chat-engine available, which has access to specific source material, and which augments the user queries with retrieved context information. \n\n---\n\n## Services Overview\n\nThe system consists of multiple containerized services that work together to provide functionality for the Eprice application, including a database, backend server, client application, and additional components for migrations, testing, and a chat engine.\n\n### 1. Database\n- **Ports**: `5432` (default PostgreSQL port)\n- **Purpose**: Stores application data, including user information and embeddings for retrieval.\n\n### 2. Database Migrations\n- **Purpose**: Handles database schema migrations.\n- **Depends On**: `Database`\n\n### 3. Server\n- **Ports**: `8000`\n- **Purpose**: Provides backend APIs for the client and other services.\n- **Depends On**: `Database`\n- **Additional Functionality**: Makes external API calls.\n\n### 4. Client\n- **Ports**: `5173` (Svelte client)\n- **Purpose**: Frontend application for user interaction.\n- **Depends On**: `Server`, `Chat Engine`\n\n### 5. E2E Tests\n- **Purpose**: Runs end-to-end tests for the system.\n- **Depends On**: `Client`\n\n### 6. Chat Engine\n- **Ports**: `7860`\n- **Purpose**: Provides a chat engine for interaction with the system.\n- **Depends On**: `Database`\n\n---\n\n\n```plantuml\n\n@startuml\ntitle Main services and their interactions\nskinparam rectangle {\n    BackgroundColor #FDF6E3\n    BorderColor #586e75\n}\nskinparam cloud {\n    BackgroundColor #FDF6E3\n    BorderColor #586e75\n}\n\ncloud \"Docker Network\\nmanages networking\" as DockerNetwork\n\npackage \"Database Layer\" {\n    [Database] <<container>> \n    [Database Migrations] <<container>> \n}\n\npackage \"Backend Layer\" {\n    [Server] <<container>> \n    [Chat Engine] <<container>> \n}\n\npackage \"Frontend Layer\" {\n    [Client] <<container>> \n    [E2E Tests] <<container>> \n}\n\ncloud \"External APIs\" as ExternalAPIs\n\n[Database Migrations] --> [Database] : Applies migrations\n[Server] --> [Database] : Reads/Writes data\n[Client] --> [Server] : API calls (port 8000)\n[Client] --> [Chat Engine] : LLM service (port 7860)\n[Chat Engine] --> [Database] : Reads/Writes embeddings\\nand retrieves texts\n[E2E Tests] --> [Client] : Tests frontend\n[Server] --> ExternalAPIs : Makes external API calls\n\nnote right of Client\nRuns on port 5173\nend note\n\n@enduml\n```\n\n## Additional Services Overview\n\n### Data-preparation\n\n- **Purpose**: Can be used independently to retrieve or update data. Saves data to a location that is available to migrations and the database.\n\n### Backend-tests\n\n- **Purpose**: Can be used to test backend functionality independently from the frontend. Beyond fault testing, backend-tests also give additional information and warnings which would be hidden from the e2e-tests.\n- **Depends On**: Database and Server.\n\nThe backend tests are mainly for development purposes, as the e2e-tests should cover the main functionalities by the end of development.\n",
  "./Documents/project_directory_structure.txt": "├── LICENSE\n├── README.md\n├── for_virtualbox.sh\n├── App\n│   ├── README.md\n│   ├── compose.yaml\n│   ├── project.env\n│   ├── backend-tests\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   └── tests\n│   │       └── test_auth_controller.py\n│   ├── chat-engine\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── gradio_dashboard.py\n│   │   └── run.sh\n│   ├── client\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── docker\n│   │   ├── src\n│   │   │   ├── app.css\n│   │   │   ├── app.html\n│   │   │   ├── hooks.server.js\n│   │   │   ├── lib\n│   │   │   │   ├── apis\n│   │   │   │   │   └── data-api.js\n│   │   │   │   ├── assets\n│   │   │   │   │   ├── image.png\n│   │   │   │   │   ├── image2.png\n│   │   │   │   │   ├── image3.png\n│   │   │   │   │   ├── image4.png\n│   │   │   │   │   └── image5.png\n│   │   │   │   ├── components\n│   │   │   │   │   ├── ChatBot.svelte\n│   │   │   │   │   ├── MainChart.svelte\n│   │   │   │   │   └── layout\n│   │   │   │   │       ├── Footer.svelte\n│   │   │   │   │       ├── Header.svelte\n│   │   │   │   │       └── PriceBall.svelte\n│   │   │   │   └── states\n│   │   │   │       ├── usePricesState.svelte.js\n│   │   │   │       └── userState.svelte.js\n│   │   │   └── routes\n│   │   │       ├── +layout.js\n│   │   │       ├── +layout.server.js\n│   │   │       ├── +layout.svelte\n│   │   │       ├── +page.svelte\n│   │   │       ├── auth\n│   │   │       │   └── [action]\n│   │   │       │       ├── +page.js\n│   │   │       │       ├── +page.server.js\n│   │   │       │       └── +page.svelte\n│   │   │       ├── logout\n│   │   │       │   ├── +page.server.js\n│   │   │       │   └── +page.svelte\n│   │   │       └── send\n│   │   │           ├── +page.server.js\n│   │   │           └── +page.svelte\n│   │   └── static\n│   │       └── favicon.png\n│   ├── data-preparation\n│   │   └── scripts\n│   │       ├── Dockerfile\n│   │       ├── README.md\n│   │       ├── clean_porssisahko.py\n│   │       ├── populate_porssisahko.py\n│   │       └── retrieve_porssisahko_update.sh\n│   ├── database-migrations\n│   │   ├── V10__documents_constraint.sql\n│   │   ├── V1__users.sql\n│   │   ├── V2__porssisahko.sql\n│   │   ├── V3__timezone.sql\n│   │   ├── V4__users_add_role.sql\n│   │   ├── V5__porssisahko_load_entries.sql\n│   │   ├── V6__users_add_isverified.sql\n│   │   ├── V8__extension_vector.sql\n│   │   └── V9__documents.sql\n│   ├── e2e-tests\n│   │   ├── Dockerfile\n│   │   └── tests\n│   │       └── example.spec.js\n│   └── python-server\n│       ├── Dockerfile\n│       ├── README.md\n│       ├── main.py\n│       ├── requirements.txt\n│       ├── config\n│       │   ├── __init__.py\n│       │   └── secrets.py\n│       ├── controllers\n│       │   ├── auth_controller.py\n│       │   └── data_controller.py\n│       ├── ext_apis\n│       │   └── ext_apis.py\n│       ├── models\n│       │   ├── custom_exception.py\n│       │   ├── data_model.py\n│       │   └── user_model.py\n│       ├── repositories\n│       │   ├── porssisahko_repository.py\n│       │   └── user_repository.py\n│       ├── scheduled_tasks\n│       │   └── porssisahko_scheduler.py\n│       ├── services\n│       │   ├── auth_service.py\n│       │   └── data_service.py\n│       └── utils\n│           ├── email_tools.py\n│           ├── porssisahko_service_tools.py\n│           └── porssisahko_tools.py\n└── Documents\n    ├── README.md\n    ├── backend_design.md\n    ├── openapi_endpoint_descriptions.md\n    ├── project_description.md\n    ├── project_directory_structure.txt\n    └── diagrams\n        └── sources\n            ├── authentication_call_sequence_diagram.wsd\n            ├── authentication_class_diagram.wsd\n            ├── authentication_use_case.wsd\n            ├── data_access_call_sequence_diagram.wsd\n            ├── llm_retrieval.wsd\n            ├── services_diagram.wsd\n            ├── tool_calling.wsd\n            └── use_case.wsd\n",
  "./LICENSE": "MIT License\n\nCopyright (c) 2025 Markus Kojo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
  "./README.md": "\n# Eprice project\n\nA complete README file in `./App/README.md`.\n\nApp directory holds the application code and all configurations.\n\nAdditional documentation in `./Documents`.\n\n",
  "./for_virtualbox.sh": "\n# Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\nsudo apt-get update\n\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n\n# install deno\ncurl -fsSL https://deno.land/install.sh | sh\n\ndeno upgrade\n\n# install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo apt install ./google-chrome*.deb\nsudo apt install xclip\n\n# for git\nssh-keygen -t ed25519 -C \"paavo.reinikka@proton.me\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\ncat ~/.ssh/id_ed25519.pub | xclip\n# Now save the pub key to github keys\n\ngit clone git@github.com:wumpfroot/Eprice.git\n\ncd Eprice/App/client\ndeno install --allow-sctips\n\nsudo apt install tmux\n\n# RUN E2E-TESTS: docker compose run --rm --entrypoint=npx e2e-tests playwright test\n# RUN endpoint tests: docker compose --profile backend-tests up (assuming the test container has been built)\n\n"
}