{
  "./App/README.md": "# Eprice App\n\nThe Eprice App is a containerized application that allows users to view the market price of electricity in Finland, both current and historical. The app is built with a modern tech stack, including a Svelte frontend, a FastAPI backend, a PostgreSQL database, and various tools for testing and data management.\n\n## Features\n\n- **Electricity Price Viewer**: View current and historical electricity prices in Finland.\n- **Svelte Frontend**: A modern, responsive UI built with Svelte and Vite.\n- **FastAPI Backend**: A Python-based backend for handling API requests and business logic.\n- **PostgreSQL Database**: A robust database for storing electricity price data.\n- **Flyway Migrations**: Manage database schema changes with ease.\n- **Testing**: End-to-end tests with Playwright and backend API tests with Pytest.\n- **User chat**: A chat-based interface to help with the functionalities of the app.\n- **Data Loading**: Load and update electricity price data into the database using scripts in a dedicated container.\n\n---\n\n## Project Structure\n.\n\n ├── README.md # Root README file\n\n ├── compose.yaml # Docker Compose configuration \n \n ├── user-chat/ # Chat engine service \n \n ├── client/ # Svelte frontend service\n \n ├── data-preparation/ # Scripts and data for populating the database\n \n ├── database-migrations/ # Flyway migration scripts \n \n ├── e2e-tests/ # Playwright end-to-end tests\n \n ├── python-server/ # FastAPI backend service\n \n └── project.env # Environment variables for the project\n\n\n---\n\n## Getting Started\n\n### Prerequisites\n\n- **Docker** and **Docker Compose**: Install Docker Desktop or Docker CLI.\n- **Deno**: Required for local development of the frontend (see `client/README.md`), and package management.\n\nAny libraries needed in the containers are installed automatically using requirements or other configuration files. `uv` is preferable package manager also inside containers, but pip is used as fallback in some cases.\n\n### Database Persistence\n\nTo ensure your PostgreSQL database data is persisted locally (so it is not lost when containers are stopped or removed), you need to create a directory for the database files and set the correct permissions:\n\n```bash\nmkdir -p ./pgdata\nchmod 700 ./pgdata\nsudo chown -R 999:999 ./pgdata\n```\n\n- `mkdir -p ./pgdata` creates the directory if it doesn't exist.\n- `chmod 700 ./pgdata` sets secure permissions (Postgres default).\n- `sudo chown -R 999:999 ./pgdata` sets ownership to the default Postgres user inside the container (UID 999).\n\n**Note:**  \nIf you change to a custom Postgres image or user, check the UID/GID with:\n\n```bash\ndocker run --rm <root/image:version> id postgres\n```\n(here we have root=reinikp2 and image=pgvector-database, and version=v1)\n\nand adjust the `chown` command accordingly.\n\n### Running the App\n\n1. Clone the repository:\n    ```bash\n    git clone <repository-url>\n    cd Eprice\n    ```\n\n2. Build and start the containers:\n    ```bash\n    docker compose up --build\n    ```\n\n3. Access the services:\n\n* Frontend: http://localhost:5173\n* Backend: http://localhost:8000\n\n4. To stop the containers:\n    ```bash\n    docker compose down\n    ```\n\n### Testing\n\n\n1. Run Playwright tests:\n    ```bash\n    docker compose run --rm --entrypoint=npx e2e-tests playwright test\n    ```\n\n2. Run Pytest for backend API (using uv inside the container):\n    ```bash\n    docker compose run backend-tests [uv run pytest]\n    ```\n\n### Environment Variables\n\n* Use `.env.local` for local development (gitignored)\n\n* Use `project.env` for containerrized development\n\n* To inspect environment variables inside a container:\n    ```bash\n    docker exec -it <container_name> bash\n    printenv\n    ```\n\n### Services overview\n\n1. Frontend (Client)\nBuilt with Svelte and Vite.\nLocated in the client/ directory.\nSee client/README.md for more details.\n\n2. Backend (Python Server)\nBuilt with FastAPI.\nLocated in the python-server/ directory.\nSee python-server/README.md for more details.\n\n3. Database\nPostgreSQL database for storing electricity price data.\nFlyway is used for managing schema migrations (database-migrations/).\n\n4. User Chat\nA chat-based interface for interacting with the app.\nLocated in the user-chat/ directory.\n\n5. Data Preparation\nScripts for loading and updating electricity price data.\nLocated in the data-preparation/ directory.\n\n### Contributing\n\n1. Fork the repository.\n\n2. Create a feature branch:\n\n    ```bash\n    git checkout -b feature-name\n    ```\n\n3. Commit your changes:\n\n    ```\n    git commit -m \"Add feature-name\"\n    ```\n\n4. Push to your branch:\n\n    ```bash\n    git push origin feature-name\n    ```\n\n5. Open pull request.\n\n\n## License\n\nThis project us under MIT license: https://mit-license.org/\n\n## Acknowledgments\n\nElectricity price data is sourced from Pörssisähkö API.\n\n## Additional notes\n\n**Install docker and docker compose**. Maybe easiest to just install docker desktop, especially on windows.\n\n**Postgres is not needed on local machine** (unless you want to run outside containers).\n\nThe compose.yaml and the individual Dockerfiles are sufficient to run the App. Docker does the installing for the containers. But you can still run `deno install --allow-scripts`, if you want to run on local host. On windows, after running deno install, `node_modules/` that are loaded into client should not be copied into the container -- the container is using arch-linux as base image. You can either remove those, or add your own `.dockerignore` file.\n\nRun using docker compose:\n\n`docker compose up --build` (no need to build everytime)\n\nYou can also simply `ctrl+C` to shut down the containers, or\n\n`docker compose down` to tear down.\n\n\n### **About environment variables**\n\nKeep private information private, preferably :-). You can use `.env.local` convention, and keep them gitignored. For public api keys, while developing, we can all get our own api keys.\n\n**If you are unsure what environment variables are loaded on your container launch -- either by docker from project.env, or by services using other tools, like dotenv -- you can always go inside the container to check:**\n\n```\ndocker compose up -d <service_name> # launch the container\n\ndocker exec -it <container_name> bash # go into cmdline inside\n\n(container): printenv # or echo etc.\n```\n\n**For developing client:** There is a sort of a bug in the denolands alpine image, which prevents us from installing with optional flags -- in our case `deno install --allow-scripts`. This means that the node modules need to be copied from local. This is not an issue if you are using linux (or wsl2 on Windows). Later, there might be a change in the client's base image later on to fix this issue.\n\nAnd note that the container names are not necessarily same as the service name (they are derived from it though); you can check running cont's with `docker <container> ps`.\n\n### Running without Docker\n\nCan be done, but needlessly cumbersome. Ask Paavo for the how.\n\n\n### Starting a new client build from scratch\n\nIf you want to start the client build from scratch, for example with typescript checking enabled, run:\n\n```bash\ndeno run -A npm:sv@latest create client\n```\n\nfrom the root directory, and choose from the given options. For this project, we have used the most minimal build setup (SveleteKit minimal, no TS typechecking, nothing added, with deno itself for dependency management).\n",
  "./App/backend-tests/Dockerfile": "FROM python:3.13-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the test files into the container\nCOPY ./tests /app/tests\n\nCOPY ./pyproject.toml /app/pyproject.toml\nCOPY ./pytest.ini /app/pytest.ini\n\n# Add python-server to PYTHONPATH\nENV PYTHONPATH=\"/app/python-server:$PYTHONPATH\"\n\n# Install dependencies and curl\nRUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates\n# Download the latest uv installer\nADD https://astral.sh/uv/install.sh /uv-installer.sh\n# Run the installer then remove it\nRUN sh /uv-installer.sh && rm /uv-installer.sh\n# Ensure the installed binary is on the `PATH`\nENV PATH=\"/root/.local/bin/:$PATH\"\n\nRUN uv sync\n\nRUN uv add pip\nRUN uv run pip install --upgrade httpx\n\n# Set the default command to run pytest\nCMD [\"uv\", \"run\", \"pytest\", \"tests\"]",
  "./App/backend-tests/README.md": "### Endpoint tests\n\nTests for the phase of development when the frontend is incomplete. Helps in picking up on errors, exceptions and warnings. Relies on Pydantic and proper exception ahndling in the backend to be fully informative. Becomes somewhat obsolete after e2e-tests have been written.\n\nYou need to have the Eprice -app running if you want to run the tests. So call `docker compose up` to run the server (or with `-d server` flag, since front is not needed for these tests). If you run `docker compose up --build`, the backend tests **will not be started** as they are defined as a \"profile\" in compose file.\n\nTo run the tests, first build the container with `docker compose build` inside the backend-tests, or just by adding the `--build` and `-d` flags:\n\n```\ndocker compose up --build -d backend-tests\n```\n\nYou can also run them in various other ways, e.g., jointly with every other container:\n\n```\ndocker compose up --profile backend-tests\n```\n\nIf you do it this way, and we add tests that depend on the loaded data in the database, this can cause issues -- we need to specify in the compose file that the backend-tests depends on `database: service_healthy`.\n\nYou can also run the tests with\n\n```\ndocker compose run backend-tests\n```\n\nbut these will leave a \"orphan\" containers behind -- these can be removed with `--remove-orphans` flag. Docker will warn you is this is happening, and in any case it is good practice to keep an eye on dockers resource usage and dangling containers:\n\n```\ndocker ps # running containers\ndocker ps -a # all containers\ndocker inspect <container_name_or_id>\ndocker stats # resource usage\ndocker container prune # remove stopped containers (it prompts you)\ndocker system prune # remove unused images, networks (and volumes)\n```\n",
  "./App/backend-tests/tests/test_auth_controller.py": "import pytest\nimport pytest_asyncio\nfrom httpx import AsyncClient, ASGITransport\nimport random\n\n# Create a pytest-asyncio fixture for the test client\n@pytest_asyncio.fixture\nasync def client():\n    async with AsyncClient(base_url=\"http://localhost:8000\") as client:\n        yield client\n\n# Test successful user registration\n@pytest.mark.asyncio\nasync def test_register_user(client):\n    # create a random email for testing\n    random_number = random.randint(0, 99999)\n    random_email = f\"email{random_number}@user.com\"\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": random_email, \"password\": \"newpassword\"},\n        headers={\"Content-Type\": \"application/json\"}\n    )\n    assert response.status_code == 200\n    assert \"message\" in response.json()\n    assert \"Confirmation email sent\" in response.json()[\"message\"]\n\n@pytest.mark.asyncio\nasync def test_register_user_invalid_email(client):\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": \"invalid-email\", \"password\": \"newpassword\"},\n        headers={\"Content-Type\": \"application/json\"}\n    )\n    assert response.status_code == 422\n    assert \"value is not a valid email address: An email address must have an @-sign.\" in response.json()[\"message\"]\n\n@pytest.mark.asyncio\nasync def test_register_user_invalid_email_end(client):\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": \"invalid-email@\", \"password\": \"newpassword\"},\n        headers={\"Content-Type\": \"application/json\"}\n    )\n    assert response.status_code == 422\n    assert \"value is not a valid email address: There must be something after the @-sign.\" in response.json()[\"message\"]\n\n\n@pytest.mark.asyncio\nasync def test_register_user_short_password(client):\n    # Generate a random email for testing\n    random_number = random.randint(0, 99999)\n    random_email = f\"another{random_number}@user.com\"\n\n    # Send a request with a short password\n    response = await client.post(\n        \"/api/auth/register\",\n        json={\"email\": random_email, \"password\": \"3\"},  # Password is too short\n        headers={\"Content-Type\": \"application/json\"}\n    )\n\n    # Assert the response status code\n    assert response.status_code == 422\n    assert \"Password must be at least 4 characters long\" in response.json()[\"message\"]\n\n",
  "./App/backend-tests/tests/test_data_controller.py": "import pytest\nimport pytest_asyncio\nfrom httpx import AsyncClient\n\n@pytest_asyncio.fixture\nasync def client():\n    \"\"\"Fixture for creating an async HTTP client with the test server base URL.\"\"\"\n    async with AsyncClient(base_url=\"http://localhost:8000\") as client:\n        yield client\n\n@pytest_asyncio.fixture\nasync def auth_client(client):\n    \"\"\"\n    Fixture for creating an authenticated HTTP client by logging in\n    and storing the session cookie for subsequent requests.\n    \"\"\"\n    login_resp = await client.post(\n        \"/api/auth/login\",\n        json={\"email\": \"test@test.com\", \"password\": \"secret\"}\n    )\n    assert login_resp.status_code == 200\n    return client\n\n@pytest.mark.asyncio\nasync def test_get_prices(auth_client):\n    \"\"\"Test that the public endpoint for retrieving price data returns a list of items with 'startDate' and 'price'.\"\"\"\n    response = await auth_client.get(\"/api/public/data\")\n    assert response.status_code == 200\n    data = response.json()\n    assert isinstance(data, list)\n    assert all(\"startDate\" in item and \"price\" in item for item in data)\n\n@pytest.mark.asyncio\nasync def test_get_prices_today(auth_client):\n    \"\"\"Test that the '/api/data/today' endpoint returns today's prices with the correct structure.\"\"\"\n    response = await auth_client.get(\"/api/data/today\")\n    assert response.status_code == 200\n    data = response.json()\n    assert isinstance(data, list)\n    assert all(\"startDate\" in item and \"price\" in item for item in data)\n\n@pytest.mark.asyncio\nasync def test_get_windpower(auth_client):\n    \"\"\"Test that the windpower endpoint returns a valid single record with 'startTime', 'endTime', and 'value'.\"\"\"\n    response = await auth_client.get(\"/api/windpower\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"startTime\" in data\n    assert \"endTime\" in data\n    assert \"value\" in data\n\n@pytest.mark.asyncio\nasync def test_post_windpower_range(auth_client):\n    \"\"\"Test that the windpower range endpoint returns a list of windpower data within the specified time range.\"\"\"\n    payload = {\n        \"startTime\": \"2024-05-01T00:00:00Z\",\n        \"endTime\": \"2024-05-01T03:00:00Z\"\n    }\n    response = await auth_client.post(\"/api/windpower/range\", json=payload)\n    assert response.status_code == 200\n    data = response.json()\n    assert isinstance(data, list)\n    assert all(\"startTime\" in item and \"endTime\" in item and \"value\" in item for item in data)\n\n@pytest.mark.asyncio\nasync def test_get_consumption(auth_client):\n    \"\"\"Test that the consumption endpoint returns an object with 'startTime', 'endTime', and 'value'.\"\"\"\n    response = await auth_client.get(\"/api/consumption\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"startTime\" in data\n    assert \"endTime\" in data\n    assert \"value\" in data\n\n@pytest.mark.asyncio\nasync def test_post_consumption_range(auth_client):\n    \"\"\"Test that the consumption range endpoint returns a list of data for the specified time range.\"\"\"\n    payload = {\n        \"startTime\": \"2024-05-01T00:00:00Z\",\n        \"endTime\": \"2024-05-01T03:00:00Z\"\n    }\n    response = await auth_client.post(\"/api/consumption/range\", json=payload)\n    assert response.status_code == 200\n    data = response.json()\n    assert isinstance(data, list)\n    assert all(\"startTime\" in item and \"endTime\" in item and \"value\" in item for item in data)\n\n@pytest.mark.asyncio\nasync def test_get_production(auth_client):\n    \"\"\"Test that the production endpoint returns an object with 'startTime', 'endTime', and 'value'.\"\"\"\n    response = await auth_client.get(\"/api/production\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"startTime\" in data\n    assert \"endTime\" in data\n    assert \"value\" in data\n\n@pytest.mark.asyncio\nasync def test_post_production_range(auth_client):\n    \"\"\"Test that the production range endpoint returns a list of production data for a given time range.\"\"\"\n    payload = {\n        \"startTime\": \"2024-05-01T00:00:00Z\",\n        \"endTime\": \"2024-05-01T03:00:00Z\"\n    }\n    response = await auth_client.post(\"/api/production/range\", json=payload)\n    assert response.status_code == 200\n    data = response.json()\n    assert isinstance(data, list)\n    assert all(\"startTime\" in item and \"endTime\" in item and \"value\" in item for item in data)\n\n@pytest.mark.asyncio\nasync def test_post_price_range(auth_client):\n    \"\"\"Test that the price range endpoint returns a list of prices for the specified time range.\"\"\"\n    payload = {\n        \"startTime\": \"2024-05-01T00:00:00Z\",\n        \"endTime\": \"2024-05-01T03:00:00Z\"\n    }\n    response = await auth_client.post(\"/api/price/range\", json=payload)\n    assert response.status_code == 200\n    data = response.json()\n    assert isinstance(data, list)\n    assert all(\"startDate\" in item and \"price\" in item for item in data)\n\n@pytest.mark.asyncio\nasync def test_post_windpower_range_invalid_time(auth_client):\n    \"\"\"Test that posting a time range where endTime is before startTime results in an error.\"\"\"\n    payload = {\n        \"startTime\": \"2024-05-01T03:00:00Z\",\n        \"endTime\": \"2024-05-01T00:00:00Z\"\n    }\n    response = await auth_client.post(\"/api/windpower/range\", json=payload)\n    assert response.status_code in (400, 422, 500)\n    data = response.json()\n    assert \"error\" in data\n\n@pytest.mark.asyncio\nasync def test_post_price_range_missing_fields(auth_client):\n    \"\"\"Test that the price range endpoint returns an error if required fields are missing.\"\"\"\n    payload = {\"startTime\": \"2024-05-01T00:00:00Z\"}  # Missing endTime\n    response = await auth_client.post(\"/api/price/range\", json=payload)\n    assert response.status_code in (400, 422)\n\n@pytest.mark.asyncio\nasync def test_post_production_range_invalid_format(auth_client):\n    \"\"\"Test that the production range endpoint returns an error on invalid date formats.\"\"\"\n    payload = {\n        \"startTime\": \"not-a-date\",\n        \"endTime\": \"also-not-a-date\"\n    }\n    response = await auth_client.post(\"/api/production/range\", json=payload)\n    assert response.status_code in (400, 422)\n\n@pytest.mark.asyncio\nasync def test_protected_route_requires_auth(client):\n    \"\"\"Test that accessing a protected route without authentication returns a 401 or 403 status code.\"\"\"\n    response = await client.get(\"/api/production\")\n    assert response.status_code in (401, 403)\n\n@pytest.mark.asyncio\nasync def test_post_windpower_range_empty_result(auth_client):\n    \"\"\"Test that querying windpower for a time range with no data returns an empty list.\"\"\"\n    payload = {\n        \"startTime\": \"1900-01-01T00:00:00Z\",\n        \"endTime\": \"1900-01-01T03:00:00Z\"\n    }\n    response = await auth_client.post(\"/api/windpower/range\", json=payload)\n    assert response.status_code == 200\n    data = response.json()\n    assert isinstance(data, list)\n    assert len(data) == 0\n\n@pytest.mark.asyncio\nasync def test_post_on_get_endpoint(auth_client):\n    \"\"\"Test that making a POST request to a GET-only endpoint returns a 405 or 404 error.\"\"\"\n    response = await auth_client.post(\"/api/windpower\")\n    assert response.status_code in (405, 404)\n",
  "./App/chat-engine/Dockerfile": "FROM python:3.11.6-slim\n\nWORKDIR /app\n\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n    libpq-dev \\\n    wget curl bash ca-certificates git default-jre build-essential \\\n    graphviz && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Download PlantUML jar to /root\nRUN wget -O /root/plantuml.jar https://github.com/plantuml/plantuml/releases/latest/download/plantuml.jar\n\n# copy everything from the chat-engine directory to the /app directory\nCOPY ./chat-engine /app\n\n# Download the latest uv installer\nADD https://astral.sh/uv/install.sh /uv-installer.sh\n\n# Run the installer (then remove it)\nRUN sh /uv-installer.sh && rm /uv-installer.sh\n\n# Ensure the installed binary is on the `PATH`\nENV PATH=\"/root/.local/bin/:$PATH\"\n\n# Set environment variables for Gradio server\nENV GRADIO_SERVER_NAME=\"0.0.0.0\"\n\n#RUN pip install --no-cache-dir -r requirements.txt\n\nRUN uv sync\n\n# fix if torch incompatibility issues\nRUN uv add torch \n\nENV UV_LINK_MODE=copy\n\nENTRYPOINT [\"uv\", \"run\", \"app.py\", \"--host\", \"--reload\", \"--port\", \"7860\"]",
  "./App/chat-engine/README.md": "# Eprice Chat Engine\n\nThis folder contains the **chat engine** for the Eprice app. The chat engine provides a knowledge base chat interface for the Eprice project, allowing users to interact with project documents, retrieve files, and generate PlantUML diagrams via a conversational interface.\n\n## Features\n\n- **Conversational AI**: Uses OpenAI's GPT models (via LangChain) for chat and document reranking.\n- **Document Search & Reranking**: Retrieves relevant project documents using a HuggingFace embedding model (`BAAI/bge-small-en`) and reranks them with an LLM.\n- **File Retrieval**: Allows users to fetch project files by name.\n- **PlantUML Diagram Generation**: Generates and displays UML diagrams from code or files.\n- **Streaming Responses**: Supports streaming chat responses for a responsive UI.\n- **Gradio UI**: Provides a web-based chat and file browser interface using Gradio.\n\n## Architecture\n\n- **Embedding Model**: Uses HuggingFace's `BAAI/bge-small-en` for vector search.\n- **LLM**: Uses OpenAI's GPT models (e.g., `gpt-4o-mini`) via LangChain's `ChatOpenAI`.\n- **Vector Store**: Uses the project's PostgreSQL database (via `langchain_postgres.PGVector`) for storing and searching document embeddings.\n- **Reranking**: LLM-based reranking of search results for improved relevance.\n- **Tools**: Exposes tools for document search, file retrieval, and diagram generation to the agent.\n\n## Components\n\n- `app.py` : Entrypoint for chat, agent and file apps.\n- `chat_app.py`: Gradio chat interface for the knowledge base.\n- `agent_app.py`: Gradio app with agent-based chat and UML diagram viewer.\n- `file_app.py`: Gradio app for browsing and viewing project files.\n- `chat_manager_with_tools.py` / `agent_manager.py`: Core logic for chat, retrieval, reranking, and tool integration.\n- `autoloading_uml.py`: Gradio component for live UML diagram rendering.\n- `utils/`: Helper functions and tool definitions.\n\n## Setup\n\nBy default, compose does everything except populates the database -- this needs to be done offline (may change in the future). You can also run the chat locally (see instructions below). The instructions to populate the database are in `Eprice/Notebooks/document_loading`. **If you want to develope or use the chat, you need to go read `Eprice/Notebooks/document_loading/README.md`. The notebooks and scripts (which ever you use) will download the required HF model to your local machine, and that will be mounted to the chat container -- this is not strictly necessary, but downloading the model repeatedly during the container on build tends to bump into HF rate limits (nasty traces to debug).\n\n### Prerequisites\n\n- see `pyproject.toml` for dependencies\n- `BAAI/bge-small-en` model locally in `~/.cache/hugginface/`\n- PostgreSQL database with project documents loaded\n- OpenAI API key (for LLM access)\n- (Optional) PlantUML JAR for diagram generation\n\n### Installation\n\n1. **Clone the repository** and navigate to `App/chat-engine/`.\n\n2. **Install dependencies** using [uv](https://github.com/astral-sh/uv) (Optional, only for local usage):\n\n   ```bash\n   uv sync\n   ```\n\n3. **Set up environment variables**:\n\n   Create a `.env.private` file with at least:\n\n   ```\n   OPENAI_API_KEY=your-openai-key\n   ```\n\n   The database connection is using `Eprice/App/project.env`. For local use/dev you can include everything in your `.env.private`:\n\n   ```\n   OPENAI_API_KEY=your-openai-key\n   PGHOST=localhost\n   PGPORT=5432\n   PGUSER=your_pg_user\n   PGPASSWORD=your_pg_password\n   PGDATABASE=your_pg_db\n   ```\n\n4. **(Optional) PlantUML**:  \n   For local UML diagram generation, download `plantuml.jar` and place it in your home directory or adjust the path in `autoloading_uml.py`. You may also need to install Graphviz (`sudo apt install graphviz` or similar).\n\n5. **Prepare the database**:  \n   Ensure your PostgreSQL database is running and contains the project documents.\n\n## Running the Chat Engine\n\nAll commands assume you have an appropriate venv active, or you can replace `python` call with `uv run`. You can launch all apps at once:\n\n```bash\npython app.py\n```\n\nOr run individual apps:\n\n```bash\npython chat_app.py      # Main chat interface\npython agent_app.py     # Agent chat + UML viewer\npython file_app.py      # File browser\n```\n\n## Usage\n\n- Access the Gradio web UI at the address printed in the terminal (http://localhost:7860-63).\n- Chat with the agent to ask questions about the project, retrieve files, or request UML diagrams.\n- Use the file browser to view project files directly.\n\n## Notes\n\n- The chat engine does **not** require Ollama or any local LLMs. All LLM calls are made via OpenAI's API.\n- Embeddings are generated using HuggingFace models locally.\n- The chat engine is started by default in Docker Compose. To exclude it, edit the compose file and uncomment the lines in chat-engine service that define it as a profile.\n- For development, you may need to adjust paths or environment variables to match your setup.\n\n## Development\n\n- Dependencies are managed via `pyproject.toml` and can be installed with `uv`.\n- The code is modular and can be extended with new tools or retrieval strategies.\n- See the source files for more details on extending or customizing the chat engine.\n\n---\n\n**For any issues or questions, please contact the Eprice project maintainers.**",
  "./App/chat-engine/agent_app.py": "\"\"\"\nGradio app for a chat interface that streams responses from a chat manager.\nThis app uses a chat manager to handle conversation history and retrieve relevant information from project documents.\nThe chat manager is initialized with an agent that can use tools to search for information in the project documents.\nThe agent is capable of generating diagrams using PlantUML and can reference the documents used in its responses.\n\"\"\"\n\nimport gradio as gr\nfrom agent_manager import AgentManager\nfrom autoloading_uml import uml_viewer\nfrom langchain.schema import HumanMessage, AIMessage\nimport dotenv\ndotenv.load_dotenv(\".env.private\")\n\nchat_manager = AgentManager()\n\n    \n# clear all files from diagrams/ directory\ndef clear_diagrams():\n    import os\n    diagrams_dir = os.path.join(os.path.dirname(__file__), \"diagrams\")\n    for filename in os.listdir(diagrams_dir):\n        file_path = os.path.join(diagrams_dir, filename)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n\ndef lc_messages_to_gradio_history(messages):\n    \"\"\"\n    Convert a list of LangChain messages to Gradio chat history format:\n    List of [user_msg, assistant_msg] pairs.\n    \"\"\"\n    history = []\n    user_msg = None\n    for msg in messages:\n        if isinstance(msg, HumanMessage):\n            user_msg = msg.content\n        elif isinstance(msg, AIMessage) and user_msg is not None:\n            history.append([user_msg, msg.content])\n            user_msg = None\n    return history\n\n\nasync def chat_stream(messages, _):\n    async for chunk in chat_manager.stream_response(messages, mem=True):\n        yield chunk\n\n\nwith gr.Blocks(fill_height=True) as app:\n    with gr.Row(equal_height=True):\n        with gr.Column(scale=1):\n            gr.Markdown(\"# UML Diagrams\")\n            uml_viewer()\n        with gr.Column(scale=1):\n            gr.ChatInterface(\n                fn=chat_stream,\n                type=\"messages\",\n                title=\"Knowledge Base Agent\",\n                autoscroll=True,\n            )\n\n\nif __name__ == \"__main__\":\n    clear_diagrams()  # Clear diagrams directory on startup\n    app.queue().launch()",
  "./App/chat-engine/agent_manager.py": "\"\"\"\n# App/chat-engine/agent_manager.py\nThis module manages the agent for interacting with project documents.\nIt uses LangChain to create a retriever that can search and rerank documents,\ngenerate PlantUML diagrams, and handle user queries effectively.\n\"\"\"\n\nimport os\nimport torch\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\nfrom langchain_postgres import PGVector\nfrom langchain.chains import LLMChain\nfrom langchain_core.documents import Document\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\nfrom langchain_core.callbacks import BaseCallbackHandler\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nfrom utils.helpers import *\nfrom utils.tools import *\n\nfrom pydantic import Field\nfrom typing import List, Optional, Sequence\nfrom concurrent.futures import ThreadPoolExecutor\n\nretriever_prompt_template = \"\"\"Given the query:\\n{query}\\n\\n\nRate the relevance of the following document to the query on a scale from 1 to 10:\\n{document}\\n\\n\nOnly output the score as an integer.\n\"\"\"\n\nbatch_retriever_prompt_template = \"\"\"Given the query:\\n{query}\\n\\n\nRate the relevance of the following documents to the query on a scale from 1 to 10:\\n{documents}\\n\\n\nOnly output the scores as a list of integers.\n\"\"\"\n\nsystem_message = (\n            \"You are an expert assistant. Use the provided context from project documents to answer the user's question. \"\n            \"The topic is a software project with various documents including code, design, and architecture. \"\n            \"The application is called 'Eprice' and is a web application for viewing market electricity prices. \"\n            \"You have access to tools to search the project documents, retrieve files by name, and generate PlantUML diagrams from code or files. \"\n            \"Always first use the project_search tool to retrieve relevant information from the project documents. \"\n            \"Only if this does not provide sufficient information, then try looking up individual files using the get_file_by_name tool. \"\n            \"You can also use the get_project_directory_structure tool to understand the project structure and see what files are available. \"\n            \"You also have access to tools to generate PlantUML diagrams from code or files. \"\n            \"When appropriate, you can use these tools to generate diagrams to give a visual representation of the information. \"\n            \"If the answer is not in the documents/files, state that the documents do not contain the answer. \"\n            \"If the answer is in the documents/files, provide it and reference the document(s) used. \"\n            \"Do not make up information. \"\n        )\n\n\nclass LLMRerankerParallel(BaseDocumentCompressor):\n    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents in parallel\")\n    document_variable_name: str = \"document\"\n    top_k: int = 10  # Number of top documents to return\n\n    def _score_document(self, doc, query):\n        inputs = {\n            \"query\": query,\n            self.document_variable_name: [doc.page_content],\n        }\n        output = self.llm_chain.invoke(inputs)\n        try:\n            score = int(output.strip())\n        except Exception:\n            score = 0\n        return (doc, score)\n\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[list] = None,\n    ) -> List[Document]:\n        with ThreadPoolExecutor() as executor:\n            results = list(executor.map(lambda doc: self._score_document(doc, query), documents))\n        results.sort(key=lambda x: x[1], reverse=True)\n        return [doc for doc, _ in results[:self.top_k]]\n\nclass LLMRerankerBatched(BaseDocumentCompressor):\n    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents\")\n    document_variable_name: str = \"documents\"\n    top_k: int = 10  # Number of top documents to return\n\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[list] = None,\n    ) -> List[Document]:\n        inputs = {\n            \"query\": query,\n            self.document_variable_name: [doc.page_content for doc in documents],\n        }\n        output = self.llm_chain.invoke(inputs)\n        try:\n            scores = [int(score.strip()) for score in output.split(\",\")]\n        except Exception:\n            scores = [0] * len(documents)\n        scored_docs = list(zip(documents, scores))\n        scored_docs.sort(key=lambda x: x[1], reverse=True)\n        return [doc for doc, score in scored_docs[:self.top_k]]\n    \nclass Embedder:\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\"):\n        self.model_name = model_name\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=model_name,\n            model_kwargs={\"device\": self.device},\n            encode_kwargs={\"normalize_embeddings\": True},\n        )\n\n    def embed_query(self, text: str):\n        return self.embedding_model.embed_query(text)\n\n    def count_tokens(self, text: str) -> int:\n        return len(self.embedding_model.tokenizer.encode(text, add_special_tokens=False))\n\nclass VectorStore:\n    def __init__(self, collection_name, connection_string, embeddings, async_mode=True):\n        self.vector_store = PGVector(\n            embeddings=embeddings.embedding_model,\n            collection_name=collection_name,\n            connection=connection_string,\n            async_mode=async_mode,\n        )\n\n    def as_retriever(self, search_kwargs=None):\n        if search_kwargs is None:\n            search_kwargs = {\"k\": 20}\n        return self.vector_store.as_retriever(search_kwargs=search_kwargs)\n\nclass RerankingRetriever:\n    def __init__(\n        self,\n        vector_store,\n        reranker_model_name=\"gpt-4o-mini\",\n        rerank_top_k=10,\n        retriever_k=20,\n    ):\n        self.base_retriever = vector_store.as_retriever(search_kwargs={\"k\": retriever_k})\n        self.rerank_prompt = PromptTemplate(\n            input_variables=[\"query\", \"document\"],\n            template=retriever_prompt_template,\n        )\n        self.llm_reranker = ChatOpenAI(\n            model_name=reranker_model_name,\n            temperature=0.0\n        )\n        self.reranker_batched = LLMRerankerParallel(\n            llm_chain=self.rerank_prompt | self.llm_reranker,\n            top_k=rerank_top_k\n        )\n        self.retriever = ContextualCompressionRetriever(\n            base_retriever=self.base_retriever,\n            base_compressor=self.reranker_batched,\n        )\n\n    def as_tool(self, name=\"project_search\", description=\"Searches the project documents for relevant information.\"):\n        return create_retriever_tool(\n            self.retriever,\n            name=name,\n            description=description\n        )\n\nclass AgentManager:\n    def __init__(self, config=None):\n        self.config = config or {}\n        self._setup_env()\n        self._setup_embeddings()\n        self._setup_vector_store()\n        self._setup_retriever()\n        self._setup_tools()\n        self._setup_agent()\n        self.max_history = 10\n        self.chat_history = []\n\n    def _setup_env(self):\n        self.PGHOST = os.getenv(\"PGHOST\", \"localhost\")\n        self.PGPORT = os.getenv(\"PGPORT\")\n        self.PGUSER = os.getenv(\"PGUSER\")\n        self.PGPASSWORD = os.getenv(\"PGPASSWORD\")\n        self.PGDATABASE = os.getenv(\"PGDATABASE\")\n        self.connection_string = f\"postgresql+psycopg://{self.PGUSER}:{self.PGPASSWORD}@{self.PGHOST}:{self.PGPORT}/{self.PGDATABASE}\"\n        self.collection_name = \"project_documents\"\n\n    def _setup_embeddings(self):\n        self.embedding_model = Embedder(model_name=\"BAAI/bge-small-en\")\n\n    def _setup_vector_store(self):\n        self.vector_store = VectorStore(\n            collection_name=self.collection_name,\n            connection_string=self.connection_string,\n            embeddings=self.embedding_model,\n            async_mode=True,\n        )\n\n    def _setup_retriever(self):\n        self.reranking_retriever = RerankingRetriever(\n            vector_store=self.vector_store,\n            reranker_model_name=\"gpt-4o-mini\",\n            rerank_top_k=10,\n            retriever_k=20\n        )\n\n    def _setup_tools(self):\n        retriever_tool = self.reranking_retriever.as_tool(\n            name=\"project_search\",\n            description=\"Searches the project documents for relevant information.\"\n        )\n        self.tools = [retriever_tool,\n                      get_file_by_name_tool,\n                      generate_plantuml_diagram_from_file_tool,\n                      generate_plantuml_diagram_from_code_tool,\n                      get_project_directory_structure_tool]\n\n    def _setup_agent(self):\n        self.llm_streaming = ChatOpenAI(\n            model=\"gpt-4o-mini\",\n            temperature=0.2,\n            streaming=True,\n        )\n        self.agent = initialize_agent(\n            tools=self.tools,\n            llm=self.llm_streaming,\n            agent=AgentType.OPENAI_FUNCTIONS,\n            verbose=False,\n            system_message=system_message,\n        )\n    \n    def _add_to_history(self, user_input, ai_output):\n        self.chat_history.append(HumanMessage(content=user_input))\n        self.chat_history.append(AIMessage(content=ai_output))\n        while len(self.chat_history) > self.max_history * 2:\n            self.chat_history.pop(0)\n\n    def _history_as_string(self, new_message):\n        history_str = \"\"\n        for message in self.chat_history:\n            if isinstance(message, HumanMessage):\n                history_str += f\"User: {message.content}\\n\"\n            elif isinstance(message, AIMessage):\n                history_str += f\"AI: {message.content}\\n\"\n        history_str += f\"User: {new_message}\\n\"\n        return history_str\n\n    async def stream_response(self, query: str, mem: bool = True):\n        messages = self._history_as_string(query) if mem else query\n        response = \"\"\n        async for chunk in self.agent.astream(messages):\n            content = None\n            if hasattr(chunk, \"content\") and isinstance(chunk.content, str) and chunk.content.strip():\n                content = chunk.content\n            elif isinstance(chunk, dict):\n                if \"output\" in chunk and isinstance(chunk[\"output\"], str) and chunk[\"output\"].strip():\n                    content = chunk[\"output\"]\n                elif \"content\" in chunk and isinstance(chunk[\"content\"], str) and chunk[\"content\"].strip():\n                    content = chunk[\"content\"]\n            elif isinstance(chunk, str) and chunk.strip():\n                content = chunk\n            if content:\n                response += content\n                yield response\n        # After streaming, add to history\n        if mem:\n            self._add_to_history(query, response)\n        \n",
  "./App/chat-engine/app.py": "\"\"\"\nThis script launches two applications, `agent_app.py` and `chat_app.py`, as separate processes.\n\"\"\"\n\nimport subprocess\n\n# Launch agent_app.py\nagent_proc = subprocess.Popen([\"python\", \"agent_app.py\"])\n\n# Launch chat_app.py\nchat_proc = subprocess.Popen([\"python\", \"chat_app.py\"])\n\nfile_proc = subprocess.Popen([\"python\", \"file_app.py\"])\n\ntry:\n    chat_proc.wait()\n    agent_proc.wait()\n    file_proc.wait()\nexcept KeyboardInterrupt:\n    agent_proc.terminate()\n    chat_proc.terminate()\n    file_proc.terminate()\nfinally:\n    print(\"Processes terminated.\")\n    agent_proc.kill()\n    chat_proc.kill()\n    file_proc.kill()\n",
  "./App/chat-engine/autoloading_uml.py": "\"\"\"\nGradio component for viewing and refreshing a PlantUML diagram.\nThis component checks for changes in the PlantUML file and regenerates the diagram if necessary.\n\"\"\"\n\nimport gradio as gr\nimport os\nimport subprocess\n\n# This is to keep track of the last modification time of the PlantUML file\nlast_mtime = None\n\ndef get_diagram_paths():\n    # Always resolve relative to this script's directory\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    diagrams_dir = os.path.join(base_dir, \"diagrams\")\n    puml_path = os.path.join(diagrams_dir, \"diagram.puml\")\n    png_path = os.path.join(diagrams_dir, \"diagram.png\")\n    return puml_path, png_path\n\n\ndef check_and_generate_diagram():\n    global last_mtime\n    puml_path, png_path = get_diagram_paths()\n    if os.path.exists(puml_path):\n        mtime = os.path.getmtime(puml_path)\n        if last_mtime != mtime:\n            with open(puml_path, \"r\") as f:\n                code = f.read()\n            if not code.strip().startswith(\"@startuml\"):\n                code = f\"@startuml\\n{code}\\n@enduml\\n\"\n                with open(puml_path, \"w\") as f2:\n                    f2.write(code)\n            jar_path = os.path.expanduser(\"~/plantuml.jar\")\n            subprocess.run([\"java\", \"-jar\", jar_path, \"-tpng\", puml_path], cwd=os.path.dirname(puml_path))\n            last_mtime = mtime\n    if os.path.exists(png_path):\n        return png_path\n    return None\n\ndef uml_viewer():\n    with gr.Blocks() as uml:\n        output_image = gr.Image(label=\"Generated Diagram\", interactive=False)\n        timer = gr.Timer(2)\n        timer.tick(check_and_generate_diagram, outputs=output_image)\n        refresh_button = gr.Button(\"Refresh\")\n        refresh_button.click(check_and_generate_diagram, outputs=output_image)\n        uml.load(check_and_generate_diagram, outputs=output_image)\n    return uml",
  "./App/chat-engine/chat_app.py": "\"\"\"\nGradio app for a chat interface that streams responses from a chat manager.\nThis app uses a chat manager to handle conversation history and retrieve relevant information from project documents.\nThe chat can use tools to search for information in the project documents, and is also\ncapable of generating diagrams using PlantUML and can reference the documents used in its responses.\n\"\"\"\n\nimport gradio as gr\nfrom chat_manager_with_tools import ChatManagerWithTools\nimport dotenv\ndotenv.load_dotenv(\".env.private\")\n\nchat_manager = ChatManagerWithTools()\n\nasync def chat_stream(messages, _):\n    response = \"\"\n    async for chunk in chat_manager.stream_response(messages):\n        response += chunk\n        yield response\n\napp = gr.ChatInterface(\n        fn=chat_stream,\n        type=\"messages\",\n        title=\"Eprice project knowledge base.\",\n        autoscroll=True,\n        theme=\"soft\",\n    )\n\nif __name__ == \"__main__\":\n    app.queue().launch(pwa=True)",
  "./App/chat-engine/chat_manager_with_tools.py": "\"\"\"\nApp/chat-engine/agent_manager.py\nThis module manages the agent for interacting with project documents.\nIt uses a language model to rerank documents based on relevance to a query,\nand provides tools for searching and retrieving files from the project directory.\n\"\"\"\n\n\nimport os\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\nfrom langchain_postgres import PGVector\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langchain_core.documents import Document\n\n\nfrom pydantic import Field\nfrom typing import List, Optional, Sequence, Dict, Any\nimport json\n\nfrom utils.tools import (\n    get_file_by_name_tool,\n    get_project_directory_structure_tool,\n)\n\nretriever_prompt_template = \"\"\"Given the query:\\n{query}\\n\\n\nRate the relevance of the following document to the query on a scale from 1 to 10:\\n{document}\\n\\n\nOnly output the score as an integer.\n\"\"\"\n\nbatch_retriever_prompt_template = \"\"\"Given the query:\\n{query}\\n\\n\nRate the relevance of the following documents to the query on a scale from 1 to 10:\\n{documents}\\n\\n\nOnly output the scores as a list of integers.\n\"\"\"\n\nsystem_message = (\n            \"You are an expert assistant for answering questions about a software project named 'Eprice'.\\n\"\n            \"You have access to the following tools:\\n\"\n            \"- project_search: args: {\\\"query\\\": <string>}\\n\"\n            \"- get_file_by_name: args: {\\\"file_name\\\": <string>}\\n\"\n            \"- get_project_directory_structure: args: {}\\n\"\n            \"Always first use the project_search tool to retrieve relevant information from the project documents.\\n\"\n            \"If project_search does not provide sufficient information, then try looking up individual files using get_file_by_name.\\n\"\n            \"You can use get_project_directory_structure to understand the project structure and see what files are available.\\n\"\n            \"If you get name conflicts when you use get_file_by_name tool, try using the full file name with path.\\n\"\n            \"If the answer is in the documents or files, provide it and reference the document(s) or file(s) used.\\n\"\n            \"If the answer is not in the documents or files, state that the documents do not contain the answer.\\n\"\n            \"To use a tool, respond ONLY with a JSON block in this format:\\n\"\n            '{\"tool\": \"tool_name\", \"args\": {\"arg1\": \"value1\", ...}}.\\n'\n            \"Do not include any explanation or extra text outside the JSON block when calling a tool.\\n\"\n            \"Otherwise, answer the user's question directly and clearly.\"\n        )\n\nclass LLMReranker(BaseDocumentCompressor):\n    \"\"\"LLM Reranker that uses LLMChain to rerank documents.\n    It passes each document to the LLM and expects the LLM to return a score for each document.\n    The documents are then sorted by score and the top_k documents are returned.\n    \"\"\"\n    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents\")\n    document_variable_name: str = \"document\"\n    top_k: int = 5  # Number of top documents to return\n\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        *,\n        callbacks: Optional[list] = None,\n    ) -> List[Document]:\n        \n        scored_docs = []\n        for doc in documents:\n            inputs = {\n                \"query\": query,\n                self.document_variable_name: doc.page_content,\n            }\n            output = self.llm_chain.invoke(inputs)\n            try:\n                score = int(output.strip())\n            except Exception:\n                score = 0\n            scored_docs.append((doc, score))\n        scored_docs.sort(key=lambda x: x[[1]], reverse=True)\n        return [doc for doc, score in scored_docs[:self.top_k]]  # Return only top_k documents    \n\nclass LLMRerankerBatched(BaseDocumentCompressor):\n    \"\"\" LLM Reranker that uses LLMChain to rerank documents in batches.\n    It passes the documents to the LLM in a single call and expects the LLM to return a list of scores.\n    \"\"\"\n    llm_chain: object = Field(LLMChain, description=\"LLM chain to rerank documents in batches\")\n    document_variable_name: str = \"documents\"\n    top_k: int = 5  # Number of top documents to return\n\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        *,\n        callbacks: Optional[list] = None,\n    ) -> List[Document]:\n        \n        scored_docs = []\n        \n        inputs = {\n            \"query\": query,\n            self.document_variable_name: [doc.page_content for doc in documents],\n        }\n        output = self.llm_chain.invoke(inputs)\n        try:\n            scores = [int(score.strip()) for score in output.split(\",\")]\n        except Exception:\n            scores = [0] * len(documents)\n        scored_docs = list(zip(documents, scores))\n        scored_docs.sort(key=lambda x: x[1], reverse=True)\n        return [doc for doc, _ in scored_docs[:self.top_k]]\n    \nclass RerankingRetriever:\n    \"\"\"\n    Encapsulates a retriever with LLM-based reranking using ContextualCompressionRetriever.\n    \"\"\"\n    def __init__(\n        self,\n        vector_store,\n        reranker_model_name: str = \"gpt-4o-mini\",\n        rerank_top_k: int = 10,\n        retriever_k: int = 20,\n    ):\n        # Set up the base retriever\n        self.base_retriever = vector_store.as_retriever(search_kwargs={\"k\": retriever_k})\n\n        self.rerank_prompt = PromptTemplate(\n            input_variables=[\"query\", \"documents\"],\n            template=batch_retriever_prompt_template,\n        )\n\n        # Set up the reranker LLM\n        self.llm_reranker = ChatOpenAI(\n            model_name=reranker_model_name,\n            temperature=0.0\n        )\n\n        # Set up the reranker compressor\n        self.reranker_batched = LLMRerankerBatched(\n            llm_chain=self.rerank_prompt | self.llm_reranker,\n            top_k=rerank_top_k\n        )\n\n        # Compose the contextual compression retriever\n        self.retriever = ContextualCompressionRetriever(\n            base_retriever=self.base_retriever,\n            base_compressor=self.reranker_batched,\n        )\n\n    def as_tool(self, name=\"project_search\", description=\"Searches the project documents for relevant information.\"):\n        \"\"\"\n        Returns a retriever tool for use in tool-augmented chat.\n        \"\"\"\n        return create_retriever_tool(\n            self.retriever,\n            name=name,\n            description=description\n        )\n\n\nclass Embedder:\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\"):\n        self.model_name = model_name\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed the input text using the Hugging Face model.\"\"\"\n        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n        return embeddings.tolist()\n    \n    def _count_tokens(self, text: str) -> int:\n        \"\"\"Count the number of tokens in the input text.\"\"\"\n        return len(self.tokenizer.encode(text, add_special_tokens=False))\n\n    # This is solely for suppressing the token limit warning in the Hugging Face tokenizer\n    def count_tokens(self, text: str) -> int:\n        \"\"\"Count the number of tokens in the input text, handling texts longer than the model's max length.\"\"\"\n        max_length = getattr(self.tokenizer.model_max_length, \"item\", lambda: self.tokenizer.model_max_length)()\n        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n        if len(tokens) <= max_length:\n            return len(tokens)\n        # For long texts, split into chunks and sum\n        return sum(\n            len(tokens[i:i+max_length])\n            for i in range(0, len(tokens), max_length)\n        )\n    \nclass WrappedEmbedder:\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\"):\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=model_name,\n            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n            encode_kwargs={\"normalize_embeddings\": True},\n        )\n    \n    def embed_query(self, text: str) -> List[float]:\n        return self.embedding_model.embed_query(text)\n\n    \nclass VectorStore:\n    def __init__(self, collection_name: str, connection_string: str, embeddings: Embedder, async_mode: bool = False):\n        self.vector_store = PGVector(\n            embeddings=embeddings,\n            collection_name=collection_name,\n            connection= connection_string,\n            async_mode= async_mode,\n        )\n\n    def as_retriever(self, search_kwargs: dict = None):\n        if search_kwargs is None:\n            search_kwargs = {\"k\": 20}\n        return self.vector_store.as_retriever(search_kwargs=search_kwargs)\n    \n\nclass ChatManagerWithTools:\n    def __init__(self, config=None):\n        self.config = config or {}\n        self._setup_env()\n        self._setup_embeddings()\n        self._setup_vector_store()\n        self._setup_retriever()\n        self._setup_llm()\n        self._setup_memory()\n        self._setup_tools()\n        self._setup_prompt()\n\n    def _setup_env(self):\n        self.PGHOST = os.getenv(\"PGHOST\", \"localhost\")\n        self.PGPORT = os.getenv(\"PGPORT\")\n        self.PGUSER = os.getenv(\"PGUSER\")\n        self.PGPASSWORD = os.getenv(\"PGPASSWORD\")\n        self.PGDATABASE = os.getenv(\"PGDATABASE\")\n        self.connection_string = f\"postgresql+psycopg://{self.PGUSER}:{self.PGPASSWORD}@{self.PGHOST}:{self.PGPORT}/{self.PGDATABASE}\"\n        self.collection_name = \"project_documents\"\n\n    def _setup_embeddings(self):\n        self.embedding_model = Embedder(model_name=\"BAAI/bge-small-en\")\n\n    def _setup_vector_store(self):\n        self.vector_store = VectorStore(\n            collection_name=self.collection_name,\n            connection_string=self.connection_string,\n            embeddings=self.embedding_model,\n        )\n\n    def _setup_retriever(self):\n        self.reranking_retriever = RerankingRetriever(\n            vector_store=self.vector_store,\n            reranker_model_name=\"gpt-4o-mini\",\n            rerank_top_k=10,\n            retriever_k=20\n        ).retriever\n\n    def _setup_llm(self):\n        self.llm = ChatOpenAI(\n            model=\"gpt-4o-mini\",\n            temperature=0.2,\n            streaming=True,\n        )\n\n    def _setup_memory(self):\n        self.message_history = InMemoryChatMessageHistory()\n\n    def _setup_tools(self):\n        retriever_tool = self.reranking_retriever.as_tool(\n            name=\"project_search\",\n            description=\"Searches the project documents for relevant information.\"\n        )\n        self.tools = {\n            \"project_search\": retriever_tool,\n            \"get_file_by_name\": get_file_by_name_tool,\n            \"get_project_directory_structure\": get_project_directory_structure_tool,\n        }\n\n    def _setup_prompt(self):\n        self.system_message = system_message\n\n    def _count_memory_tokens(self) -> int:\n        return sum(self.embedding_model.count_tokens(msg.content) for msg in self.message_history.messages)\n\n    def _enforce_memory_limit(self, max_tokens=8000):\n        messages = self.message_history.messages\n        while self._count_memory_tokens() > max_tokens and messages:\n            # Remove the oldest message (after system message)\n            messages.pop(0)\n\n    # All the milk and honey is here\n    async def stream_response(self, user_message: str):\n        # Build chat history\n        history_msgs = self.message_history.messages\n        system_msg = SystemMessage(content=self.system_message)\n        user_msg = HumanMessage(content=user_message)\n        messages = [system_msg] + history_msgs + [user_msg]\n\n        # Stream LLM response\n        assistant_reply = \"\"\n        async for chunk in self.llm.astream(messages):\n            if hasattr(chunk, \"content\") and chunk.content:\n                assistant_reply += chunk.content\n \n        # Check if LLM wants to use a tool (by outputting a JSON block)\n        tool_call = self._extract_tool_call(assistant_reply)\n        if tool_call:\n            tool_name = tool_call.get(\"tool\")\n            args = tool_call.get(\"args\", {})\n            tool_func = self.tools.get(tool_name)\n            if tool_func:\n                tool_result = tool_func.invoke(args)\n                tool_context_msg = HumanMessage(\n                    content=f\"The result of your tool call `{tool_name}` is:\\n{tool_result}\\n\"\n                            \"Please use this information to answer the user's question.\"\n                )\n                messages.append(AIMessage(content=assistant_reply))\n                messages.append(tool_context_msg)\n                final_reply = \"\"\n                async for chunk in self.llm.astream(messages):\n                    if hasattr(chunk, \"content\") and chunk.content:\n                        final_reply += chunk.content\n                        yield chunk.content  # Only yield the final answer\n                assistant_reply = final_reply\n        else:\n            # Only yield if no tool call was made\n            yield assistant_reply\n\n        # Update memory\n        self.message_history.add_user_message(user_message)\n        self.message_history.add_ai_message(assistant_reply)\n        self._enforce_memory_limit(max_tokens=4000)\n\n    def _extract_tool_call(self, text: str) -> Dict[str, Any] | None:\n        # Look for a JSON block in the LLM output\n        try:\n            start = text.index(\"{\")\n            end = text.rindex(\"}\") + 1\n            json_block = text[start:end]\n            return json.loads(json_block)\n        except Exception:\n            return None\n        ",
  "./App/chat-engine/file_app.py": "import gradio as gr\nimport os\nfrom utils.db_calls import get_all_files\n\n# Load files and prepare lookup\nall_files = get_all_files()\nfile_names = [file[\"name\"] for file in all_files]\nfile_dict = {file[\"name\"]: file[\"content\"] for file in all_files}\n\ndef detect_type(filename):\n    ext = os.path.splitext(filename)[1].lower()\n    if ext in [\".py\", \".js\", \".json\", \".html\", \".css\", \".sh\", \".java\", \".c\", \".cpp\", \".ts\"]:\n        return \"code\"\n    elif ext in [\".md\", \".markdown\"]:\n        return \"markdown\"\n    else:\n        return \"text\"\n\ndef read_file(file_path):\n    content = file_dict.get(file_path, \"File not found.\")\n    file_type = detect_type(file_path)\n    return content, file_type\n\nwith gr.Blocks(fill_height=True) as app:\n    with gr.Row():\n        with gr.Column():\n            file_dropdown = gr.Dropdown(choices=file_names, label=\"Select a file\")\n            code_view = gr.Code(label=\"File Content\", visible=False)\n            md_view = gr.Markdown(label=\"File Content\", visible=False)\n            text_view = gr.Textbox(label=\"File Content\", lines=35, interactive=True, visible=False)\n\n            def update_view(file_path):\n                content, file_type = read_file(file_path)\n                # Hide all, show one\n                return (\n                    gr.update(visible=file_type == \"code\", value=content if file_type == \"code\" else None),\n                    gr.update(visible=file_type == \"markdown\", value=content if file_type == \"markdown\" else None),\n                    gr.update(visible=file_type == \"text\", value=content if file_type == \"text\" else None),\n                )\n\n            file_dropdown.change(\n                update_view,\n                inputs=file_dropdown,\n                outputs=[code_view, md_view, text_view]\n            )\n\nif __name__ == \"__main__\":\n    app.launch()",
  "./App/chat-engine/utils/__init__.py": "\"\"\"This module contains utility functions for processing documents, and managing databases and embeddings.\"\"\"\nimport dotenv\ndotenv.load_dotenv(\"../.env.private\")\n",
  "./App/chat-engine/utils/db_calls.py": "\"\"\"\n    Helper functions for managing database operations related to code and documents.\n    This module provides functions to save, load, and retrieve code and document entries,\n    including their embeddings, from a PostgreSQL database using the pgvector extension.\n    Functions assume that the database schema is already set up with appropriate tables.\n\"\"\"\nimport os\nimport psycopg\nimport numpy as np\n\n\ndef save_code_to_db(code_data, embedding_model):\n    # Connect to the database\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    # Table should already exist\n    # Insert code data into the table\n    for entry in code_data:\n        embdedding = embedding_model.embed_query(entry[\"docstring\"])\n        cur.execute(\"\"\"\n            INSERT INTO code (file, type, name, docstring, start_line, code, embedding)\n            VALUES (%s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (file, type, name, start_line) DO NOTHING\n        \"\"\", (\n            entry[\"file\"],\n            entry[\"type\"],\n            entry[\"name\"],\n            entry[\"docstring\"],\n            entry[\"start_line\"],\n            entry[\"code\"],\n            embdedding\n        ))\n    \n    conn.commit()\n    cur.close()\n    conn.close()\n\ndef load_code_from_db():\n    # Connect to the database\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    # Fetch all code data from the table\n    cur.execute(\"SELECT file, type, name, docstring, start_line, code FROM code\")\n    rows = cur.fetchall()\n\n    # Convert rows to a list of dictionaries\n    code_data = []\n    for row in rows:\n        code_data.append({\n            \"file\": row[0],\n            \"type\": row[1],\n            \"name\": row[2],\n            \"docstring\": row[3],\n            \"start_line\": row[4],\n            \"code\": row[5]\n        })\n\n    cur.close()\n    conn.close()\n    \n    return code_data\n\n\ndef get_most_similar_code_from_db(embedding_model, query, n=5):\n    import psycopg\n    import os\n\n    # Embed the query\n    query_embedding = embedding_model.embed_query(query)\n    # Ensure it's a list of floats (pgvector expects this format)\n    query_embedding = list(map(float, np.array(query_embedding).flatten()))\n\n    # Connect to the database\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    # Use the <-> operator for L2 distance (or use <#> for cosine distance if pgvector >= 0.5.0)\n    cur.execute(\"\"\"\n        SELECT file, type, name, docstring, start_line, code, embedding\n        FROM code\n        ORDER BY embedding <-> %s\n        LIMIT %s\n    \"\"\", (query_embedding, n))\n\n    rows = cur.fetchall()\n    code_data = []\n    for row in rows:\n        code_data.append({\n            \"file\": row[0],\n            \"type\": row[1],\n            \"name\": row[2],\n            \"docstring\": row[3],\n            \"start_line\": row[4],\n            \"code\": row[5],\n            \"embedding\": row[6]\n        })\n\n    cur.close()\n    conn.close()\n    return code_data\n\ndef retrieve_similar_code(query_embedding, n=5):\n    \"\"\"\n    Retrieve the n most similar code entries from the database using an embedded query vector.\n\n    Args:\n        query_embedding (list or np.ndarray): The query embedding vector.\n        n (int): Number of results to return.\n\n    Returns:\n        List[dict]: List of code entries with metadata and code.\n    \"\"\"\n    # Ensure embedding is a flat list of floats\n    if not isinstance(query_embedding, list):\n        import numpy as np\n        query_embedding = list(map(float, np.array(query_embedding).flatten()))\n\n    # Connect to the database\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    # Perform similarity search using pgvector\n    cur.execute(\"\"\"\n        SELECT file, type, name, docstring, start_line, code, embedding\n        FROM code\n        ORDER BY embedding <-> %s::vector\n        LIMIT %s\n    \"\"\", (query_embedding, n))\n\n    rows = cur.fetchall()\n    code_data = []\n    for row in rows:\n        code_data.append({\n            \"file\": row[0],\n            \"type\": row[1],\n            \"name\": row[2],\n            \"docstring\": row[3],\n            \"start_line\": row[4],\n            \"code\": row[5],\n            \"embedding\": row[6]\n        })\n\n    cur.close()\n    conn.close()\n    return code_data\n\ndef get_code_by_file_name(file_name):\n    \"\"\"\n    Retrieve code entries from the database by file name.\n\n    Args:\n        file_name (str): The name of the file to search for.\n\n    Returns:\n        List[dict]: List of code entries with metadata and code.\n    \"\"\"\n    # Connect to the database\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    # Perform the query\n    cur.execute(\"\"\"\n        SELECT file, type, name, docstring, start_line, code\n        FROM code\n        WHERE file = %s\n    \"\"\", (file_name,))\n\n    rows = cur.fetchall()\n    code_data = []\n    for row in rows:\n        code_data.append({\n            \"file\": row[0],\n            \"type\": row[1],\n            \"name\": row[2],\n            \"docstring\": row[3],\n            \"start_line\": row[4],\n            \"code\": row[5]\n        })\n\n    cur.close()\n    conn.close()\n    # if no rows are found, return an entry with empty values\n    if not code_data:\n        code_data.append({\n            \"file\": file_name,\n            \"type\": \"\",\n            \"name\": \"\",\n            \"docstring\": \"not available\",\n            \"start_line\": 0,\n            \"code\": \"# not available\"\n        })\n    return code_data\n\ndef save_documents_to_db(documents, embedding_model):\n    \"\"\"\n    Save document chunks (with embeddings) to the documents table in the database.\n\n    Args:\n        documents (list): List of dicts with keys 'file', 'type', 'content', and optionally 'embedding'.\n        embedding_model: Embedding model with an embed_query method.\n    \"\"\"\n    import psycopg\n    import os\n    import numpy as np\n\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    for doc in documents:\n        # Compute embedding if not already present\n        embedding = doc.get(\"embedding\")\n        if embedding is None:\n            embedding = embedding_model.embed_query(doc[\"content\"])\n        # Ensure embedding is a flat list of floats\n        embedding = list(map(float, np.array(embedding).flatten()))\n\n        cur.execute(\"\"\"\n            INSERT INTO documents (file, type, content, embedding)\n            VALUES (%s, %s, %s, %s)\n            ON CONFLICT (file, type) DO NOTHING\n        \"\"\", (\n            doc[\"file\"],\n            doc[\"type\"],\n            doc[\"content\"],\n            embedding\n        ))\n\n    conn.commit()\n    cur.close()\n    conn.close()\n\n\ndef load_documents_from_db():\n    \"\"\"\n    Retrieve all document entries from the documents table.\n\n    Returns:\n        list: List of dictionaries with document metadata and content.\n    \"\"\"\n    import psycopg\n    import os\n\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        SELECT file, type, content, embedding\n        FROM documents\n    \"\"\")\n\n    rows = cur.fetchall()\n    documents = []\n    for row in rows:\n        documents.append({\n            \"file\": row[0],\n            \"type\": row[1],\n            \"content\": row[2],\n            \"embedding\": row[3] if row[3] is not None else None\n        })\n\n    cur.close()\n    conn.close()\n    \n    return documents\n\n\ndef save_files_to_db(files):\n    \"\"\"\n    Save all files to the documents table in the database (no embedding).\n    Args:\n        files (list): List of dicts with keys 'file', 'type', 'content'.\n    \"\"\"\n    import psycopg\n    import os\n\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    for doc in files:\n        cur.execute(\"\"\"\n            INSERT INTO files (name, type, content)\n            VALUES (%s, %s, %s)\n            ON CONFLICT (name) DO NOTHING\n        \"\"\", (\n            doc[\"name\"],\n            doc[\"type\"],\n            doc[\"content\"]\n        ))\n\n    conn.commit()\n    cur.close()\n    conn.close()\n\ndef get_file_by_name(file_name):\n    \"\"\"\n    Retrieve a single document entry from the database by file name.\n\n    Args:\n        file_name (str): The name of the file to search for.\n\n    Returns:\n        dict or None: Document entry with metadata and content, or None if not found.\n    \"\"\"\n    import psycopg\n    import os\n\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        SELECT name, type, content\n        FROM files\n        WHERE name = %s\n        LIMIT 1\n    \"\"\", (file_name,))\n\n    row = cur.fetchone()\n    cur.close()\n    conn.close()\n\n    if row:\n        return {\n            \"name\": row[0],\n            \"type\": row[1],\n            \"content\": row[2]\n        }\n    else:\n        return None\n\ndef get_all_files():\n    \"\"\"\n    Retrieve all file entries from the database.\n\n    Returns:\n        list: List of dictionaries with file metadata and content.\n    \"\"\"\n    import psycopg\n    import os\n\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        SELECT name, type, content\n        FROM files\n    \"\"\")\n\n    rows = cur.fetchall()\n    files = []\n    for row in rows:\n        files.append({\n            \"name\": row[0],\n            \"type\": row[1],\n            \"content\": row[2]\n        })\n\n    cur.close()\n    conn.close()\n    \n    return files\n\ndef truncate_table(table_name):\n    \"\"\"\n    Truncate (remove all rows from) a table.\n    Args:\n        table_name (str): Name of the table to truncate.\n    \"\"\"\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n    cur.execute(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE;\")\n    conn.commit()\n    cur.close()\n    conn.close()\n\ndef drop_table(table_name, cascade=False):\n    \"\"\"\n    Drop a table from the database.\n    Args:\n        table_name (str): Name of the table to drop.\n        cascade (bool): Whether to use CASCADE option.\n    \"\"\"\n    conn = psycopg.connect(\n        dbname=os.getenv(\"PGDATABASE\"),\n        user=os.getenv(\"PGUSER\"),\n        password=os.getenv(\"PGPASSWORD\"),\n        host=os.getenv(\"PGHOST\"),\n        port=os.getenv(\"PGPORT\")\n    )\n    cur = conn.cursor()\n    sql = f\"DROP TABLE IF EXISTS {table_name} {'CASCADE' if cascade else ''};\"\n    cur.execute(sql)\n    conn.commit()\n    cur.close()\n    conn.close()",
  "./App/chat-engine/utils/helpers.py": "\"\"\"some helper functions for document processing\"\"\"\nimport subprocess\n\n# count tokens in the results\ndef count_tokens(text, tokenizer):\n    tokens = tokenizer.encode(text)\n    return len(tokens)\n\ndef count_all_tokens(texts, tokenizer):\n    total_tokens = 0\n    for text in texts:\n        tokens = tokenizer.encode(text)\n        total_tokens += len(tokens)\n    return total_tokens\n\ndef join_metadata(metadata):\n    key, value = list(metadata.items())[0]\n    return f\"{key}: {value}\"\n\ndef run_command(command):\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    assert result.returncode == 0, f\"Command '{command}' failed with error: {result.stderr}\"\n\ndef format_document(doc):\n    # Join all metadata key-value pairs as \"key: value\"\n    meta_str = \"\\n\".join(f\"{k}: {v}\" for k, v in doc.metadata.items())\n    # Combine metadata and content for the prompt\n    return f\"{meta_str}\\n{doc.page_content}\"\ndef format_documents(docs):\n    return \"\\n\\n\".join(format_document(doc) for doc in docs)\n\ndef format_code_entry(entry):\n    meta = [\n        f\"file: {entry.get('file', '')}\",\n        f\"type: {entry.get('type', '')}\",\n        f\"name: {entry.get('name', '')}\",\n        f\"start_line: {entry.get('start_line', '')}\"\n    ]\n    docstring = entry.get('docstring', '')\n    code = entry.get('code', '')\n    meta_str = \", \".join(meta)\n    docstring_str = f\"\\nDocstring:\\n{docstring}\" if docstring else \"\"\n    return f\"{meta_str}\\n{docstring_str}\\ncontent:\\n{code}\"\n",
  "./App/chat-engine/utils/tools.py": "\"\"\"Tools for interacting with the database to retrieve file information.\"\"\"\n\nfrom .db_calls import (\n    get_file_by_name,\n)\nimport os\nimport subprocess\nfrom langchain.tools import tool\n\n@tool\ndef get_project_directory_structure_tool() -> str:\n    \"\"\"\n    Returns the project directory structure as a string.\n    The structure shows what files and directories are available in the project.\n    The input argument is ignored.\n    \n    Returns:\n        str: The directory structure as a formatted string.\n    \"\"\"\n    # Get the project directory\n    fname = \"./data/project_structure.txt\"\n    # Run the command to get the directory structure\n    with open(fname, \"r\") as f:\n        lines = f.readlines()\n    # Format the output\n    output = \"\"\"\n    Project Directory Structure:\n    \"\"\"\n    for line in lines:\n        output += f\"{line.strip()}\\n\"\n    return output\n    \n\n@tool\ndef get_file_by_name_tool(file_name: str) -> str:\n    \"\"\"\n    Retrieve a single document entry from the database by file name. Preferably use the full path.\n    If multiple files have the same name, list all matches and ask for the full path.\n\n    Args:\n        file_name (str): The name of the file to retrieve. Should start with './' for relative paths.\n    Returns:\n        str: A formatted string with the file name, type, and content if found.\n             If multiple files match, lists all matches and asks for the full path.\n             If no file is found, returns a message indicating that.\n    \"\"\"\n    from .db_calls import get_file_by_name, get_all_files\n\n    # Try direct match (full path)\n    result = get_file_by_name(file_name)\n    if result:\n        return f\"File: {result['name']}\\nType: {result['type']}\\nContent:\\n{result['content']}\"\n\n    # If not found, try matching by filename only\n    all_files = get_all_files()\n    matches = [f for f in all_files if os.path.basename(f[\"name\"]) == os.path.basename(file_name)]\n\n    if not matches:\n        return f\"No file found with name: {file_name}\"\n\n    if len(matches) == 1:\n        f = matches[0]\n        return f\"File: {f['name']}\\nType: {f['type']}\\nContent:\\n{f['content']}\"\n\n    # Multiple matches\n    match_list = \"\\n\".join(f\"- {f['name']}\" for f in matches)\n    return (\n        f\"Multiple files found with the name '{os.path.basename(file_name)}':\\n\"\n        f\"{match_list}\\n\"\n        \"Please specify the full file path.\"\n    )\n\n\ndef _save_plantuml_code(code: str) -> str:\n    diagrams_dir = os.path.join(os.path.dirname(__file__), \"..\", \"diagrams\")\n    diagrams_dir = os.path.abspath(diagrams_dir)\n    os.makedirs(diagrams_dir, exist_ok=True)\n    diagram_path = os.path.join(diagrams_dir, \"diagram.puml\")\n    with open(diagram_path, \"w\") as f:\n        f.write(code)\n    return diagram_path\n\n@tool\ndef generate_plantuml_diagram_from_file_tool(file_name: str) -> str:\n    \"\"\"\n    Loads PlantUML code from a file in the database and saves it to ./diagrams/diagram.puml for rendering.\n    Returns a message indicating the diagram was generated.\n    \"\"\"\n    # Ensure file_name starts with ./\n    if not file_name.startswith(\"./\"):\n        file_name = \"./\" + file_name\n    file_entry = get_file_by_name(file_name)\n    if not file_entry or \"content\" not in file_entry:\n        return f\"No file found with name: {file_name}\"\n    code = file_entry[\"content\"].strip()\n    if not code.startswith(\"@startuml\"):\n        code = f\"@startuml\\n{code}\\n@enduml\"\n    diagram_path = _save_plantuml_code(code)\n    return f\"The diagram should be rendered automatically -- refresh the image is it's not showing up.\"\n\n@tool\ndef generate_plantuml_diagram_from_code_tool(plantuml_code: str) -> str:\n    \"\"\"\n    Saves PlantUML code (provided as a string) to ./diagrams/diagram.puml for rendering.\n    Returns a message indicating the file was saved.\n    \"\"\"\n    code = plantuml_code.strip()\n    if not code.startswith(\"@startuml\"):\n        code = f\"@startuml\\n{code}\\n@enduml\"\n    diagram_path = _save_plantuml_code(code)\n    return f\"The diagram should be rendered automatically -- refresh the image is it's not showing up.\"\n",
  "./App/client/Dockerfile": "FROM denoland/deno:alpine-2.0.2\n\nWORKDIR /app\n\nCOPY package.json .\n\nRUN DENO_FUTURE=1 deno install\n\nCOPY . .\n\nCMD [ \"run\", \"dev\", \"--host\" ]\n",
  "./App/client/README.md": "### Client/front template for Eprice app\n\nYou need to have deno installed: https://docs.deno.com/runtime/\n\n* missing project.env -- ask Paavo for this.\n\n* run this with docker compose.\n\n* first run `deno install --allow-scripts` in `client/` and `e2e-tests/` directories. This is likely to change in the future, but for now the denoland's alpine base image does not allow installing with optional flags (why this is the case beats me).\n\n",
  "./App/client/src/app.css": "@import \"tailwindcss\";\n@plugin \"@tailwindcss/forms\";\n\n@import \"@skeletonlabs/skeleton\";\n@import \"@skeletonlabs/skeleton/optional/presets\";\n@import \"@skeletonlabs/skeleton/themes/cerberus\";\n@import \"@skeletonlabs/skeleton/themes/rocket\";\n@import \"@skeletonlabs/skeleton/themes/crimson\";\n@import \"@skeletonlabs/skeleton/themes/legacy\";\n@import \"@skeletonlabs/skeleton/themes/nosh\";\n@import \"@skeletonlabs/skeleton/themes/terminus\";\n@import \"@skeletonlabs/skeleton/themes/catppuccin\";\n\n@source \"../node_modules/@skeletonlabs/skeleton-svelte/dist\";\n/*@tailwind base;\n@tailwind components;\n@tailwind utilities;\n*/\n\nhtml,\nbody {\n  @apply h-full;\n}\n\n#mainheading {\n  @apply mt-2 mb-4 text-3xl font-extrabold text-gray-900 dark:text-white md:text-4xl lg:text-6xl\n}\n\n#minorheading {\n  @apply mt-8 mb-4 font-extrabold text-gray-900 dark:text-white md:text-3xl lg:text-4xl\n}\n\n/* ChatBot styles */\n.chat-toggle {\n  position: fixed;\n  display: flex;\n  z-index: 10000; /* Ensure it appears above the chat window */\n  right: 0;\n  bottom: 800px; /* Default position when the chat is visible */\n  /* padding: 3.5rem 2rem; Increase padding for a larger button */\n  font-size: 1.25rem; /* Increase font size */\n}\n\n.chat-toggle.hidden {\n  right: fixed; /* Reset right alignment */\n  right: 0; /* Move to the left lower corner */\n  /* lift the button up slightly */\n  bottom: 3.5rem; /* Position at the bottom of the screen */\n}\n\n.stats-row {\n    display: flex;\n    gap: 1rem;\n    margin: 1rem 0;\n    justify-content: center;\n}\n.stat-card {\n    background: #f8fafc;\n    border: 1px solid #e5e7eb;\n    border-radius: 0.5rem;\n    padding: 1rem 2rem;\n    min-width: 120px;\n    text-align: center;\n    box-shadow: 0 1px 2px rgba(0,0,0,0.03);\n}\n.stat-label {\n    font-size: 1rem;\n    color: #64748b;\n    margin-bottom: 0.5rem;\n}\n.stat-value {\n    font-size: 1.25rem;\n    font-weight: bold;\n    color: #0f172a;\n}\n\n/* [data-theme='cerberus'] body {\n  background-image:\n    radial-gradient(at 24% 25%, color-mix(in oklab, var(--color-primary-500) 30%, transparent) 0px, transparent 30%),\n    radial-gradient(at 35% 13%, color-mix(in oklab, var(--color-success-500) 18%, transparent) 0px, transparent 30%),\n    radial-gradient(at 100% 64%, color-mix(in oklab, var(--color-error-500) 3%, transparent) 0px, transparent 40%);\n  background-attachment: fixed;\n  background-position: center;\n  background-repeat: no-repeat;\n  background-size: cover;\n} */\n",
  "./App/client/src/app.html": "<!doctype html>\n<html lang=\"en\">\n\t<head>\n\t\t<meta charset=\"utf-8\" />\n\t\t<link rel=\"icon\" href=\"%sveltekit.assets%/favicon.png\" />\n\t\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n\t\t<script src=\"https://kit.fontawesome.com/df7761c597.js\" crossorigin=\"anonymous\"></script>\n\t\t%sveltekit.head%\n\t</head>\n\t<body data-sveltekit-preload-data=\"hover\" data-theme=\"cerberus\">\n\t\t<div style=\"display: contents\">%sveltekit.body%</div>\n\t</body>\n</html>\n",
  "./App/client/src/hooks.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { decodeJwt } from \"jose\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\n// handle with decoding only\nexport const handle = async ({ event, resolve }) => {\n  const authCookie = event.cookies.get(COOKIE_KEY);\n  if (authCookie) {\n    try {\n      const payload = decodeJwt(authCookie);\n      event.locals.user = payload;\n    } catch (e) {\n      console.log(e);\n    }\n  }\n\n  return await resolve(event);\n};\n\n// handle using verify endpoint\nconst OLDhandle = async ({ event, resolve }) => {\n  const authCookie = event.cookies.get(COOKIE_KEY);\n  if (authCookie) {\n    const response = await fetch(`${PUBLIC_INTERNAL_API_URL}/api/auth/verify`, {\n      method: \"POST\",\n      headers: {\n        \"cookie\": `${COOKIE_KEY}=${authCookie}`,\n      },\n    });\n\n    // if response not ok, clear cookie and resolve\n    if (!response.ok) {\n      event.cookies.delete(COOKIE_KEY, { path: \"/\" });\n      return await resolve(event);\n    }\n\n    const responseCookies = response.headers.getSetCookie();\n    const cookie = responseCookies.find((cookie) =>\n      cookie.startsWith(COOKIE_KEY)\n    );\n\n    // if no cookie, resolve\n    if (!cookie) {\n      return await resolve(event);\n    }\n\n    const cookieValue = cookie.split(\"=\")[1].split(\";\")[0];\n    event.cookies.set(COOKIE_KEY, cookieValue, { path: \"/\", secure: false });\n\n    try {\n      const payload = decodeJwt(authCookie);\n      event.locals.user = payload;\n    } catch (e) {\n      console.log(e);\n    }\n  }\n\n  return await resolve(event);\n};\n",
  "./App/client/src/lib/apis/data-api.js": "import { PUBLIC_API_URL } from \"$env/static/public\";\n\nconst readData = async () => {\n    const response = await fetch(`${PUBLIC_API_URL}/api/data`, {\n        headers: {\n        \"Content-Type\": \"application/json\",\n        },\n        method: \"GET\",\n        credentials: \"include\", // Include cookies in the request\n    });\n    return await response.json();\n};\n\nconst readPublicData = async () => {\n    try {\n        const response = await fetch(`${PUBLIC_API_URL}/api/public/data`, {\n            headers: {\n            \"Content-Type\": \"application/json\",\n            },\n            method: \"GET\",\n        });\n        if (!response.ok) {\n            console.error(`API error! status: ${response.status}`);\n            return [];\n        } \n        \n        return await response.json(); \n    } catch (error) {\n        console.error('Failed to fetch public data:', error);\n\t\treturn [];\n    }  \n};\n\nconst readPriceRange = async (startTime, endTime) => {\n    const response = await fetch(`${PUBLIC_API_URL}/api/price/range`, {\n        headers: {\n        \"Content-Type\": \"application/json\",\n        },\n        credentials: \"include\",\n        method: \"POST\",\n        body: JSON.stringify({\n            startTime,\n            endTime,\n        }),\n    });\n    return await response.json();\n}\n\nexport { readData, readPublicData, readPriceRange };\n",
  "./App/client/src/lib/components/ChatBot.svelte": "<script>\n    // import { PUBLIC_CHAT_URL2 } from \"$env/static/public\";\n    import { PUBLIC_USER_CHAT_URL } from \"$env/static/public\";\n    \n    let chatOnState = $state(false);\n    const toggleChat = () => {\n        chatOnState = !chatOnState;\n    };\n    let user = $props();\n\n</script>\n\n<!-- Styles like chat-toggle are in app.css (mix with tailwind) -->\n<button\n    class=\"text-primary-500 font-bold  px-4 rounded chat-toggle {chatOnState ? '' : 'hidden'}\"\n    onclick={toggleChat}>\n    {chatOnState ? 'Hide Chat' : 'Open Chat'}\n</button>\n\n{#if chatOnState}\n    <div class=\"rounded-lg shadow-lg\">\n        <div class=\"gradio-embed\" style=\"position: fixed; bottom: 0; right: 0; width: 400px; height: 800px; z-index: 9999;\">\n            <iframe title=\"Gradio App\"\n                src=\"{PUBLIC_USER_CHAT_URL}\"\n                width=\"100%\"\n                height=\"100%\"\n                frameborder=\"20\"\n                allowfullscreen\n                name=\"gradio-iframe\"\n                class=\"rounded-t-lg\"\n                sandbox=\"allow-same-origin allow-scripts allow-popups allow-forms\"\n            ></iframe>\n        </div>\n    </div>\n{/if}\n",
  "./App/client/src/lib/components/ChatView1.svelte": "<script>\n    import { PUBLIC_CHAT_URL2 } from \"$env/static/public\";\n</script>\n\n<style>\n    .gradio-embed {\n        width: 100%;\n        height: 70vh; /* Adjust as needed, or use 100% if your layout is flex */\n        margin: 0;\n        padding: 0;\n    }\n    .gradio-embed iframe {\n        width: 100%;\n        height: 100%;\n    }\n</style>\n\n<div class=\"gradio-embed\" style=\"flex; 1 1 auto\" data-gradio-embed=\"false\">\n    <iframe title=\"Gradio App\"\n        src={PUBLIC_CHAT_URL2}\n        allowfullscreen\n        name=\"gradio-iframe\"\n        sandbox=\"allow-same-origin allow-scripts allow-popups allow-forms\"\n    ></iframe>\n</div>\n",
  "./App/client/src/lib/components/ChatView2.svelte": "<script>\n    import { PUBLIC_CHAT_URL3 } from \"$env/static/public\";\n</script>\n\n<style>\n    .gradio-embed {\n        width: 100%;\n        height: 70vh; /* Adjust as needed, or use 100% if your layout is flex */\n        margin: 0;\n        padding: 0;\n    }\n    .gradio-embed iframe {\n        width: 100%;\n        height: 100%;\n        border: none;\n        display: block;\n    }\n</style>\n\n<div class=\"gradio-embed\" data-gradio-embed=\"false\">\n    <iframe title=\"Gradio App\"\n        src={PUBLIC_CHAT_URL3}\n        allowfullscreen\n        name=\"gradio-iframe\"\n        sandbox=\"allow-same-origin allow-scripts allow-popups allow-forms\"\n    ></iframe>\n</div>",
  "./App/client/src/lib/components/ChatView3.svelte": "<script>\n    import { PUBLIC_CHAT_URL1 } from \"$env/static/public\";\n</script>\n\n<style>\n    .gradio-embed {\n        width: 100%;\n        height: 70vh; /* Adjust as needed, or use 100% if your layout is flex */\n        margin: 0;\n        padding: 0;\n    }\n    .gradio-embed iframe {\n        width: 100%;\n        height: 100%;\n        border: none;\n        display: block;\n    }\n</style>\n\n<div class=\"gradio-embed\" data-gradio-embed=\"false\">\n    <iframe title=\"Gradio App\"\n        src={PUBLIC_CHAT_URL1}\n        allowfullscreen\n        name=\"gradio-iframe\"\n        sandbox=\"allow-same-origin allow-scripts allow-popups allow-forms\"\n    ></iframe>\n</div>",
  "./App/client/src/lib/components/ChatView4.svelte": "\n<div class=\"container mx-auto p-4\">\n  <h1 class=\"text-3xl font-bold mb-4\">Developer Chat</h1>\n  <p class=\"mb-4\">\n    The Developer Chat is based on retrieval augmented generation (RAG) and uses\n    the project documentation and code as a knowledge base. It is designed to assist both developers\n    and those evaluating the project in understanding the codebase, answering questions, and providing insights.\n    There are two chat views available, both based on the same model (OpenAI's gpt-4o-mini).\n    Both models keep conversation history, so you can always ask follow-up questions to get more detailed answers.\n  </p>\n  <p class=\"mb-4\">\n    In addition to LLM's, you can also access the code base and documentation yourself.\n  </p>\n  <h2 class=\"text-2xl font-bold mb-2\">Files</h2>\n  <p class=\"mb-4\">\n    A view that allows you to scroll through the files in the codebase and documentation.\n    You can click on the files to view their contents. Both chat views have access to the same files,\n    and understand the directory structure of the codebase.\n  </p>\n  <h2 class=\"text-2xl font-bold mb-2\">Streaming</h2>\n  <p class=\"mb-4\">\n    A sreaming chat with access to the documents and the codebase (traditional chat view).\n    Based on a user query, the model retrieves relevant code snippets and\n    other documentatation related to the query. The retrieved content is then ranked and\n    only the most relevant texts are passed to the model as context information.\n    The model then generates a response based on the provided context. \n  </p>\n    <h2 class=\"text-2xl font-bold mb-2\">Agent</h2>\n    <p class=\"mb-4\">\n    An agent-based chat that uses the same model as the streaming chat, but with a different approach.\n    The agent can access the codebase and documentation using tool-calls, allowing it to retrieve relevant information dynamically.\n    This allows for more flexible and dynamic interactions, as the agent can provide insights and answers without being limited to\n    pre-defined context. The agent can also perform other actions for the user, such as searching the codebase or documentation,\n    and even generate UML diagrams based on the code structure.\n  </p>\n</div>\n",
  "./App/client/src/lib/components/PriceCards.svelte": "<script>\n    import { getStats } from \"$lib/utils/stats-helpers.js\";\n    let { values, kind, unit } = $props();\n    let stats = $state({});\n    $effect(() => {\n        if (values && values.length > 0) {\n            stats = getStats(values);\n        } else {\n            stats = undefined;\n        }\n    });\n</script>\n\n{#if stats}\n    <div class=\"stats-row\">\n        <div class=\"stat-card\">\n            <div class=\"stat-label\">Max {kind}</div>\n            <div class=\"stat-value\">{stats.max} {unit}</div>\n        </div>\n        <div class=\"stat-card\">\n            <div class=\"stat-label\">Min {kind}</div>\n            <div class=\"stat-value\">{stats.min} {unit}</div>\n        </div>\n        <div class=\"stat-card\">\n            <div class=\"stat-label\">Average {kind}</div>\n            <div class=\"stat-value\">{stats.mean} {unit}</div>\n        </div>\n        <div class=\"stat-card\">\n            <div class=\"stat-label\">Std. {kind}</div>\n            <div class=\"stat-value\">{stats.std} {unit}</div>\n        </div>\n    </div>\n{/if}\n\n",
  "./App/client/src/lib/components/Tabs.svelte": "<script>\n  import { createEventDispatcher } from 'svelte';\n  \n  let { activeTabValue, items } = $props();\n\n  const dispatch = createEventDispatcher();\n\n  const handleClick = tabValue => () => dispatch('change', tabValue);\n  let size=$state(\"\");\n\n  $effect(() => {\n    //This is really hacky, but it works for now\n    const activeItem = items.find(item => item.value === activeTabValue);\n    if (activeItem && activeItem.label === \"Agent\") {\n      size = \"max-auto\";\n    } else {\n      size = \"max-w-3xl\";\n    }\n  });\n</script>\n\n<div class=\"mx-auto {size} mt-12\">\n    <ul>\n    {#each items as item}\n        <li class={activeTabValue === item.value ? 'active' : ''}>\n            <button type=\"button\" onclick={handleClick(item.value)}\n                    class=\"tab-btn\"\n            >{item.label}\n            </button>\n        </li>\n    {/each}\n    </ul>\n    {#each items as item}\n        {#if activeTabValue == item.value}\n        <div class=\"box\">\n            <item.component />\n        </div>\n        {/if}\n    {/each}\n</div>\n\n\n<style>\n\t.box {\n\t\tmargin-bottom: 0px;\n\t\tpadding: 0px;\n\t\t/* border: 1px solid #dee2e6; */\n    border-radius: 0.5rem; /* Rounded all corners */\n    border: 0px solid var(--color-border, rgb(8, 40, 73)); /* Use preset or fallback */\n    border-top: 0;\n    /*background-image: url('$lib/assets/image3.png'); /* <-- Add this line */\n    background-size: cover;               /* Optional: cover the box */\n    background-repeat: no-repeat;         /* Optional: no repeat */\n    background-position: center;\n\t}\n  ul {\n    display: flex;\n    flex-wrap: wrap;\n    padding-left: 0;\n    margin-bottom: 0;\n    list-style: none;\n    border-bottom: 1px solid #dee2e6;\n  }\n\tli {\n\t\tmargin-bottom: -1px;\n\t}\n\n  .tab-btn {\n    border: 1px solid transparent;\n    border-top-left-radius: 0.25rem;\n    border-top-right-radius: 0.25rem;\n    display: block;\n    padding: 0.5rem 1rem;\n    cursor: pointer;\n    background: none;\n    color: inherit;\n    font: inherit;\n    outline: none;\n  }\n\n  .tab-btn:hover {\n    border-color: #e9ecef #e9ecef #dee2e6;\n  }\n\n  li.active > .tab-btn {\n    color: #495057;\n    background-color: #fff;\n    border-color: #dee2e6 #dee2e6 #fff;\n  }\n</style>\n",
  "./App/client/src/lib/components/layout/Clock.svelte": "<script>\n    import { clockStore } from \"$lib/utils/clock.js\";\n    const clock = clockStore({interval: 1000})\n</script>\n\n<p class=\"font-bold px-4 text-left w-full\">\n    <span>{$clock}</span>\n</p>\n",
  "./App/client/src/lib/components/layout/Footer.svelte": "<script>\n    let data = $props();\n    let theme = \"cerberus\"; // default theme\n\n    function setTheme(newTheme) {\n        theme = newTheme;\n        document.body.setAttribute(\"data-theme\", newTheme);\n    }\n</script>\n\n\n<!-- <footer class=\"p-4 bg-black border-t-1\">\n    <p class=\"text-center text-white font-bold\">\n        Dogs seem to dislike cats, sadly... © 2025\n    </p>\n</footer> -->\n<footer class=\"p-4 bg-black border-t-1\">\n    <div class=\"flex items-center justify-between\">\n        <button class=\"btn btn-sm text-white\" onclick={() => setTheme(\"cerberus\")}>Cerberus</button>\n        <button class=\"btn btn-sm text-white\" onclick={() => setTheme(\"crimson\")}>Crimson</button>\n        <button class=\"btn btn-sm text-white\" onclick={() => setTheme(\"legacy\")}>Legacy</button>\n        <button class=\"btn btn-sm text-white\" onclick={() => setTheme(\"nosh\")}>Nosh</button>\n        <button class=\"btn btn-sm text-white\" onclick={() => setTheme(\"terminus\")}>Terminus</button>\n        <button class=\"btn btn-sm text-white\" onclick={() => setTheme(\"catppuccin\")}>Catppuccin</button>\n        <button class=\"btn btn-sm text-white\" onclick={() => setTheme(\"rocket\")}>Rocket</button>\n    </div>\n    <!-- <p class=\"text-center text-white font-bold flex-1\">\n        Dogs seem to dislike cats, sadly... © 2025\n    </p> -->\n</footer>\n",
  "./App/client/src/lib/components/layout/Header.svelte": "<script>\n  import { onMount } from 'svelte';\n  import { page } from '$app/stores';\n  let { user } = $props();\n\n  let devChatAvailable = $state(false);\n\n  onMount(async () => {\n    const res = await fetch('/api/devchat');\n    const data = await res.json();\n    devChatAvailable = data.available;\n  });\n</script>\n\n<header class=\"flex items-center justify-between bg-black drop-shadow-xl backdrop-blur-lg p-4 mb-2\">\n<!--<header class=\"flex items-center bg-primary-300 p-4 mb-6\">-->\n  <h1 class=\"text-2xl text-white\">Electricity prices</h1>\n    <nav>\n      <ul class=\"ml-4 flex space-x-4 text-white\">\n        <li>\n          <a href=\"/\" class=\"\">Home</a>\n        </li>\n        {#if user}\n        <li>\n          <a href=\"/epc\" class=\"\">Production/Consumption</a>\n        </li>\n        <li>\n          <a href=\"/price\" class=\"\">Price</a>\n        </li>\n        {:else}\n            <li>\n              <a href=\"/auth/login\" class=\"\">Login</a>\n            </li>\n            <li>\n              <a href=\"/auth/register\" class=\"\">Register</a>\n          </li>\n        {/if}\n      </ul>\n    </nav>\n    {#if user && $page.url.pathname !== '/logout'}\n      <div class=\"ml-auto\">\n        <ul class=\"ml-4 flex space-x-4 text-white\">\n          {#if user.role === 'admin' && devChatAvailable}\n            <li>\n              <a href=\"/chat\" class=\"underline text-white font-bold\">Developer Chat</a>\n            </li>\n          {/if}\n          <li>\n            <a href=\"/auth/remove\" class=\"underline text-white\">Delete account</a>\n          </li>\n          <li>\n            <a href=\"/logout\" class=\"underline text-white\">Logout</a>\n          </li>\n        </ul>\n      </div>\n    {:else if $page.url.pathname === '/logout'}\n      <div class=\"ml-auto\">\n        <!--This is a hack to force the placement of other elements-->\n      </div>\n    {/if}\n</header>",
  "./App/client/src/lib/components/layout/User.svelte": "<script>\n    let { user } = $props();\n</script>\n\n{#if user?.email}\n    <p class=\"text-right text-gray-900 dark:text-white w-full px-4\">\n        Logged in as: <b>{user.email}</b>\n    </p>\n{/if}\n",
  "./App/client/src/lib/states/usePricesState.svelte.js": "// $lib/states/usePricesState.js\nimport { browser } from \"$app/environment\";\nimport * as dataApi from \"$lib/apis/data-api.js\";\n\nlet pricesState = $state([]); // Use empty array as default\n\nif (browser) {\n  pricesState = await dataApi.readPublicData()\n}\n\nconst usePricesState = () => {\n  return {\n    get data() {\n      return pricesState;\n    },\n    update: async () => {\n      pricesState = await dataApi.readPublicData();\n    },\n    state2js: () => {\n      return JSON.parse(JSON.stringify(pricesState));\n    },\n    get values() {\n      return pricesState.map((item) => item.value);\n    },\n    get labels() {\n      return pricesState.map((item) => item.startDate);\n    },\n  };\n};\n\nexport { usePricesState };\n",
  "./App/client/src/lib/states/userState.svelte.js": "let user = $state({ });\n\nconst useUserState = () => {\n  return {\n    get user() {\n      return user;\n    },\n    set user(u) {\n      user = u;\n    },\n  }; // you can also define other methods/properties here\n};\n\nexport { useUserState };",
  "./App/client/src/lib/utils/clock.js": "import { readable } from 'svelte/store'\n\nconst formatDate = (date) => {\n    return date.toString().replace(/\\s*\\(.*?\\)\\s*$/, \"\");\n}\n\nexport const clockStore = (options={}) => {\n    const initial = new Date()\n\n    // return a readable store\n    return readable(formatDate(initial), set => {\n        const update = () => set(formatDate(new Date()))\n        const interval = setInterval(update, options.interval || 1000)\n        return () => clearInterval(interval)\n    })\n}",
  "./App/client/src/lib/utils/date-helpers.js": "\nexport const datesInOrder = (startTime, endTime) => {\n    // This function checks if the start date is before or equal to the end date.\n    const start = new Date(startTime);\n    const end = new Date(endTime);\n    return start < end;\n};\n\nexport const datesCloseEnough = (startTime, endTime) => {\n    const start = new Date(startTime);\n    const end = new Date(endTime);\n    const sixMonthsAgo = new Date(end);\n    sixMonthsAgo.setMonth(sixMonthsAgo.getMonth() - 6);\n    if (start < sixMonthsAgo) {\n        return false; // start date is more than 6 months before end date\n    }\n    return true;\n};\n\nexport const getFormattedDates = (data, time_key = \"startTime\", value_key = \"value\") => {\n    // This function takes an array of objects and \n    // returns formatted and sorted dates and values.\n    const sorted = [...data].sort(\n        (a, b) => new Date(a[time_key]) - new Date(b[time_key])\n    );\n\n    // Map to Helsinki time and extract values\n    const labels = sorted.map(item =>\n        new Date(item[time_key]).toLocaleString(\"fi-FI\", {\n            timeZone: \"Europe/Helsinki\",\n            year: \"numeric\",\n            month: \"numeric\",\n            day: \"numeric\",\n            hour: \"2-digit\",\n            minute: \"2-digit\"\n        })\n    );\n    const values = sorted.map(item => item[value_key]);\n\n    return { values, labels };\n};\n\nexport const getTomorrow = () => {\n    // This function returns tomorrow's date in ISO format (YYYY-MM-DD)\n    const d = new Date();\n    d.setDate(d.getDate() + 1);\n    d.setHours(d.getHours() + 3);\n    return d.toISOString().slice(0, 10);\n}\n\nexport const isTodayHelsinki = (dateStr) => {\n    // This function checks if the given date string is today in Helsinki time\n        const d = new Date(dateStr);\n        const today = new Date().toLocaleDateString('fi-FI', { timeZone: 'Europe/Helsinki' });\n        const dDate = d.toLocaleDateString('fi-FI', { timeZone: 'Europe/Helsinki' });\n        return dDate === today;\n    }",
  "./App/client/src/lib/utils/stats-helpers.js": "export const getStats = (values) => {\n    if (!values || values.length === 0) return {};\n\n    const n = values.length;\n    const mean = values.reduce((a, b) => a + b, 0) / n;\n    const min = Math.min(...values);\n    const max = Math.max(...values);\n    const variance = values.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / n;\n    const std = Math.sqrt(variance);\n    const median = values.sort((a, b) => a - b)[Math.floor(n / 2)];\n\n    //round to 1 decimal places\n    return {\n        mean: Math.round(mean * 10) / 10,\n        min: Math.round(min * 10) / 10,\n        max: Math.round(max * 10) / 10,\n        variance: Math.round(variance * 10) / 10,\n        std: Math.round(std * 10) / 10,\n        median: Math.round(median * 10) / 10,\n        count: n,\n    };\n}",
  "./App/client/src/routes/+layout.js": "//export const prerender = true;\n",
  "./App/client/src/routes/+layout.server.js": "//export const ssr = true;\nexport const load = async ({ locals }) => {\n    return locals;\n  };",
  "./App/client/src/routes/+layout.svelte": "<script>\n    import \"../app.css\";\n    import { useUserState } from \"$lib/states/userState.svelte.js\";\n    import Header from \"$lib/components/layout/Header.svelte\";\n    import Footer from \"$lib/components/layout/Footer.svelte\";\n    import Clock from \"$lib/components/layout/Clock.svelte\";\n    import User from \"$lib/components/layout/User.svelte\";\n    import ChatBot from \"$lib/components/ChatBot.svelte\";\n\n    \n    let { children, data } = $props();\n    const userState = useUserState();\n    if (data.user) {\n      userState.user = data.user;\n    }\n\n  </script>\n\n<div class=\"flex flex-col h-full\">\n  \n  \n  <Header user={data.user} />\n  <div class=\"flex row\">\n    <Clock />\n    <User user={data.user} />\n  </div>\n  \n  <main class=\"container grow mx-auto\"> <!-- mx-auto grow max-w-4xl-->\n    {@render children()}\n  </main>\n\n  {#if data.user?.email}\n    <!-- <p class=\"text-right text-gray-500 dark:text-gray-400\">\n      Logged in as: <b>{data.user?.email}</b>\n    </p> -->\n\n    <ChatBot user={data.user} />\n  {/if}\n\n  <Footer />\n  \n</div>\n\n<!--\n<style>\n  :global(body){\n  background-image: url(\"https://images.pexels.com/photos/956981/milky-way-starry-sky-night-sky-star-956981.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: center;\n  font-family: 'Poppins', sans-serif;\n  font-size: 1.2rem\n  }\n</style>\n-->",
  "./App/client/src/routes/+page.server.js": "export const load = async ({ locals }) => {\n    return locals;\n  };",
  "./App/client/src/routes/+page.svelte": "<script>\n    import chartjs from 'chart.js/auto';\n    import { onMount } from \"svelte\";\n    import { usePricesState } from \"$lib/states/usePricesState.svelte\";\n    import { useUserState } from \"$lib/states/userState.svelte.js\";\n    import { isTodayHelsinki } from '$lib/utils/date-helpers';\n    import PriceCards from '$lib/components/PriceCards.svelte';\n\n    // TODO: switch to stateful variables\n    let priceCanvas;\n    let priceChart;\n    let prices = [];\n    let { data } = $props();\n    const userState = useUserState();\n    if (data.user) {\n      userState.user = data.user;\n    }\n\n    // Fetch prices on mount and after login/redirect\n    const fetchPrices = async () => {\n        const { data, update } = usePricesState();\n        await update(); // Always fetch fresh data\n        prices = data;\n    }\n\n    // Also fetch prices if the page becomes visible again (e.g. after login)\n    if (typeof window !== \"undefined\") {\n        window.addEventListener(\"visibilitychange\", () => {\n            if (document.visibilityState === \"visible\") fetchPrices();\n        });\n    }\n\n    // TODO: switch to stateful variables\n    let todayPrices = $state([]);\n    let todayValues = [];\n    let labels = [];\n\n    onMount(async () => {\n        await fetchPrices();\n        // prices.sort((a, b) => new Date(a.startDate) - new Date(b.startDate));\n        todayPrices = prices.filter(p => isTodayHelsinki(p.startDate));\n        // keep the first 24 hours of prices (Fix for Juho's issue)\n        // todayPrices = prices.slice(0, 24);\n        todayValues = todayPrices.map(p => p.price);\n        labels = todayPrices.map(p =>\n            new Date(p.startDate).toLocaleTimeString('fi-FI', {\n                timeZone: 'Europe/Helsinki',\n                hour: '2-digit',\n                minute: '2-digit'\n            })\n        );\n\n        // Draw or update chart\n        if (priceCanvas && labels.length) {\n            if (priceChart) priceChart.destroy();\n            priceChart = new chartjs(priceCanvas.getContext('2d'), {\n                type: \"bar\",\n                data: {\n                    labels,\n                    datasets: [{\n                        label: 'Price (c/kWh)',\n                        backgroundColor: 'rgba(10, 200, 245, 0.6)',\n                        borderColor: 'rgb(10, 200, 245)',\n                        data: todayValues\n                    }]\n                },\n                options: {\n                    responsive: true,\n                    interaction: { mode: 'index', intersect: false },\n                    plugins: { title: { display: false } },\n                }\n            });\n        }\n    });\n</script>\n\n<title>Home - Market Electricity Prices Today</title>\n<div class=\"max-w-3xl mx-auto mt-16\">\n    <h1 class=\"text-center text-3xl font-extrabold mb-8\">\n        Market Electricity Prices Today<br>\n        <span class=\"text-xl\">{new Date().toLocaleDateString('fi-FI', { timeZone: 'Europe/Helsinki' })}</span>\n    </h1>\n    <div class=\"shadow-lg p-4 border-1 border-primary-100 rounded-xl bg-white dark:bg-gray-800\">\n        <canvas bind:this={priceCanvas} id=\"priceChart\" style=\"width:100%;height:400px;\"></canvas>\n    </div>\n    <div class=\"py-8\">\n        <PriceCards values={todayValues} kind=\"price\" unit=\"c/kWh\"/>\n    </div>\n    {#if data.user?.email}\n    <div class=\"text-center\">\n        <p class=\"text-xl font-bold mb-4\">\n            Welcome back!<br />\n        </p>\n    </div>\n    {:else}\n    <div class=\"text-center\">\n        <p class=\"text-lg\">\n            Want to see more features and get the full functionality of the app?\n            <br />\n            <a href=\"http://localhost:5173/auth/register\" class=\"inline-block mt-6 px-4 py-2 bg-primary-500 text-white font-bold rounded hover:bg-primary-600 transition\">\n                Register for free\n            </a>\n        </p>\n    </div>\n    {/if}\n</div>\n\n",
  "./App/client/src/routes/api/devchat/+server.js": "import { json } from '@sveltejs/kit';\nimport { PUBLIC_INTERNAL_CHAT_URL } from '$env/static/public';\n\n/*\nThe purpose of this endpoint is to check if the chat service is available.\n*/\nexport const GET= async () => {\n    try {\n        const res = await fetch(`${PUBLIC_INTERNAL_CHAT_URL}`, {\n            method: 'GET',\n            headers: {\n                'Content-Type': 'application/json',\n            }\n        });\n        const text = await res.text();\n        // Check if response is HTML\n        if (res.ok && text.trim().startsWith('<!doctype html>')) {\n            console.log('Chat service is available');\n            return json({ available: true });\n        }\n    } catch (e) {}\n    return json({ available: false });\n}",
  "./App/client/src/routes/auth/[action]/+page.js": "import { error } from \"@sveltejs/kit\";\n\nexport const load = ({ params, url }) => {\n  if (params.action !== \"login\" && params.action !== \"register\" && params.action !== \"verify\" && params.action !== \"remove\") {\n    throw error(404, \"Page not found.\");\n  }\n\n  if (url.searchParams.has(\"registered\")) {\n    params.registered = true;\n  }\n\n  if (url.searchParams.has(\"email\")) {\n    params.email = url.searchParams.get(\"email\");\n  } else {\n    params.email = null;\n  }\n\n  if (url.searchParams.has(\"code\")) {\n    params.code = url.searchParams.get(\"code\");\n  } else {\n    params.code = null;\n  }\n  if (url.searchParams.has(\"is_verified\")) {\n    params.is_verified = true;\n  } else {\n    params.is_verified = null;\n  }\n\n  return params;\n};",
  "./App/client/src/routes/auth/[action]/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { redirect } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\nconst apiRequest = async (url, data) => {\n  return await fetch(`${PUBLIC_INTERNAL_API_URL}${url}`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify(data),\n  });\n};\n\nexport const actions = {\n\n  login: async ({ request, cookies }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/login\",\n      Object.fromEntries(data),\n    );\n\n    if (response.ok) {\n      const responseCookies = response.headers.getSetCookie();\n      const cookie = responseCookies.find((cookie) =>\n        cookie.startsWith(COOKIE_KEY),\n      );\n      const cookieValue = cookie.split(\"=\")[1].split(\";\")[0];\n      cookies.set(COOKIE_KEY, cookieValue, { path: \"/\", secure: false });\n      throw redirect(302, \"/\");\n    }\n    \n    return response.json();\n  },\n  \n  register: async ({ request }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/register\",\n      Object.fromEntries(data),\n    );\n\n    if (response.ok) {\n      throw redirect(302, \"/auth/verify?registered=true\");\n    }\n\n    return await response.json();\n  },\n\n  verify: async ({ request }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/verify\",\n      Object.fromEntries(data),\n    );\n\n    if (response.ok) {\n      throw redirect(302, \"/auth/login?is_verified=true\");\n    }\n\n    return await response.json();\n  },\n\n  remove: async ({ request }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/remove\",\n      Object.fromEntries(data),\n    );\n\n    if (response.ok) {\n      throw redirect(302, \"/logout?removed=true\");\n    } else {\n      throw redirect(302, \"/logout?removed=false&remove_error=true\");\n    }\n  }\n\n};\n",
  "./App/client/src/routes/auth/[action]/+page.svelte": "<script>\n    let { data, form } = $props();\n    let title = $state(\"\");\n    let email = $state(\"\");\n    let code = $state(\"\");\n\n    $effect(() => {\n        if (data.action === \"login\") {\n            title = \"Login\";\n        } else if (data.action === \"register\") {\n            title = \"Register\";\n        } else if (data.action === \"verify\") {\n            title = \"Verify\";\n        } else if (data.action === \"remove\") {\n            title = \"Remove\";\n        }\n        if (data.code && data.action === \"verify\") {\n            code = data.code;\n        }\n        if (data.email) {\n            email = data.email;\n        }\n    });\n\n</script>\n\n<div class=\"max-w-xl mx-auto mt-20\" >\n  <h1 id=\"minorheading\" class=\"text-center\"><!--class=\"text-xl pb-4\"> h2-->\n    {title} Form<!--{data.action === \"login\" ? \"Login\" : \"Register\"} form-->\n  </h1>\n  \n  {#if form?.message}\n    <p class=\"text-xl\">{form.message}</p>\n  {/if}\n\n  {#if form?.error}\n    <p class=\"text-xl text-red-500\">{form.error}</p>\n  {/if}\n\n  {#if data.registered}\n    <p class=\"text-xl\">\n      Verification has been sent to your email. Please verify your account to continue.\n    </p><br/>\n  {/if}\n\n  {#if data.is_verified}\n    <p class=\"text-xl\">\n      Your email has been verified. You can now login.\n    </p><br/>\n  {/if}\n\n  {#if form?.message===\"Email not verified.\"}\n    <p class=\"text-xl\">\n      Please check your email for the verification code.\n    </p><br/>\n  {/if}\n\n\n\n  <form class=\"space-y-4\" method=\"POST\" action=\"?/{data.action}\" enctype=\"application/json\">\n    <label class=\"label\" for=\"email\">\n      <span class=\"label-text\">Email</span>\n      <input\n        class=\"input\"\n        id=\"email\"\n        name=\"email\"\n        type=\"email\"\n        placeholder=\"Email\"\n        bind:value={email}\n        required\n      />\n    </label>\n    {#if data.action === \"verify\"}\n      <label class=\"label\" for=\"code\">\n        <span class=\"label-text\">Verification Code</span>\n        <input\n          class=\"input\"\n          id=\"code\"\n          name=\"code\"\n          type=\"text\"\n          bind:value={code}\n          required\n        />\n      </label>\n    <!--else if login or register, show password-->\n    {:else}\n      <label class=\"label\" for=\"password\">\n        <span class=\"label-text\">Password</span>\n        <input \n          class=\"input\"\n          id=\"password\"\n          name=\"password\"\n          type=\"password\"\n          required />\n      </label>\n    {/if}\n    <button class=\"w-full btn preset-filled-primary-500\" type=\"submit\">\n      {title}<!-- === \"login\" ? \"Login\" : \"Register\"}-->\n    </button>\n  </form>\n\n\n  {#if form?.message===\"Email not verified.\"}\n  <br/>\n  <div class=\"flex gap-2\">\n    <a href=\"/send\" class=\"btn preset-filled-primary-500\">\n      Resend Verification Email\n    </a>\n    <a href=\"/auth/verify\" class=\"btn preset-filled-primary-500 w-full\">\n      Verify Email\n    </a>\n  </div>\n  <br/>\n{/if}\n</div>",
  "./App/client/src/routes/chat/+page.svelte": "<script>\n    import ChatView1 from \"$lib/components/ChatView1.svelte\";\n    import ChatView2 from \"$lib/components/ChatView2.svelte\";\n    import ChatView3 from \"$lib/components/ChatView3.svelte\";\n    import ChatView4 from \"$lib/components/ChatView4.svelte\";\n    import Tabs from \"$lib/components/Tabs.svelte\";\n\n    let activeTabValue = $state(4);\n\n    let items = [\n        { label: \"Docs\",\n            value: 4,\n            component: ChatView4\n        },\n        { label: \"Files\",\n            value: 3,\n            component: ChatView3\n        },\n        { label: \"Streaming\",\n            value: 1,\n            component: ChatView1\n            },\n        { label: \"Agent\",\n            value: 2,\n            component: ChatView2\n            },\n    ];\n</script>\n\n\n<Tabs items={items} activeTabValue={activeTabValue} on:change={e => activeTabValue = e.detail} />",
  "./App/client/src/routes/epc/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { datesInOrder, getFormattedDates, datesCloseEnough } from '$lib/utils/date-helpers.js';\nimport { fail } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\n\nexport const actions = {\n    getCombinedRange: async ({ request, cookies }) => {\n        const fdata = await request.formData();\n        const data = Object.fromEntries(fdata);\n\n        if (!data.startTime || !data.endTime) {\n            return fail(400, { error: \"Start and end dates are required.\" });\n        } else if (!datesInOrder(data.startTime, data.endTime)) {\n            return fail(400, { error: \"Start date must come before end date.\" });\n        } else if (!datesCloseEnough(data.startTime, data.endTime)) {\n            return fail(400, { error: \"The date range must be within the last 6 months.\" });\n        }\n\n        try {\n            // Fetch datasets in parallel\n            const [prodRes, consRes, priceRes] = await Promise.all([\n                fetch(`${PUBLIC_INTERNAL_API_URL}/api/production/range`, {\n                    method: \"POST\",\n                    headers: {\n                        \"Content-Type\": \"application/json\",\n                        cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}`\n                    },\n                    body: JSON.stringify(data)\n                }),\n                fetch(`${PUBLIC_INTERNAL_API_URL}/api/consumption/range`, {\n                    method: \"POST\",\n                    headers: {\n                        \"Content-Type\": \"application/json\",\n                        cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}`\n                    },\n                    body: JSON.stringify(data)\n                }),\n                fetch(`${PUBLIC_INTERNAL_API_URL}/api/price/range`, {\n                    method: \"POST\",\n                    headers: {\n                        \"Content-Type\": \"application/json\",\n                        cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}`\n                    },\n                    body: JSON.stringify({\n                        startTime: data.startTime,\n                        endTime: data.endTime\n                    })\n                })\n            ]);\n\n            if (!prodRes.ok || !consRes.ok || !priceRes.ok) {\n                console.error(`API error! Production: ${prodRes.status}, Consumption: ${consRes.status}, Price: ${priceRes.status}`);\n                return fail(500, { error: \"API refusing to serve data...\" });\n            }\n\n            const productionData = await prodRes.json();\n            const consumptionData = await consRes.json();\n            const priceData = await priceRes.json();\n\n            const prodFormatted = getFormattedDates(productionData);\n            const consFormatted = getFormattedDates(consumptionData);\n            const priceFormatted = getFormattedDates(priceData, \"startDate\", \"price\");\n\n            // Use the longer label array for the x-axis\n            const labels =\n                prodFormatted.labels.length >= consFormatted.labels.length\n                    ? prodFormatted.labels\n                    : consFormatted.labels;\n\n            let productionValues = prodFormatted.values;\n            let consumptionValues = consFormatted.values;\n            let differenceValues = [];\n\n            const meanProduction = productionValues.reduce((sum, val) => sum + val, 0) / productionValues.length;\n            const meanConsumption = consumptionValues.reduce((sum, val) => sum + val, 0) / consumptionValues.length;\n            const meanDifference = meanProduction - meanConsumption;\n\n            differenceValues = productionValues.map((val, i) => val - (consumptionValues[i] ?? 0));\n            return {\n                priceLabels: priceFormatted.labels,\n                priceValues: priceFormatted.values,\n                labels,\n                productionValues,\n                consumptionValues,\n                differenceValues,\n                meanProduction,\n                meanConsumption,\n                meanDifference,\n                startTime: data.startTime,\n                endTime: data.endTime,\n                selection: data.selection\n            };\n            \n        } catch (e) {\n            return fail(500, { error: \"Server error\" });\n        }\n    }\n};",
  "./App/client/src/routes/epc/+page.svelte": "<script>\n    import PriceCards from '$lib/components/PriceCards.svelte';\n    import chartjs from 'chart.js/auto';\n    import { getTomorrow } from '$lib/utils/date-helpers.js';\n    import { enhance } from '$app/forms';\n    import { onMount } from 'svelte';\n\n    let { form } = $props();\n    let startTime = $state(form?.startTime || \"\");\n    let endTime = $state(form?.endTime || \"\");\n    let selection = $state(form?.selection || \"both\");\n    const maxDate = getTomorrow();\n\n    let labels = form?.labels || [];\n    let productionValues = form?.productionValues || [];\n    let consumptionValues = form?.consumptionValues || [];\n    let differenceValues = form?.differenceValues || [];\n    let priceLabels = form?.priceLabels || [];\n    let prices = form?.priceValues || [];\n\n    let chartCanvas;\n    let chart;\n    let chartType = $state(\"bar\");\n\n    let isLoading = $state(false);\n    let selectedValues1 = $state([]);\n    let selectedValues2 = $state([]);\n    let kind1 = $state(\"\");\n    let kind2 = $state(\"\");\n    let units1 = $state(\"\");\n    let units2 = $state(\"\");\n\n    let dataLoader = $state(false);\n\n    const getChartData = () => {\n        if (selection === \"both\") {\n            return {\n                labels,\n                datasets: [\n                    {\n                        label: 'Production',\n                        backgroundColor: 'rgba(245, 39, 157, 0.5)',\n                        borderColor: 'rgb(245, 39, 157)',\n                        data: productionValues,\n                        yAxisID: 'y',\n                    },\n                    {\n                        label: 'Consumption',\n                        backgroundColor: 'rgba(54, 162, 235, 0.5)',\n                        borderColor: 'rgb(54, 162, 235)',\n                        data: consumptionValues,\n                        yAxisID: 'y',\n                    }\n                ]\n            };\n        } else if (selection === \"difference\") {\n            return {\n                labels,\n                datasets: [\n                    {\n                        label: 'Production - Consumption',\n                        backgroundColor: 'rgba(157,39, 245, 0.7)',\n                        borderColor: 'rgb(157,39, 245)',\n                        data: differenceValues,\n                        yAxisID: 'y',\n                    }\n                ]\n            };\n        } else if (selection === \"price\") {\n            return {\n                labels: priceLabels,\n                datasets: [\n                    {\n                        label: 'Price',\n                        backgroundColor: 'rgba(75, 192, 192, 0.7)',\n                        borderColor: 'rgb(75, 192, 192)',\n                        data: prices,\n                        yAxisID: 'y',\n                    }\n                ]\n            };\n        }\n        return { labels: [], datasets: [] };\n    };\n\n    const toggleChartType = () => {\n        chartType = chartType === \"line\" ? \"bar\" : \"line\";\n        if (chart) {\n            chart.config.type = chartType;\n            chart.update();\n        }\n    };\n\n    onMount(() => {\n        chart = new chartjs(chartCanvas.getContext('2d'), {\n            type: chartType,\n            data: getChartData(),\n            options: {\n                responsive: true,\n                interaction: { mode: 'index', intersect: false },\n                stacked: false,\n                plugins: { title: { display: false } },\n                scales: { y: { type: 'linear', display: true, position: 'left' } }\n            }\n        });\n    });\n\n    $effect(() => {\n        // Update local state from form after submit\n        if (form) {\n            labels = form.labels || [];\n            productionValues = form.productionValues || [];\n            consumptionValues = form.consumptionValues || [];\n            differenceValues = form.differenceValues || [];\n            priceLabels = form.priceLabels || [];\n            prices = form.priceValues || [];\n            startTime = form.startTime || \"\";\n            endTime = form.endTime || \"\";\n        }\n        // Update chart data\n        if (chart) {\n            const data = getChartData();\n            chart.data.labels = data.labels;\n            chart.data.datasets = data.datasets;\n            chart.update();\n        }\n        // Update cards\n        if (selection === \"both\") {\n            selectedValues1 = productionValues;\n            selectedValues2 = consumptionValues;\n            kind1 = \"prod.\";\n            kind2 = \"cons.\";\n            units1 = \"kWh\";\n            units2 = \"kWh\";\n        } else if (selection === \"difference\") {\n            selectedValues1 = differenceValues;\n            selectedValues2 = [];\n            kind1 = \"difference\";\n            kind2 = \"\";\n            units1 = \"kWh\";\n            units2 = \"\";\n        } else if (selection === \"price\") {\n            selectedValues1 = prices;\n            selectedValues2 = [];\n            kind1 = \"price\";\n            kind2 = \"\";\n            units1 = \"c/kWh\";\n            units2 = \"\";\n        }\n        dataLoader = (labels.length > 0 || priceLabels.length > 0)\n    });\n</script>\n\n<div class=\"max-w-4xl mx-auto\" >\n    <div style=\"margin-top: 4rem; width: 100%; max-width: 1200px;\">\n        <h1 class=\"text-center text-3xl py-8 mt-8 mb-4 font-extrabold text-gray-900 dark:text-white\">\n            Production vs. Consumption\n        </h1>\n        <div class=\"shadow-lg p-4 mb-4 border-1 border-primary-100 rounded-xl bg-white dark:bg-gray-800 transition-all duration-300\">\n            <canvas bind:this={chartCanvas} id=\"mainChart\" class=\"\"></canvas>\n        </div>\n        <br/>\n        <form method=\"POST\" use:enhance={() => {\n                                    isLoading = true;\n                                    return async ({update}) => {\n                                        await update();\n                                        isLoading = false;\n                                    }\n                                }}\n            action=\"?/getCombinedRange\" class=\"mx-auto w-full max-w-md space-y-4\">\n            <div class=\"flex items-center justify-between gap-1\">\n                <label class=\"label\">\n                    <span class=\"label-text\">Start date</span>\n                    <input  class=\"input preset-outlined-primary-500\"\n                            name=\"startTime\"\n                            id=\"startTime\"\n                            type=\"date\"\n                            required\n                            bind:value={startTime}\n                            max={maxDate}\n                    />\n                </label>\n                <label class=\"label\">\n                    <span class=\"label-text\">End date</span>\n                    <input  class=\"input preset-outlined-primary-500\"\n                            name=\"endTime\"\n                            id=\"endTime\"\n                            type=\"date\"\n                            required\n                            bind:value={endTime}\n                            max={maxDate}\n                    />\n                </label>\n                <label class=\"label\">\n                    <span class=\"label-text\">Filter</span>\n                    <select class=\"select preset-outlined-primary-500\"\n                            id=\"selection\"\n                            name=\"selection\"\n                            bind:value={selection}\n                            disabled={!dataLoader}>\n                        <option value=\"both\">Prod./Cons.</option>\n                        <option value=\"difference\">Difference</option>\n                        <option value=\"price\">Price</option>\n                    </select>\n                </label>\n            </div>\n            <div class=\"flex items-center justify-between gap-1\">\n                <button class=\"w-full btn preset-filled-primary-500 hover:preset-filled-primary-500\"\n                        type=\"submit\"\n                        disabled={isLoading}>\n                    {#if isLoading}\n                        Loading...\n                    {:else}\n                        Retrieve data\n                    {/if}\n                </button>\n                <button class=\"btn preset-outlined-primary-500 hover:preset-filled-primary-500\"\n                        type=\"button\"\n                        onclick={toggleChartType}\n                        disabled={!dataLoader}>\n                    Toggle Chart Type\n                </button>\n            </div>\n        </form>\n        <div class=\"mt-10\">\n            {#if form?.error}\n                <div class=\"mt-10 text-center\">\n                    {form.error}\n                </div>\n            {/if}\n            <div>\n                <PriceCards values={selectedValues1} kind={kind1} unit={units1}/>\n            </div>\n            <div>\n                <PriceCards values={selectedValues2} kind={kind2} unit={units2}/>\n            </div>\n        </div>\n    </div>\n</div>\n\n",
  "./App/client/src/routes/logout/+page.js": "import { error } from \"@sveltejs/kit\";\n\nexport const load = ({ url, params }) => {\n  // If there are any params, this is not /logout\n  if (Object.keys(params).length > 0) {\n    throw error(404, \"Page not found.\");\n  }\n\n  return {\n    remove_error: url.searchParams.get(\"remove_error\") === \"true\",\n    removed: url.searchParams.get(\"removed\") === \"true\",\n  };\n};",
  "./App/client/src/routes/logout/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { redirect } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\nconst apiRequest = async (url) => {\n  return await fetch(`${PUBLIC_INTERNAL_API_URL}${url}`, {\n    method: \"GET\",\n  });\n};\n\nexport const actions = {\n\n  logout: async ({ cookies, locals }) => {\n    const response = await apiRequest(\"/api/auth/logout\");\n    if (response.ok) {\n      cookies.delete(COOKIE_KEY, { path: \"/\"});\n      //TODO: clear locals\n      // locals.user = undefined; // Clear user session in locals\n      throw redirect(302, \"/\");\n    } else {\n      return { error: \"Logout failed\" };\n    }\n  }\n\n};\n",
  "./App/client/src/routes/logout/+page.svelte": "<script>\n    let { data, form } = $props();\n</script>\n\n<div class=\"max-w-xl mx-auto mt-20\">\n<h1 id=\"minorheading\" class=\"text-center\"><!--class=\"text-xl pb-4\"> h2-->\n    Confirm your logout\n</h1>\n\n{#if form?.error}\n    <div class=\"alert alert-error\">\n        {form.error}\n    </div>\n{/if}\n\n{#if form?.message}\n    <div class=\"alert alert-success\">\n        {form.message}\n    </div>\n{/if}\n\n{#if data?.removed}\n    <p class=\"text-xl text-center\">\n        Your account has been removed.\n    </p>\n{/if}\n\n{#if data?.remove_error}\n    <p class=\"text-xl\">\n        User account could not be removed.<br/>contact site admin: eprice.varmennus@gmail.com.\n    </p>\n{/if}\n\n\n    <form method=\"POST\" action=\"?/logout\" class=\"p-8\">\n        <button class=\"w-full btn preset-filled-primary-500\" type=\"submit\">Logout</button>\n    </form>\n</div>\n",
  "./App/client/src/routes/price/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { datesInOrder, getFormattedDates, datesCloseEnough } from '$lib/utils/date-helpers.js';\nimport { fail } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\n\nexport const actions = {\n  getPriceRangeAll: async ({ request, cookies }) => {\n    const fdata = await request.formData();\n    const data = Object.fromEntries(fdata);\n\n    if (!data.startTime || !data.endTime) {\n      return fail(400, { error: \"Start and end dates are required.\" });\n    } else if (!datesInOrder(data.startTime, data.endTime)) {\n      return fail(400, { error: \"Start date must be before end date.\" });\n    } else if (!datesCloseEnough(data.startTime, data.endTime)) {\n      return fail(400, { error: \"The date range must be within the last 6 months.\" });\n    }\n\n    try {\n      // Fetch all three datasets in parallel\n      const [plainRes, weekdayRes, hourlyRes] = await Promise.all([\n        fetch(`${PUBLIC_INTERNAL_API_URL}/api/price/range`, {\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\", cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}` },\n          body: JSON.stringify(data)\n        }),\n        fetch(`${PUBLIC_INTERNAL_API_URL}/api/price/weekdayavg`, {\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\", cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}` },\n          body: JSON.stringify(data)\n        }),\n        fetch(`${PUBLIC_INTERNAL_API_URL}/api/price/hourlyavg`, {\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\", cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}` },\n          body: JSON.stringify(data)\n        })\n      ]);\n\n      if (!plainRes.ok || !weekdayRes.ok || !hourlyRes.ok) {\n        return fail(500, { error: \"API refusing to serve data...\" });\n      }\n\n      // Parse all datasets\n      const plainData = await plainRes.json();\n      const weekdayData = await weekdayRes.json();\n      const hourlyData = await hourlyRes.json();\n\n      const plainSorted = getFormattedDates(plainData, \"startDate\", \"price\");\n      const plainLabels = plainSorted.labels;\n      const plainValues = plainSorted.values;\n\n      const weekdayNames = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"];\n      const weekdaySorted = [...weekdayData].sort((a, b) => a.weekday - b.weekday);\n      const weekdayLabels = weekdaySorted.map(item => weekdayNames[item.weekday % 7]);\n      const weekdayValues = weekdaySorted.map(item => item.avgPrice);\n\n      const hourlySorted = [...hourlyData].sort((a, b) => a.hour - b.hour);\n      const hourlyLabels = hourlySorted.map(item => `${item.hour.toString().padStart(2, \"0\")}:00`);\n      const hourlyValues = hourlySorted.map(item => item.avgPrice);\n\n      return {\n        plainPriceLabels: plainLabels,\n        plainPriceValues: plainValues,\n        weekdayPriceLabels: weekdayLabels,\n        weekdayPriceValues: weekdayValues,\n        hourlyPriceLabels: hourlyLabels,\n        hourlyPriceValues: hourlyValues,\n        startTime: data.startTime,\n        endTime: data.endTime\n      };\n    } catch (e) {\n      return fail(500, { error: \"Server error\" });\n    }\n  }\n};\n\nconst OLDactions = {\n  getProductionRange: async ({ request, cookies }) => {\n    const fdata = await request.formData();\n    const data = Object.fromEntries(fdata);\n\n    if (!data.startTime || !data.endTime) {\n      return fail(400, { error: \"Start and end dates are required.\" });\n    } else if (!datesInOrder(data.startTime, data.endTime)) {\n      return fail(400, { error: \"Start date must be before end date.\" });\n    }\n\n    try {\n      const res = await fetch(`${PUBLIC_INTERNAL_API_URL}/api/production/range`, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\",\n            cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}`\n         },\n        body: JSON.stringify(data)\n      });\n\n      if (!res.ok) {\n        return fail(res.status, { error: \"API refusing to serve data...\" });\n      }\n\n      const productionData = await res.json();\n      const { values, labels } = getFormattedDates(productionData);\n      return { \n        productionValues: values,\n        productionLabels: labels,\n        startTime: data.startTime,\n        endTime: data.endTime\n      };\n    } catch (e) {\n      return fail(500, { error: \"Server error\" });\n    }\n  },\n\n  getConsumptionRange: async ({ request, cookies }) => {\n    const fdata = await request.formData();\n    const data = Object.fromEntries(fdata);\n\n    if (!data.startTime || !data.endTime) {\n      return fail(400, { error: \"Start and end dates are required.\" });\n    } else if (!datesInOrder(data.startTime, data.endTime)) {\n      return fail(400, { error: \"Start date must be before end date.\" });\n    }\n\n    try {\n      const res = await fetch(`${PUBLIC_INTERNAL_API_URL}/api/consumption/range`, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\",\n            cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}`\n         },\n        body: JSON.stringify(data)\n      });\n\n      if (!res.ok) {\n        return fail(res.status, { error: \"API refusing to serve data...\" });\n      }\n\n      const consumptionData = await res.json();\n      const { values, labels } = getFormattedDates(consumptionData);\n      return { \n        consumptionValues: values,\n        consumptionLabels: labels,\n        startTime: data.startTime,\n        endTime: data.endTime\n      };\n    } catch (e) {\n      return fail(500, { error: \"Server error\" });\n    }\n  },\n  getPriceRange: async ({ request, cookies }) => {\n    const fdata = await request.formData();\n    const data = Object.fromEntries(fdata);\n\n    if (!data.startTime || !data.endTime) {\n      return fail(400, { error: \"Start and end dates are required.\" });\n    } else if (!datesInOrder(data.startTime, data.endTime)) {\n      return fail(400, { error: \"Start date must be before end date.\" });\n    }\n\n    try {\n      const res = await fetch(`${PUBLIC_INTERNAL_API_URL}/api/price/range`, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\",\n            cookie: `${COOKIE_KEY}=${cookies.get(COOKIE_KEY)}`\n         },\n        body: JSON.stringify({\n          startTime: data.startTime,\n          endTime: data.endTime\n        })\n      });\n\n      if (!res.ok) {\n        return fail(res.status, { error: \"API refusing to serve data...\" });\n      }\n\n      const priceData = await res.json();\n      const datetimesUTC = priceData.map(item => item.startDate);\n      const datetimesHelsinki = datetimesUTC.map(dt =>\n        new Date(dt).toLocaleString(\"fi-FI\", {\n          timeZone: \"Europe/Helsinki\",\n          year: \"numeric\",\n          month: \"numeric\",\n          day: \"numeric\",\n          hour: \"2-digit\",\n          minute: \"2-digit\"\n        })\n      );\n      const priceValues = priceData.map(item => item.price);\n\n      return { \n        priceValues,\n        priceLabels: datetimesHelsinki,\n        startTime: data.startTime,\n        endTime: data.endTime\n      };\n    } catch (e) {\n      return fail(500, { error: \"Server error\" });\n    }\n  }\n  \n};",
  "./App/client/src/routes/price/+page.svelte": "<script>\n    import PriceCards from '$lib/components/PriceCards.svelte';\n    import chartjs from 'chart.js/auto';\n    import { getTomorrow } from '$lib/utils/date-helpers.js';\n    import { enhance } from '$app/forms';\n    import { onMount } from 'svelte';\n\n    let selection = $state(\"none\");\n    let { form } = $props();\n    let startTime = $state(form?.startTime || \"\");\n    let endTime = $state(form?.endTime || \"\");\n    const maxDate = getTomorrow();\n\n    // All datasets\n    let plainPriceLabels = form?.plainPriceLabels || [];\n    let plainPriceValues = form?.plainPriceValues || [];\n    let weekdayPriceLabels = form?.weekdayPriceLabels || [];\n    let weekdayPriceValues = form?.weekdayPriceValues || [];\n    let hourlyPriceLabels = form?.hourlyPriceLabels || [];\n    let hourlyPriceValues = form?.hourlyPriceValues || [];\n    let selectedValues = $state([]);\n    \n    let priceCanvas;\n    let priceChart;\n    let chartType = $state(\"bar\");\n    let isLoading = $state(false);\n\n    let dataLoader = $state(false);\n    //TODO: Weekday avg. not active if less than week of data\n\n    const toggleChartType = () => {\n        chartType = chartType === \"line\" ? \"bar\" : \"line\";\n        priceChart.config.type = chartType;\n        priceChart.update();\n    };\n\n    const getSelectedLabels = () => {\n        if (selection === \"weekdays\") return weekdayPriceLabels;\n        if (selection === \"hourly\") return hourlyPriceLabels;\n        return plainPriceLabels;\n    }\n    const getSelectedValues = () => {\n        if (selection === \"weekdays\") return weekdayPriceValues;\n        if (selection === \"hourly\") return hourlyPriceValues;\n        return plainPriceValues;\n    }\n\n    // Chart initialization\n    onMount(() => {\n        priceChart = new chartjs(priceCanvas.getContext('2d'), {\n            type: chartType,\n            data: {\n                labels: getSelectedLabels(),\n                datasets: [{\n                    label: 'Price',\n                    backgroundColor: 'rgba(10, 200, 245, 0.7)',\n                    borderColor: 'rgb(10, 200, 245)',\n                    data: getSelectedValues()\n                }]\n            },\n            options: {\n                responsive: true,\n                interaction: { mode: 'index', intersect: false },\n                plugins: { title: { display: false } },\n            }\n        });\n    });\n\n    $effect(() => {\n        // Update local state from form after submit\n        if (form) {\n            plainPriceLabels = form.plainPriceLabels || [];\n            plainPriceValues = form.plainPriceValues || [];\n            weekdayPriceLabels = form.weekdayPriceLabels || [];\n            weekdayPriceValues = form.weekdayPriceValues || [];\n            hourlyPriceLabels = form.hourlyPriceLabels || [];\n            hourlyPriceValues = form.hourlyPriceValues || [];\n            startTime = form.startTime || \"\";\n            endTime = form.endTime || \"\";\n        }\n        // Always update chart if chart exists (hacky way to ensure reactivity)\n        if (priceChart) {\n            priceChart.data.labels = getSelectedLabels();\n            priceChart.data.datasets[0].data = getSelectedValues();\n            priceChart.update();\n        }\n        if (selection === \"none\") {\n            selectedValues = plainPriceValues;\n        } else if (selection === \"weekdays\") {\n            selectedValues = weekdayPriceValues;\n        } else if (selection === \"hourly\") {\n            selectedValues = hourlyPriceValues;\n        }\n        dataLoader = (plainPriceLabels.length > 0 ||\n                      weekdayPriceLabels.length > 0 ||\n                      hourlyPriceLabels.length > 0);\n    });\n\n</script>\n\n<div class=\"max-w-4xl mx-auto\" >\n    <div style=\"margin-top: 4rem; width: 100%; max-width: 1200px;\">\n        <h1 id=\"\" class=\"text-center text-3xl py-8 mt-8 mb-4 font-extrabold text-gray-900 dark:text-white\">\n            Electricity Market Price\n        </h1>\n        <div class=\"shadow-lg p-4 mb-4 border-1 border-primary-100 rounded-xl bg-white dark:bg-gray-800 transition-all duration-300\">\n            <canvas bind:this={priceCanvas}\n                    id=\"priceChart\"\n                    class=\"\"\n                    ></canvas>\n        </div>\n        <br />\n        <div class=\"\">\n            <form method=\"POST\" use:enhance={() => {\n                                        isLoading = true;\n                                        return async ({update}) => {\n                                            await update();\n                                            isLoading = false;\n                                        }\n                                    }}\n                action=\"?/getPriceRangeAll\" class=\"mx-auto w-full max-w-md space-y-4\">\n                <div class=\"flex items-center justify-between gap-1\">\n                    <label class=\"label\">\n                        <span class=\"label-text\">Start date</span>\n                        <input  class=\"input preset-outlined-primary-500\"\n                                name=\"startTime\"\n                                id=\"startTime\"\n                                type=\"date\"\n                                required\n                                bind:value={startTime}\n                                max={maxDate}\n                            />\n                    </label>\n                    <label class=\"label\">\n                        <span class=\"label-text\">End date</span>\n                        <input  class=\"input preset-outlined-primary-500\"\n                                name=\"endTime\"\n                                id=\"endTime\"\n                                type=\"date\"\n                                required\n                                bind:value={endTime}\n                                max={maxDate}\n                        />\n                    </label>\n                    <label class=\"label\">\n                        <span class=\"label-text\">Averaging</span>\n                        <select class=\"select preset-outlined-primary-500\"\n                                bind:value={selection}\n                                id=\"selection\"\n                                disabled={!dataLoader}>\n                            <option value=\"none\">None</option>\n                            <option value=\"weekdays\">Weekdays avg</option>\n                            <option value=\"hourly\">Hourly avg.</option>\n                        </select>\n                    </label>\n                </div>\n                <div class=\"flex items-center justify-between gap-1\">\n                    <button class=\"w-full btn preset-filled-primary-500 hover:preset-filled-primary-500\"\n                            type=\"submit\"\n                            disabled={isLoading}>\n                        {#if isLoading}\n                            Loading...\n                        {:else}\n                            Retrieve data\n                        {/if}\n                    </button>\n                    <button class=\"btn preset-outlined-primary-500 hover:preset-filled-primary-500\"\n                            type=\"button\"\n                            onclick={toggleChartType}\n                            disabled={!dataLoader}>\n                        Toggle Chart Type\n                    </button>\n                </div>\n            </form>\n        </div>\n            {#if form?.error}\n                <div class=\"mt-10 text-center\">\n                    {form.error}\n                </div>\n            {/if}\n            <div class=\"py-8\">\n                <PriceCards values={selectedValues} kind=\"price\" unit=\"c/kWh\"/>\n            </div>\n    </div>\n</div>\n\n\n",
  "./App/client/src/routes/send/+page.server.js": "import { PUBLIC_INTERNAL_API_URL } from \"$env/static/public\";\nimport { redirect } from \"@sveltejs/kit\";\nimport { COOKIE_KEY } from \"$env/static/private\";\n\nconst apiRequest = async (url, data) => {\n  return await fetch(`${PUBLIC_INTERNAL_API_URL}${url}`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify(data),\n  });\n};\n\nexport const actions = {\n\n  resend: async ({ request, cookies }) => {\n    const data = await request.formData();\n    const response = await apiRequest(\n      \"/api/auth/resend\",\n      Object.fromEntries(data),\n    );\n    return response.json();\n  }\n};",
  "./App/client/src/routes/send/+page.svelte": "<script>\n    let { data, form } = $props();\n</script>\n\n<h1 id=\"minorheading\" class=\"text-center\">\n    Resend Verification Email\n</h1>\n  \n{#if form?.message}\n<p class=\"text-xl\">{form.message}</p>\n{/if}\n\n{#if form?.error}\n<p class=\"text-xl text-red-500\">{form.error}</p>\n{/if}\n\n<form class=\"space-y-4\" method=\"POST\" action=\"?/resend\" enctype=\"application/json\">\n    <label class=\"label\" for=\"email\">\n        <span class=\"label-text\">Email</span>\n        <input\n            class=\"input\"\n            id=\"email\"\n            name=\"email\"\n            type=\"email\"\n            placeholder=\"Email\"\n            required />\n    </label>\n    <button class=\"w-full btn preset-filled-primary-500\" type=\"submit\">\n      Send\n    </button>\n</form>\n",
  "./App/compose.yaml": "services:\n  database:\n    container_name: postgresql_database\n    image: reinikp2/pgvector-database:v1 #alt: postgres:17.0 + pgvector\n    restart: unless-stopped\n    env_file:\n      - project.env\n    ports: # TODO: comment out later\n      - 5432:5432 # Only for development/debugging\n    environment:\n      - TZ=Europe/Helsinki  # Set the timezone\n    volumes:\n      - ./data-preparation/data:/data\n      - ./pgdata:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  database-migrations:\n    image: flyway/flyway:10\n    env_file:\n      - project.env\n    depends_on:\n      database:\n        condition: service_started\n    volumes:\n      - ./data-preparation/data:/data\n      - ./database-migrations:/flyway/sql\n    command: -connectRetries=60 -baselineOnMigrate=true migrate\n      \n  client:\n    build: client\n    restart: unless-stopped\n    volumes:\n      - ./client:/app\n      - /app/node_modules # fixes node_modules issue -- Hurrays :)\n    ports:\n      - 5173:5173\n    depends_on:\n      - server\n    ulimits:\n      nofile:\n        soft: 65536\n        hard: 65536\n\n  e2e-tests:\n    entrypoint: \"/bin/true\"\n    build: e2e-tests\n    network_mode: host\n    depends_on:\n      - client\n    volumes:\n      - ./e2e-tests/tests:/app.tests\n\n  server:\n    build:\n      context: ./python-server  # Updated to point to the python-server directory\n    restart: unless-stopped\n    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload\n    volumes:\n      - ./python-server:/app\n    ports:\n      - 8000:8000\n    env_file:\n      - project.env\n    depends_on:\n      database:\n        condition: service_healthy\n      #- database\n    environment:\n      - TZ=Europe/Helsinki  # Set the timezone\n\n  data-preparation:\n    build:\n      context: ./data-preparation/scripts  # Here be Dockerfile\n    volumes:\n      - ./data-preparation/data:/data\n      - ./data-preparation/scripts:/scripts\n    working_dir: /scripts # this is already done in the Dockerfile (remove for production)\n    env_file:\n      - ./project.env\n    entrypoint: >\n      bash -c \"\n      ./retrieve_porssisahko_update.sh\n      #tail -f /dev/null # for debugging -- keeps the container running after the script finishes\n      \"\n    profiles:\n      - data-preparation\n\n  backend-tests:\n    build:\n      context: ./backend-tests\n    network_mode: host\n    depends_on:\n      - server\n      - database\n    env_file:\n      - ./project.env\n    volumes:\n      - ./backend-tests/tests:/app/tests\n      - ./python-server:/app/python-server\n    environment:\n      - PYTHONPATH=/app/python-server\n    profiles:\n      - backend-tests\n\n  chat-engine:\n    container_name: chat-engine\n    build:\n      context: .\n      dockerfile: chat-engine/Dockerfile\n    env_file:\n      - project.env\n    ports:\n      - 7860:7860\n      - 7861:7861\n      - 7862:7862\n    restart: always\n    # profiles:\n    #   - chat-engine\n    volumes:\n      - ./chat-engine:/app\n      - ~/.cache/huggingface:/root/.cache/huggingface  # to prevent downloading models every time\n    environment:\n      - HF_HOME=/root/.cache/huggingface\n",
  "./App/data-preparation/README.md": "# Data preparation service\n\nThis services is mainly meant for development, during which the base/historical data might still evolve. After launch for production, the service can be used in exceptional circumstances (i.e., app has been down for some reason) to populate database.\n\n## Data directory\n\nHolds `.csv` and `.xslx` files for bulk loading to database tables.\n\n## Scripts directory\n\nHolds the scripts for fetching data from external sources and for populating the database.\n",
  "./App/data-preparation/scripts/Dockerfile": "# python:3.10-slim has security issues, so we changed to python:3.13-slim\nFROM python:3.13-slim \n\n# Install required system packages (libpq-dev for psycopg2, vim for Q&D edits)\n# and gnumeric for Excel file handling\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    gnumeric \\\n    libpq-dev \\\n    vim \\\n    && apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies using pip\nRUN pip install --no-cache-dir \\\n    pandas \\\n    psycopg2-binary\n\n# Set the working directory\nWORKDIR /scripts\n\n# Copy the scripts into the container\nCOPY . /scripts\n\n# Ensure all .sh scripts are executable\nRUN chmod +x /scripts/*.sh\n\n# Default command (can be overridden in docker-compose)\nCMD [\"bash\"]",
  "./App/data-preparation/scripts/README.md": "### Scipts for populating the database\n\nThe historical data stays constant but as new datapoints arrive they are being fetched daily. If for some reason, the service has been down for a long time it might be best to run data-preparation service -- it get's the new historical data and inserts it into the database in bulk. This way the backend server is not needlessly burdened with dataloading on launch.\n\nThe service can be run using profile `data-preparation` (depends on database and migrations).\n\n**If dangling/orphan containers cause problems:** You can remove them by adding the `--remove-orphans` flag for compose. If you want to populate the database with your own scripts, you should start by tearing down (with the above flag), and then re-build. You can use a profile, or target the service with `-d data-preparation` argument for compose up.",
  "./App/data-preparation/scripts/clean_porssisahko.py": "import pandas as pd\nimport datetime\nimport sys\n\ndef clean_data(filename):\n    # Read the CSV file\n    df = pd.read_csv(filename, sep=\",\", encoding=\"utf-8\")\n\n    # Extract times and prices\n    times = df[\"Aika\"].tolist()\n    prices = df[df.columns[1]].tolist()\n\n    # Convert prices to float, handle errors by setting to average of adjacent values\n    for i in range(len(prices)):\n        try:\n            #prices[i] = float(prices[i].replace(\",\", \".\"))\n            prices[i] = float(prices[i])\n        except ValueError:\n            print(f\"Invalid price at index {i}: {prices[i]}\")\n            if i == 0:\n                prices[i] = prices[i + 1]\n            elif i == len(prices) - 1:\n                prices[i] = prices[i - 1]\n            else:\n                prices[i] = (prices[i - 1] + prices[i + 1]) / 2\n\n    # Extract year, month, day, hour, and weekday\n    years = []\n    months = []\n    days = []\n    hours = []\n    weekdays = []\n    dates = []\n    datetimes = []\n\n    for line in times:\n        #datetimes.append(line)\n        date = line.split(\" \")[0]\n        time = line.split(\" \")[1]\n        year, month, day = date.split(\"/\")\n        hour = time.split(\":\")[0]\n        \n        # datetime as \"YYYY-MM-DD HH:00:00\",always beginning hours\n        datetime_str = f\"{year}-{month.zfill(2)}-{day.zfill(2)} {hour.zfill(2)}:00:00\"\n        datetimes.append(datetime_str)\n\n        dates.append(date)\n        years.append(int(year))\n        months.append(int(month))\n        days.append(int(day))\n        hours.append(int(hour))\n        weekday = datetime.datetime(int(year), int(month), int(day)).weekday()\n        weekdays.append(weekday)        \n\n    # Create a new DataFrame\n    df2 = pd.DataFrame({\n        \"datetime\": datetimes,\n        \"date\": dates,\n        \"year\": years,\n        \"month\": months,\n        \"day\": days,\n        \"hour\": hours,\n        \"weekday\": weekdays,\n        \"price\": prices\n    })\n\n    # Save the cleaned data to a new CSV file\n    output_file = filename.replace(\".csv\", \"_cleaned.csv\")\n    df2.to_csv(output_file, index=False, sep=\";\")\n    print(f\"Cleaned data saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python clean_data.py <filename>\")\n        sys.exit(1)\n\n    input_file = sys.argv[1]\n    clean_data(input_file)",
  "./App/data-preparation/scripts/populate_porssisahko.py": "# this script populates the porssisahko table in the database\n# NOTE: environment variables are loaded by docker compose\n\n# whatever dependencies you need, add them\n# to Dockerfile or create a requirements.txt file\nimport psycopg2 # pip install psycopg2-binary\nimport pandas as pd\nimport sys\n\n\ndef populate_db(df):\n\n    conn = psycopg2.connect() # env is already loaded by docker compose\n    cursor = conn.cursor()\n\n    # Juho, Markus:\n    # the table creation is handled by migrations (let's not do it in scripts)\n\n    # Insert data into the table (on conflict do nothing)\n    for _, row in df.iterrows():\n        cursor.execute(\"\"\"\n            INSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (Datetime) DO NOTHING\n        \"\"\", (row['datetime'], row['date'], row['year'], row['month'], row['day'], row['hour'], row['weekday'], row['price']))\n    # Commit the changes and close the connection\n    conn.commit()\n    cursor.close()\n    conn.close()\n    \n    print(\"Data populated successfully.\")\n    print(f\"Inserted {len(df)} rows into the database.\")\n\nif __name__ == \"__main__\":\n    # csv file has been cleaned and is in the correct format\n    filename = sys.argv[1]\n    df = pd.read_csv(filename, sep=\";\", encoding=\"utf-8\")\n    # Populate the database\n    populate_db(df)\n    ",
  "./App/data-preparation/scripts/retrieve_porssisahko_update.sh": "#!/bin/bash\n\n# downloads the xlsx file from the porssisahko API\ncurl https://porssisahko.net/api/internal/excel-export --output ../data/updated_porssisahko.xlsx\n\n# checks if the file was downloaded successfully\nif [ $? -ne 0 ]; then\n    echo \"Failed to download the file\"\n    exit 1\nfi\n\ninput_file=\"../data/updated_porssisahko.xlsx\"\n# output filename is same as input file with .csv extension\noutput_file=\"${input_file%.*}.csv\"\n\n# ssconvert transforms the file to csv format in temp_file.csv\nssconvert \"$input_file\" temp_file.csv # for debugging\n\n# removes first 3 rows from temp_file.csv\nsed -i '1,3d' temp_file.csv\n\n# save the modified file as output_file\nmv temp_file.csv \"$output_file\"\n\necho \"File converted and saved as $output_file\"\n\n# python script to process the csv file (nicer format with pandas and datetime)\npython clean_porssisahko.py \"$output_file\"\n# the new name has \"_cleaned.csv\" appended to the original name\nrm -f \"$output_file\" # (comment for debugging)\nmv \"${output_file%.csv}_cleaned.csv\" \"$output_file\"\n\n# run the populate_porssisahko.py script\npython populate_porssisahko.py \"$output_file\"\n# print success message\necho \"Table porssisahko populated from file $output_file\"\n# remove the original file\n#rm -f \"$input_file\" # (comment for debugging)",
  "./App/database-migrations/V10__code_constraint.sql": "ALTER TABLE code\nADD CONSTRAINT code_file_type_name_start_line_unique UNIQUE (file, type, name, start_line);\n",
  "./App/database-migrations/V11__documents.sql": "\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    file TEXT NOT NULL,\n    type TEXT NOT NULL,\n    content TEXT NOT NULL,\n    embedding VECTOR(384),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    CONSTRAINT documents_unique_file_type UNIQUE (file, type)\n);",
  "./App/database-migrations/V12__files.sql": "\nCREATE TABLE files (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    type TEXT NOT NULL,\n    content TEXT NOT NULL,\n    CONSTRAINT file_unique_name UNIQUE (name)\n);\n",
  "./App/database-migrations/V13__fingrid.sql": "-- Migration script to create the fingrid table\nCREATE TABLE IF NOT EXISTS fingrid (\n    id SERIAL PRIMARY KEY,\n    datetime_orig TEXT NOT NULL, -- Original column for original datetime in text format\n    datetime TIMESTAMP NOT NULL, -- Original column for datetime\n    date DATE NOT NULL, -- New column for the date\n    year INT NOT NULL, -- Year, etc. for statistics\n    month INT NOT NULL,\n    day INT NOT NULL,\n    hour INT NOT NULL,\n    weekday INT NOT NULL,\n    dataset_id INT NOT NULL DEFAULT 0,\n    value NUMERIC(10, 3) NOT NULL,\n    predicted BOOLEAN NOT NULL DEFAULT FALSE,\n    createdAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Add a unique constraint to prevent duplicate rows (date and time)\nALTER TABLE fingrid\n    ADD CONSTRAINT unique_datetime_fg UNIQUE (datetime, dataset_id);",
  "./App/database-migrations/V14__fingrid_load_entries.sql": "-- Create a temporary table for staging the data\nCREATE TEMP TABLE fingrid_staging (\n    id SERIAL PRIMARY KEY,\n    datetime_orig TEXT NOT NULL, -- Original column for original datetime in text format\n    datetime TIMESTAMP NOT NULL, -- Original column for datetime\n    date DATE NOT NULL, -- New column for the date\n    year INT NOT NULL, -- Year, etc. for statistics\n    month INT NOT NULL,\n    day INT NOT NULL,\n    hour INT NOT NULL,\n    weekday INT NOT NULL,\n    dataset_id INT NOT NULL DEFAULT 0,\n    value NUMERIC(10, 3) NOT NULL,\n    predicted BOOLEAN NOT NULL DEFAULT FALSE,\n    createdAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Load data into the temporary table\nCOPY fingrid_staging (datetime_orig, datetime, date, year, month, day, hour, weekday, dataset_id, value)\nFROM '/data/fingrid_hourly.csv' \nDELIMITER ';' CSV HEADER;\n\n\nINSERT INTO fingrid (datetime_orig, datetime, date, year, month, day, hour, weekday, dataset_id, value)\nSELECT datetime_orig, datetime, date, year, month, day, hour, weekday, dataset_id, value\nFROM fingrid_staging\nON CONFLICT (datetime, dataset_id) DO NOTHING;\n\nDROP TABLE fingrid_staging;\n",
  "./App/database-migrations/V1__users.sql": "CREATE TABLE users (\n  id SERIAL PRIMARY KEY,\n  email TEXT NOT NULL,\n  password_hash TEXT NOT NULL\n);\n\nCREATE UNIQUE INDEX ON users(lower(email));\n",
  "./App/database-migrations/V2__porssisahko.sql": "CREATE TABLE IF NOT EXISTS porssisahko (\n    id SERIAL PRIMARY KEY,\n    datetime TIMESTAMP NOT NULL, -- Original column for datetime\n    date DATE NOT NULL, -- New column for the date\n    year INT NOT NULL, -- Year, etc. for statistics\n    month INT NOT NULL,\n    day INT NOT NULL,\n    hour INT NOT NULL,\n    weekday INT NOT NULL,\n    price NUMERIC(10, 3) NOT NULL,\n    predicted BOOLEAN NOT NULL DEFAULT FALSE,\n    createdAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Add a unique constraint to prevent duplicate rows (date and time)\nALTER TABLE porssisahko\n    ADD CONSTRAINT unique_datetime UNIQUE (datetime);\n",
  "./App/database-migrations/V3__timezone.sql": "ALTER DATABASE database SET timezone TO 'Europe/Helsinki';",
  "./App/database-migrations/V4__users_add_role.sql": "ALTER TABLE users\nADD COLUMN role VARCHAR(10) NOT NULL DEFAULT 'user';\n\nALTER TABLE users\nADD CONSTRAINT role_check CHECK (role IN ('user', 'admin'));\n\nUPDATE users\nSET role = 'user'\nWHERE role IS NULL;\n\n-- Insert a default admin user if it doesn't already exist\nINSERT INTO users (email, password_hash, role)\nVALUES ('test@test.com', '$2b$12$eh8m1dy3N2e/P5OvSuzHeeBwoaS9RbZPMThDhGoD0EuHrKbBq9JIW', 'admin')\nON CONFLICT ((lower(email))) DO NOTHING;\n\n-- Insert a default user if it doesn't already exist\nINSERT INTO users (email, password_hash)\nVALUES ('testi@testi.fi', '$2b$12$j6.jBujeoaNAenFbA/iELeoy2.Jlt9jNV.NunCCPZHet.z/4lKJtu')\nON CONFLICT ((lower(email))) DO NOTHING;",
  "./App/database-migrations/V5__porssisahko_load_entries.sql": "-- Create a temporary table for staging the data\nCREATE TEMP TABLE porssisahko_staging (\n    id SERIAL PRIMARY KEY,\n    datetime TIMESTAMP NOT NULL, -- Original column for datetime\n    date DATE NOT NULL, -- New column for the date\n    year INT NOT NULL, -- Year, etc. for statistics\n    month INT NOT NULL,\n    day INT NOT NULL,\n    hour INT NOT NULL,\n    weekday INT NOT NULL,\n    price NUMERIC(10, 3) NOT NULL,\n    predicted BOOLEAN NOT NULL DEFAULT FALSE,\n    createdAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Load data into the temporary table\nCOPY porssisahko_staging (datetime, date, year, month, day, hour, weekday, price)\nFROM '/data/porssisahko.csv'\nWITH CSV HEADER DELIMITER ';';\n\n-- Insert unique rows into the target table\nINSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\nSELECT datetime, date, year, month, day, hour, weekday, price\nFROM porssisahko_staging\nON CONFLICT (datetime) DO NOTHING;\n\n-- Drop the temporary table\nDROP TABLE porssisahko_staging;\n",
  "./App/database-migrations/V6__users_add_isverified.sql": "ALTER TABLE users\nADD COLUMN is_verified BOOLEAN NOT NULL DEFAULT FALSE;\n\nALTER TABLE users\nADD COLUMN verification_code VARCHAR(7);\n\n-- Set is_verified to TRUE and verification_code to 'ABC-123' for existing users\nUPDATE users\nSET is_verified = TRUE,\n    verification_code = 'ABC-123'\nWHERE is_verified = FALSE OR verification_code IS NULL;\n\n-- Enforce that verification_code is always exactly 7 characters\nALTER TABLE users\nADD CONSTRAINT verification_code_length CHECK (char_length(verification_code) = 7);\n",
  "./App/database-migrations/V8__extension_vector.sql": "CREATE EXTENSION vector;\n",
  "./App/database-migrations/V9__code.sql": "\nCREATE TABLE code (\n    id SERIAL PRIMARY KEY,\n    file TEXT NOT NULL,\n    type TEXT NOT NULL, -- 'function', 'class', 'document', etc.\n    name TEXT,\n    docstring TEXT,\n    start_line INTEGER,\n    code TEXT,\n    embedding VECTOR(384),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);",
  "./App/e2e-tests/Dockerfile": "FROM mcr.microsoft.com/playwright:v1.48.1-jammy\n\nWORKDIR /app\n\nCOPY package*.json .\nCOPY *config.js .\n\nRUN npm install\nRUN npx playwright install chromium\n\nCOPY . .\n\nCMD [ \"npc\", \"playwright\", \"test\" ]\n",
  "./App/e2e-tests/tests/frontend.spec.js": "const { test, expect } = require(\"@playwright/test\");\n\ntest('Home page loads successfully', async ({ page }) => {\n  await page.goto(\"http://localhost:5173/\");\n  // Check if the page title is correct\n  await expect(page).toHaveTitle(\"Home - Market Electricity Prices Today\");\n});\n\ntest('Home page has the chart visible.', async ({ page }) => {\n  await page.goto(\"http://localhost:5173/\");\n  // wait for a second for the page to load\n  await page.waitForTimeout(1000);\n  const canvas = page.locator(\"#priceChart\");\n  await expect(canvas).toBeVisible();\n});\n",
  "./App/project.env": "FLYWAY_USER=username\nFLYWAY_PASSWORD=password\nFLYWAY_URL=jdbc:postgresql://postgresql_database:5432/database\n\nPOSTGRES_USER=username\nPOSTGRES_PASSWORD=password\nPOSTGRES_DB=database\n\nPGUSER=username\nPGPASSWORD=password\nPGDATABASE=database\nPGHOST=postgresql_database\nPGPORT=5432\n",
  "./App/python-server/Dockerfile": "# Python 3.11 has vulnerability issues\nFROM python:3.13-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the requirements file into the container\nCOPY requirements.txt .\n\nRUN pip install --upgrade pip\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n#RUN pip install --upgrade passlib\n# incompatibility issues with new bcrypt (revert to old version)\nRUN pip install bcrypt==3.2.0\n# Copy the rest of the application code into the container\nCOPY . .\n\n# Install curl for debugging\nRUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*\n\n# Command to run the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n",
  "./App/python-server/README.md": "\n## Python FastAPI server template for Eprice app\n\n**See parent `README.md`.**\n\nYou need to define `.env` -file for production. The defaults should work for development purposes. See config folder.\n\n* requirements.txt has all the requirements -- if you add more, upodate the requirements.\n\n### Dockerized\n\nThe server is meant to be run  from docker container, and it is using dockerized Postgres database by default. It is also possible to develop it without docker by setting up an independent Postgres database, by running the postgres container, or by ignoring the database for unrelated dev. You can also define you own SQLite database locally from the python-server -- just change how the repositories makes the connection.\n\n* .dockerignore has the basics, but if you use some other .venv naming convention, add them. The container uses pip and requirements to install dependencies -- so using managers like uv/pip/poetry might produce files/folders you must exclude from docker (it copies all contents not excluded into the container).\n\n* .gitignore, same things. No venv's.\n\n",
  "./App/python-server/config/__init__.py": "\"\"\"\nconfig package initializer\n\nLoads environment variables from .env files for the Eprice backend.\n\"\"\"\n\n\nimport dotenv\ndotenv.load_dotenv(\".env.development\")\ndotenv.load_dotenv(\".env.local\")\n",
  "./App/python-server/config/secrets.py": "\"\"\"\nsecrets.py\n\nConfiguration and secrets for the Eprice backend.\n\nThis module loads environment variables for database, JWT, and email settings.\nIt also defines the list of public routes that do not require authentication.\n\"\"\"\n\nimport os\n\n# Database configuration\n# check if environment variables for postgres are set, otherwise use default values\n# Default values are for development purposes only\n# and should not be used in production\n\nif os.getenv(\"POSTGRES_USER\") is None:\n    os.environ[\"POSTGRES_USER\"] = \"username\"\nif os.getenv(\"POSTGRES_PASSWORD\") is None:\n    os.environ[\"POSTGRES_PASSWORD\"] = \"password\"\nif os.getenv(\"PGHOST\") is None:\n    os.environ[\"PGHOST\"] = \"postgresql_database\"\nif os.getenv(\"PGPORT\") is None:\n    os.environ[\"PGPORT\"] = \"5432\"\nif os.getenv(\"POSTGRES_DB\") is None:\n    os.environ[\"POSTGRES_DB\"] = \"database\"\n\n# PostgreSQL connection string\nDATABASE_URL = f\"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('PGHOST')}:{os.getenv('PGPORT')}/{os.getenv('POSTGRES_DB')}\"\n# JWT configuration\nJWT_SECRET = os.getenv(\"JWT_SECRET\", \"wsd-project-secret\")  # Default value for development\nALGORITHM = os.getenv(\"ALGORITHM\", \"HS256\")  # Default algorithm\nCOOKIE_KEY = os.getenv(\"COOKIE_KEY\", \"token\")  # Default cookie key\nACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7  # 7 days\n\nMAIL_USERNAME=os.getenv(\"MAIL_USERNAME\", \"eprice.varmennus@gmail.com\")  # Default sender\nMAIL_PASSWORD=os.getenv(\"MAIL_PASSWORD\")  # Default password\nMAIL_FROM=os.getenv(\"MAIL_FROM\", \"eprice.varmennus@gmail.com\")  # Default sender email\nMAIL_PORT=os.getenv(\"MAIL_PORT\", 587)  # Default port for TLS\nMAIL_SERVER=os.getenv(\"MAIL_SERVER\", \"smtp.gmail.com\")  # Default SMTP server\nMAIL_FROM_NAME=os.getenv(\"MAIL_FROM_NAME\", \"Eprice-verification\")\n\n\n# Public routes that do not require authentication\n# These routes can be accessed without a valid JWT token\npublic_routes = [\n    \"/api/public/data\",\n    \"/api/auth/login\",\n    \"/api/auth/register\",\n    \"/api/auth/verify\",\n    \"/api/auth/resend\",\n    \"/api/auth/logout\",\n    \"/api/auth/remove\",\n    #\"/docs\",\n    \"/openapi.json\",\n    \"/api/price/range\",\n    ]",
  "./App/python-server/controllers/auth_controller.py": "\"\"\" auth_controller.py defines the authentication controller for the Eprice backend API using FastAPI.\nIt provides endpoints for user registration, login, logout, email verification, and resending verification codes.\nThe controller manages authentication logic, JWT token handling, and cookie management for session persistence.\n\nKey Endpoints:\n\nPOST /api/auth/register: Registers a new user and sends a confirmation email. Handles duplicate email errors.\nPOST /api/auth/login: Authenticates a user, checks email verification status, and issues a JWT token as an HTTP-only cookie.\nGET /api/auth/logout: Logs out the user by deleting the authentication cookie.\nPOST /api/auth/verify: Verifies a user's email using a code sent to their email address.\nPOST /api/auth/resend: Resends the email verification code to the user.\nThe controller uses dependency-injected service and repository layers for business logic and database access.\nIt also provides a JWT middleware factory for protecting private routes by validating JWT tokens from cookies and attaching user info to the request state.\n\nError handling is performed by setting appropriate HTTP status codes and returning informative messages for frontend handling.\nAll endpoints expect and return JSON payloads.\n\nDependencies:\n\nFastAPI for API routing and response handling.\njose for JWT encoding/decoding.\nasyncpg for async PostgreSQL operations.\nCustom modules for user models, authentication services, and configuration.\nThis controller is intended to be used as part of the FastAPI application and imported into the main app router. \"\"\"\n\nfrom fastapi import APIRouter, Response, Request\nfrom fastapi.responses import JSONResponse\nfrom jose import jwt, JWTError\nfrom services.auth_service import AuthService\nfrom repositories.user_repository import UserRepository\nfrom models.user_model import User, UserCode, EmailRequest\nfrom config.secrets import DATABASE_URL, JWT_SECRET, ALGORITHM, COOKIE_KEY\nimport asyncpg\n\n\nrouter = APIRouter()\nuser_repository = UserRepository(DATABASE_URL)\nauth_service = AuthService(user_repository)\n\n@router.post(\"/api/auth/register\")\nasync def register(user: User, response: Response):\n    \"\"\"\n    Registers a new user and sends a confirmation email.\n\n    Attempts to create a new user account with the provided email and password.\n    If successful, sends a confirmation email with a verification code.\n    Handles duplicate email registration and unexpected errors.\n\n    Args:\n        user (User): The user registration data (email and password).\n        response (Response): FastAPI response object for setting status codes.\n\n    Returns:\n        dict: JSON message indicating success or the reason for failure.\n\n    NOTE: email can raise fastapi_mail.errors.ConnectionErrors for SMTP connection issues,\n          or some other errors related to email sending.\n    \"\"\"\n    try:\n        await auth_service.register_user(user.email.lower(), user.password)\n        return {\"message\": f\"Confirmation email sent to address {user.email.lower()}.\"}\n    except asyncpg.UniqueViolationError:\n        print(f\"Email already registered: {user.email.lower()}\")\n        response.status_code = 400\n        return {\"message\": \"Email already registered.\"}\n    except Exception as e:\n        print(f\"Error during registration: {str(e)}\")\n        response.status_code = 500\n        return {\"error\": \"An error occurred during registration.\"}\n\n@router.post(\"/api/auth/login\")\nasync def login(user: User, response: Response):\n    \"\"\"\n    Authenticates a user and issues a JWT token as an HTTP-only cookie.\n\n    Verifies the user's email and password. Checks if the user's email is verified.\n    If authentication is successful, sets a JWT token in a secure cookie.\n    Handles incorrect credentials and unverified email cases.\n\n    Args:\n        user (User): The user login data (email and password).\n        response (Response): FastAPI response object for setting cookies and status codes.\n\n    Returns:\n        dict: JSON message indicating the result of the login attempt.\n    \"\"\"\n    db_user = await auth_service.authenticate_user(user.email.lower(), user.password)\n    if not db_user:\n        # SUGGESTION TO JUHO:\n        # dont't raise exceptions, just return set status code and return a message\n        # and we can handle it in the front\n        #raise HTTPException(status_code=401, detail=\"Incorrect email or password.\")\n        response.status_code = 401\n        return {\"message\": \"Incorrect email or password.\"}\n    \n    if not db_user[\"is_verified\"]:\n        print(f\"Email not verified: {user.email.lower()}\")\n        response.status_code = 401\n        return {\"message\": \"Email not verified.\"}\n\n    payload = {\"email\": db_user[\"email\"], \"role\": db_user[\"role\"]}\n    token = auth_service.create_access_token(payload)\n    response.set_cookie(key=COOKIE_KEY,\n                        value=token,\n                        httponly=True, samesite=\"lax\",\n                        path=\"/\",\n                        secure=False,\n                        )\n\n    return {\"message\": \"Welcome!\"}\n\n\n@router.get(\"/api/auth/logout\")\nasync def logout(response: Response):\n    \"\"\"\n    Logs out the current user by deleting the authentication cookie.\n\n    Removes the JWT authentication cookie from the client to end the session.\n\n    Args:\n        response (Response): FastAPI response object for deleting cookies.\n\n    Returns:\n        dict: JSON message confirming successful logout.\n    \"\"\"\n    response.delete_cookie(\n        key=COOKIE_KEY,\n        path=\"/\",\n    )\n    return {\"message\": \"User has successfully logged out\"}\n\n@router.post(\"/api/auth/verify\")\nasync def verify(user_code: UserCode, response: Response):\n    \"\"\"\n    Verifies a user's email address using a verification code.\n\n    Checks the provided verification code against the stored code for the user.\n    If valid, marks the user's email as verified. Handles invalid or expired codes.\n\n    Args:\n        user_code (UserCode): The user's email and verification code.\n        response (Response): FastAPI response object for setting status codes.\n\n    Returns:\n        dict: JSON message indicating the result of the verification attempt.\n    \"\"\"\n    try:\n        await auth_service.verify_user(user_code.email.lower(), user_code.code)\n        return {\"message\": \"Email verified successfully.\"}\n    except Exception as e:\n        print(f\"Error during verification: {str(e)}\")\n        response.status_code = 400\n        return {\"message\": \"Verification failed.\"}\n\n\n@router.post(\"/api/auth/resend\")\nasync def resend_verification_code(request: EmailRequest, response: Response):\n    \"\"\"\n    Resends a new email verification code to the user's email address.\n\n    Used when the user did not receive or lost the original verification code.\n    Handles errors such as invalid email addresses.\n\n    Args:\n        request (EmailRequest): The user's email address.\n        response (Response): FastAPI response object for setting status codes.\n\n    Returns:\n        dict: JSON message indicating whether the code was resent successfully.\n\n    NOTE: email can raise fastapi_mail.errors.ConnectionErrors for SMTP connection issues,\n          or some other errors related to email sending.\n    \"\"\"\n    try:\n        await auth_service.update_verification_code(request.email.lower())\n        return {\"message\": \"Verification code resent successfully.\"}\n    except Exception as e:\n        print(f\"Error during resending verification code: {str(e)}\")\n        response.status_code = 400\n        return {\"message\": \"Failed to resend verification code.\"}\n\n@router.post(\"/api/auth/remove\")\nasync def remove_user(user: User, response: Response):\n    \"\"\"\n    Removes a user record from the database.\n\n    Deletes the user account associated with the provided email address.\n    Handles errors such as non-existent users. First authenticates the user\n    to ensure they have the right to delete the account.\n\n    Args:\n        user (User): The user data containing the email address.\n        response (Response): FastAPI response object for setting status codes.\n\n    Returns:\n        dict: JSON message indicating whether the user was removed successfully.\n    \"\"\"\n    db_user = await auth_service.authenticate_user(user.email.lower(), user.password)\n\n    if not db_user:\n        print(f\"Authentication failed for user: {user.email.lower()}\")\n        response.status_code = 401\n        return {\"message\": \"Authentication failed. Incorrect email or password.\"}\n\n    try:\n        await auth_service.remove_user(user.email.lower())\n        return {\"message\": \"User account removed successfully.\"}\n    except asyncpg.NoDataFoundError:\n        print(f\"Unable to remove user: {user.email.lower()}\")\n        response.status_code = 404\n        return {\"message\": \"User account was not removed -- contact site admin: eprice.varmennus@gmail.com.\"}\n\n                                   \ndef create_jwt_middleware(public_routes):\n    \"\"\"\n    Creates a FastAPI middleware for validating JWT tokens on protected routes.\n\n    Extracts the JWT token from cookies, decodes and verifies it, and attaches\n    the user payload to the request state. Skips validation for public routes and\n    handles preflight (OPTIONS) requests. Returns a 401 error if the token is\n    missing or invalid.\n\n    Args:\n        public_routes (list): List of route paths that do not require authentication.\n\n    Returns:\n        Callable: The JWT validation middleware function.\n    \"\"\"\n    async def jwt_middleware(request: Request, call_next):\n        # Accept preflight requests\n        if request.method == \"OPTIONS\":\n            return await call_next(request)\n\n        # Skip validation for public routes\n        if request.url.path in public_routes:\n            return await call_next(request)\n\n        # Extract token from cookies\n        token = request.cookies.get(COOKIE_KEY)\n        if not token:\n            return JSONResponse(status_code=401, content={\"message\": \"No token found!\"})\n\n        # Validate the token\n        try:\n            payload = jwt.decode(token, JWT_SECRET, algorithms=[ALGORITHM])\n            request.state.user = payload\n        except JWTError:\n            return JSONResponse(status_code=401, content={\"message\": \"Invalid token!\"})\n\n        # Proceed to the next middleware or route handler\n        return await call_next(request)\n\n    return jwt_middleware",
  "./App/python-server/controllers/data_controller.py": "\"\"\"\ndata_controller.py\n\nThis module defines the FastAPI routes for the Eprice backend service. It provides API endpoints for retrieving\nand querying electricity production, consumption, wind power, and price data. The endpoints fetch data from\nFingrid and Porssisähkö APIs, and return results as Pydantic models or error responses.\n\nAll datetime fields in API requests and responses use the RFC 3339 format. Unless otherwise specified:\n- Input datetimes (startTime, endTime) should be provided as UTC-aware datetimes (e.g., \"2025-06-01T20:00:00Z\").\n- Returned datetimes (such as startDate, startTime, endTime) are serialized as UTC datetime strings in RFC 3339 format (e.g., \"2025-06-01T20:00:00Z\").\n\nRoutes:\n    - /api/windpower\n    - /api/windpower/range\n    - /api/consumption\n    - /api/consumption/range\n    - /api/production\n    - /api/production/range\n    - /api/price/range\n    - /api/public/data\n    - /api/data/today\n    - /api/price/hourlyavg\n    - /api/price/weekdayavg\n\"\"\"\n\nfrom fastapi import APIRouter\nfrom fastapi.responses import JSONResponse\nfrom typing import List\nfrom services.data_service import FingridDataService, PriceDataService\nfrom models.data_model import FingridDataPoint, TimeRangeRequest, PriceDataPoint, ErrorResponse\nfrom fastapi import HTTPException\n\nrouter = APIRouter()\nfingrid_data_service = FingridDataService()\nprice_data_service = PriceDataService()\n\n\n\n@router.get(\"/api/windpower\", response_model=FingridDataPoint, responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_windpower():\n    \"\"\"\n    Get wind power production forecast.\n\n    Fetches forecast data from Fingrid dataset ID 245.\n\n    Returns:\n        FingridDataPoint | JSONResponse: A wind power data point or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data(dataset_id=245)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/windpower/range\", response_model=List[FingridDataPoint],\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def post_windpower_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get wind power production data for a given time range.\n\n    Fetches data from Fingrid dataset ID 245.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time in RFC 3339 format.\n\n    Returns:\n        List[FingridDataPoint] | JSONResponse: List of wind power data points or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data_range(dataset_id=245, time_range=time_range)\n\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.get(\"/api/consumption\",\n    response_model=FingridDataPoint,\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_consumption():\n    \"\"\"\n    Get electricity consumption forecast.\n\n    Fetches consumption data from Fingrid dataset ID 165.\n\n    Returns:\n        FingridDataPoint | JSONResponse: A consumption data point or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data(dataset_id=165)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/consumption/range\", response_model=List[FingridDataPoint],\n             responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def post_consumption_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get electricity consumption data for a given time range.\n\n    Fetches consumption data from Fingrid dataset ID 165.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time in RFC 3339 format (UTC).\n\n    Returns:\n        List[FingridDataPoint] | JSONResponse: List of consumption data points or an error message.\n            Each FingridDataPoint's startTime and endTime are returned as UTC datetimes (RFC 3339).\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data_range(\n            dataset_id=165,time_range=time_range)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.get(\"/api/production\",\n    response_model=FingridDataPoint,\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_production():\n    \"\"\"\n    Get electricity production forecast.\n\n    Fetches production data from Fingrid dataset ID 241.\n\n    Returns:\n        FingridDataPoint | JSONResponse: A production data point or an error message.\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data(dataset_id=241)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/production/range\", response_model=List[FingridDataPoint],\n            responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def post_production_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get electricity production data for a given time range.\n\n    Fetches production data from Fingrid dataset ID 241.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time in RFC 3339 format (UTC).\n\n    Returns:\n        List[FingridDataPoint] | JSONResponse: List of production data points or an error message.\n            Each FingridDataPoint's startTime and endTime are returned as UTC datetimes (RFC 3339).\n    \"\"\"\n    try:\n        return await fingrid_data_service.fingrid_data_range(\n            dataset_id=241, time_range=time_range)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.post(\"/api/price/range\",\n             response_model=List[PriceDataPoint])\nasync def post_price_range(time_range: TimeRangeRequest):\n    \"\"\"\n    Get price data for a specific time range from the Porssisahko API.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time as UTC datetime objects (RFC 3339).\n\n    Returns:\n        List[PriceDataPoint] | JSONResponse: List of price data points or an error message.\n            Each PriceDataPoint's startDate is returned as a UTC datetime string in RFC 3339 format (e.g., '2025-06-01T20:00:00Z').\n    \"\"\"\n    try:\n        return await price_data_service.price_data_range(time_range)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        print(e)\n        return JSONResponse({\"error\":\"InternalServerError\", \"message\": str(e)})\n\n\n@router.get(\n    \"/api/public/data\",\n    response_model=List[PriceDataPoint],\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_prices():\n    \"\"\"\n    Retrieve the latest 48 hours of electricity price data.\n\n    Returns:\n        List[PriceDataPoint] | JSONResponse: List of the latest price data points or an error message.\n            Each PriceDataPoint's startDate is returned as a UTC datetime string in RFC 3339 format (e.g., '2025-06-01T20:00:00Z').\n    \"\"\"\n    try:\n        return await price_data_service.price_data_latest()\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n\n@router.get(\n    \"/api/data/today\",\n    response_model=List[PriceDataPoint],\n    responses={500: {\"model\": ErrorResponse, \"description\": \"Internal server error\"}})\nasync def get_prices_today():\n    \"\"\"\n    Retrieve today's electricity price data for Finland (Europe/Helsinki).\n\n    Returns:\n        List[PriceDataPoint] | JSONResponse: List of today's price data points or an error message.\n            Each PriceDataPoint's startDate is returned as a UTC datetime string in RFC 3339 format (e.g., '2025-06-01T20:00:00Z').\n    \"\"\"\n    try:\n        return await price_data_service.price_data_today()\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n@router.post(\"/api/price/hourlyavg\")\nasync def post_price_hourly_avg(time_range: TimeRangeRequest):\n    \"\"\"\n    Get hourly average price data for a specific time range.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time as UTC datetime objects (RFC 3339).\n\n    Returns:\n        List[HourlyAvgPricePoint] | JSONResponse: List of hourly average price data points or an error message.\n            The 'hour' field is in Helsinki time (0-23).\n    \"\"\"\n    try:\n        return await price_data_service.price_data_hourly_avg(time_range)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n@router.post(\"/api/price/weekdayavg\")\nasync def post_price_weekday_avg_hki(time_range: TimeRangeRequest):\n    \"\"\"\n    Get average price data grouped by weekday for a specific time range.\n\n    Args:\n        time_range (TimeRangeRequest): Start and end time as either UTC-aware datetimes (RFC 3339) or naive datetimes in Helsinki time.\n\n    Returns:\n        List[PriceAvgByWeekdayPoint] | JSONResponse: List of average price data points by weekday or an error message.\n            The 'weekday' field is in Helsinki time (0=Monday, 6=Sunday).\n    \"\"\"\n    try:\n        return await price_data_service.price_data_avg_by_weekday(time_range)\n    except HTTPException as e:\n        return JSONResponse(status_code=e.status_code, content={\"error\": \"HTTPError\", \"message\": e.detail})\n    except Exception as e:\n        return JSONResponse({\"error\": \"InternalServerError\", \"message\": str(e)})\n\n",
  "./App/python-server/ext_apis/ext_apis.py": "\"\"\"\next_apis.py\n\nThis module provides service classes for fetching electricity production, consumption, and price data\nfrom external APIs (Fingrid and Porssisähkö). It handles API requests, rate limiting, retries, error handling,\nand conversion of API responses into application models used by the backend.\n\nDependencies:\n    - httpx: For making asynchronous HTTP requests to external APIs.\n    - python-dotenv: For loading environment variables (API keys) from .env files.\n    - fastapi: For raising HTTPException on API errors.\n    - models.data_model: For Pydantic data models used to structure API responses.\n    - zoneinfo: For timezone-aware datetime handling.\n    - datetime, typing, urllib.parse, os, asyncio: Standard library modules for time, typing, URL handling, environment, and async support.\n\nClasses:\n    - FetchFingridData: Fetches production and consumption data from the Fingrid API.\n    - FetchPriceData: Fetches electricity price data from the Porssisähkö API.\n\"\"\"\n\nfrom datetime import datetime, timezone, timedelta\nimport httpx\nfrom urllib.parse import urlencode\nfrom typing import List\nfrom dotenv import load_dotenv\nimport os\nfrom models.data_model import *\nfrom zoneinfo import ZoneInfo\nfrom models.data_model import PriceDataPoint\nfrom fastapi import HTTPException\nimport asyncio\n\n\nload_dotenv(dotenv_path=\"./.env.local\")\nFINGRID_API_KEY = os.getenv(\"FINGRID_API_KEY\")\n\nclass FetchFingridData:\n    \"\"\"\n    Service for fetching electricity production and consumption data from the Fingrid API.\n\n    Provides methods to fetch the latest data point or a range of data points for a given Fingrid dataset.\n    Handles API rate limiting, retries on failure, and parsing of the Fingrid API response into application models.\n    \"\"\"\n\n    base_url = \"https://data.fingrid.fi/api/datasets/\"\n\n    def __init__(self):\n        self._lock = asyncio.Lock()\n        self._last_call_time: datetime | None = None\n        self._sleep_time = 1.5\n\n    async def _rate_limiter(self):\n        async with self._lock:\n            if self._last_call_time:\n                elapsed = (datetime.now(timezone.utc)- self._last_call_time).total_seconds()\n                if elapsed < self._sleep_time:\n                    await asyncio.sleep(self._sleep_time - elapsed)\n            self._last_call_time = datetime.now(timezone.utc)\n\n    async def fetch_fingrid_data(self, dataset_id: int) -> FingridDataPoint:\n        \"\"\"\n        Fetch the latest data point for a given Fingrid dataset ID.\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n\n        Returns:\n            FingridDataPoint: The closest data point to the current time.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        await self._rate_limiter() \n        url = f\"{self.base_url}{dataset_id}/data\"\n        headers = {}\n        if FINGRID_API_KEY is not None:\n            headers[\"x-api-key\"] = FINGRID_API_KEY\n        max_retries = 3\n        retry_delay = 3\n\n        for attempt in range(max_retries):\n            try:\n                async with httpx.AsyncClient() as client:\n                    # Remove any None values from headers to satisfy type checker\n                    clean_headers = {k: v for k, v in headers.items() if v is not None}\n                    response = await client.get(url, headers=clean_headers)\n                    response.raise_for_status()\n                    full_data = response.json()\n                    data = full_data.get(\"data\", [])\n\n                    if not data:\n                        raise ValueError(\"No data available from Fingrid API\")\n\n                    now = datetime.now(timezone.utc)\n\n                    def time_diff(item):\n                        start = datetime.fromisoformat(item[\"startTime\"].replace(\"Z\", \"+00:00\"))\n                        end = datetime.fromisoformat(item[\"endTime\"].replace(\"Z\", \"+00:00\"))\n                        return min(abs(start - now), abs(end - now))\n\n                    closest_item = min(data, key=time_diff)\n                    closest_item.pop(\"datasetId\", None)\n                    return FingridDataPoint(**closest_item)\n\n            except httpx.HTTPStatusError as exc:\n                if attempt == max_retries - 1:\n                    raise HTTPException(\n                        status_code=exc.response.status_code,\n                        detail=f\"HTTP error while fetching data for dataset {dataset_id} from Fingrid API. Number of attempts: {attempt +1}\"\n                    ) from exc\n                await asyncio.sleep(retry_delay)\n\n            except Exception as e:\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Unexpected error fetching data for dataset {dataset_id} from Fingrid API.\"\n                ) from e\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to fetch data for dataset {dataset_id} from Fingrid API after {max_retries} attempts.\"\n        )\n        \n\n    async def fetch_fingrid_data_range(self, dataset_id: int, time_range: TimeRange) -> List[FingridDataPoint]:\n        \"\"\"\n        Fetch a list of data points for a given Fingrid dataset ID and time range,\n        but return only data points that are aligned with full hours (e.g., 13:00, 14:00).\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n            time_range (TimeRange): Time range with startTime and endTime (both in UTC).\n\n        Returns:\n            List[FingridDataPoint]: Filtered list of data points for the specified range.\n        \"\"\"\n        headers = {\"x-api-key\": FINGRID_API_KEY} if FINGRID_API_KEY is not None else {}\n        headers = {k: v for k, v in headers.items() if v is not None}\n        url = f\"{self.base_url}{dataset_id}/data\"\n\n        query_params = {\n            \"startTime\": time_range.startTime.isoformat().replace(\"+00:00\", \"Z\"),\n            \"endTime\": time_range.endTime.isoformat().replace(\"+00:00\", \"Z\"),\n            \"sortBy\": \"startTime\",\n            \"sortOrder\": \"asc\",\n            \"pageSize\": \"20000\"\n        }\n\n        url = f\"{url}?{urlencode(query_params)}\"\n        max_retries = 3\n        retry_delay = 1\n\n        for attempt in range(max_retries):\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(url, headers=headers)\n                    response.raise_for_status()\n                    full_data = response.json()\n                    data = full_data.get(\"data\", [])\n\n                    filtered_points = []\n                    for item in data:\n                        item.pop(\"datasetId\", None)\n                        start_dt = datetime.fromisoformat(item[\"startTime\"].replace(\"Z\", \"+00:00\"))\n\n                        if start_dt.minute == 0 and start_dt.second == 0:\n                            filtered_points.append(FingridDataPoint(**item))\n\n                    return filtered_points\n\n            except httpx.HTTPStatusError as exc:\n                if attempt == max_retries - 1:\n                    raise HTTPException(\n                        status_code=exc.response.status_code,\n                        detail=f\"HTTP error while fetching data for dataset {dataset_id} from Fingrid API. Number of attempts: {attempt + 1}\"\n                    ) from exc\n                await asyncio.sleep(retry_delay)\n\n            except Exception as e:\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Unexpected error fetching data for dataset {dataset_id} from Fingrid API.\"\n                ) from e\n\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to fetch data for dataset {dataset_id} from Fingrid API after {max_retries} attempts.\"\n        )\n\nclass FetchPriceData:\n    \"\"\"\n    Service for fetching electricity price data from the Porssisähkö API.\n\n    Provides methods to fetch price data for a specific time range, the latest prices, or today's prices.\n    Handles API requests, error handling, and conversion of API responses into application models.\n    \"\"\"\n\n    base_url = \"https://api.porssisahko.net/v1/price.json\"\n\n    async def fetch_price_data_range(self, time_range: TimeRange) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch hourly electricity price data for a given time range from the Porssisähkö API.\n\n        Args:\n            start_time (datetime): Start time in UTC.\n            end_time (datetime): End time in UTC.\n\n        Returns:\n            List[PriceDataPoint]: A list of PriceDataPoint objects with 'startDate' (UTC RFC3339 string) and 'price' (float) for each hour.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        result = []\n        current_time = time_range.startTime\n        end_time = time_range.endTime\n        \n        while current_time <= end_time:\n            # Queries to the Pörssisähko API are in Europe/Helsinki timezone\n            hki_time = current_time.astimezone(ZoneInfo(\"Europe/Helsinki\"))\n            date_str = hki_time.strftime(\"%Y-%m-%d\")\n            hour_str = hki_time.strftime(\"%H\")\n            url = f\"{self.base_url}?{urlencode({'date': date_str, 'hour': hour_str})}\"\n\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(url)\n                    response.raise_for_status()\n                    data = response.json()\n                    if not data:\n                        raise ValueError(f\"No price data returned for {date_str} {hour_str}\")\n                    # Convert to PriceDataPoint\n                    result.append(\n                        PriceDataPoint(\n                            startDate=datetime.strptime(current_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"), \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc),\n                            price=data[\"price\"]\n                        )\n                    )\n            except httpx.HTTPStatusError as exc:\n                raise HTTPException(\n                    status_code=exc.response.status_code,\n                    detail=f\"HTTP error while fetching price data from Porssisahko: {exc.response.status_code}: {exc.response.text}\"\n                ) from exc\n            except Exception as e:\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Unexpected error occurred while fetching data from Porssisahko: {str(e)}\"\n                ) from e\n\n            current_time += timedelta(hours=1)\n\n        return sorted(result, key=lambda x: x.startDate)\n\n\n    async def fetch_price_data_latest(self) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch the latest hourly electricity prices from the Porssisähkö API.\n\n        Returns:\n            List[PriceDataPoint]: A list of PriceDataPoint instances containing hourly electricity price data.\n                The 'endDate' key is removed from each dictionary.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        url = \"https://api.porssisahko.net/v1/latest-prices.json\"\n\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()[\"prices\"]\n\n                for item in data:\n                    item.pop(\"endDate\", None)\n\n                return [PriceDataPoint(**item) for item in sorted(data, key=lambda x: x[\"startDate\"])]\n\n        except httpx.HTTPStatusError as exc:\n            raise HTTPException(\n                status_code=exc.response.status_code,\n                detail=f\"HTTP error while fetching latest price data from Porssisahko: {exc.response.status_code} - {exc.response.text}\"\n            ) from exc\n        except Exception as e:\n            raise HTTPException(\n                status_code=500,\n                detail=f\"Unexpected error while fetching latest price data from Porssisahko: {str(e)}\"\n            ) from e\n\n\n\n    async def fetch_price_data_today(self) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch today's electricity price data and return it as a list of PriceDataPoint models.\n\n        The data is filtered so that only prices for the current day in Europe/Helsinki timezone are returned.\n\n        Returns:\n            List[PriceDataPoint]: A list of price data points for today.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        data = await self.fetch_price_data_latest()\n\n        # Get the current date in Finland's timezone\n        now_fi = datetime.now(ZoneInfo(\"Europe/Helsinki\"))\n        today_fi = now_fi.date()\n\n        # Filter data for today's date\n        \n        filtered_data = [item for item in data if item.startDate.astimezone(ZoneInfo(\"Europe/Helsinki\")).date() == today_fi]\n\n        # Convert filtered data to PriceDataPoint models\n        return [PriceDataPoint(**item.dict()) for item in sorted(filtered_data, key=lambda x: x.startDate)]\n",
  "./App/python-server/main.py": "\"\"\"\nmain.py initializes and configures the FastAPI application for the Eprice backend.\n\nFeatures:\n- Sets up application lifespan events for startup and shutdown, including checking and inserting missing price data on startup.\n- Registers custom exception handlers for request validation errors.\n- Configures CORS middleware for frontend and test environments.\n- Includes routers for authentication and external API endpoints.\n- Adds JWT authentication middleware for protected routes.\n- Integrates scheduled tasks and ensures graceful shutdown of background schedulers.\n\nDependencies:\n- fastapi for API framework and routing.\n- fastapi.middleware.cors for CORS configuration.\n- controllers for API route definitions.\n- scheduled_tasks for background data synchronization.\n- config for application and secret settings.\n- models.custom_exception for custom error handling.\n\nIntended Usage:\n- Entry point for running the Eprice backend server.\n- Should be run with a compatible ASGI server (e.g., uvicorn).\n\"\"\"\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom controllers.auth_controller import router as auth_router\nfrom controllers.auth_controller import create_jwt_middleware\nfrom controllers.data_controller import router as external_api_router\n\nfrom scheduled_tasks.porssisahko_scheduler import shutdown_scheduler, fetch_and_insert_missing_porssisahko_data\n\nimport config\nfrom config.secrets import public_routes\n\nfrom models.custom_exception import custom_validation_exception_handler\nfrom fastapi.exceptions import RequestValidationError\n\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Lifespan event handler for the FastAPI application.\n    This function is called when the application starts up and shuts down.\n    It is used to perform startup tasks, such as checking for missing data.\n    On shutdown, it ensures scheduled tasks are properly terminated.\n\n    Args:\n        app (FastAPI): The FastAPI application instance.\n    \"\"\"\n\n    # Startup code\n    print(\"Server is starting... Checking for missing data.\")\n    start_datetime = \"2025-05-12T23:00:00\"\n    await fetch_and_insert_missing_porssisahko_data(start_datetime)\n    print(\"Server started and missing data checked.\")\n    yield\n    # Shutdown code\n    shutdown_scheduler()\n\napp = FastAPI(lifespan=lifespan)\napp.add_exception_handler(RequestValidationError, custom_validation_exception_handler)\napp.include_router(external_api_router)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:5173\",\n                   \"http://80.221.169:5173\",\n                   \"https://192.168.10.45:5173\",\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.include_router(auth_router)\napp.middleware(\"http\")(create_jwt_middleware(public_routes))\n",
  "./App/python-server/models/custom_exception.py": "\"\"\"\ncustom_exception.py\n\nThis module defines a custom exception handler for FastAPI request validation errors.\nIt provides a user-friendly JSON error response when request data fails validation.\n\"\"\"\n\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\nfrom fastapi import Request\nfrom fastapi.exception_handlers import request_validation_exception_handler\n\n# Customized handler for RequestValidationError errors\nasync def custom_validation_exception_handler(request: Request, exc: RequestValidationError):\n    \"\"\"\n    Custom handler for FastAPI RequestValidationError.\n\n    Args:\n        request (Request): The incoming FastAPI request.\n        exc (RequestValidationError): The validation exception raised by FastAPI.\n\n    Returns:\n        JSONResponse: A JSON response with error details and HTTP status 422.\n    \"\"\"\n    status_code = 422\n    if exc.errors():\n        error_detail = exc.errors()[0]\n        # Extracting the first error detail\n        loc_list = error_detail.get(\"loc\", [])\n        str_parts = [str(loc) for loc in loc_list]\n        loc_str = \".\".join(str_parts)\n\n        msg = error_detail.get(\"msg\", \"Validation error\")\n        type_str = error_detail.get(\"type\", \"validation_error\")\n\n        error_message = (\n            f\"{msg}.\"\n        )\n\n    else:\n        error_message = f\"Validation failed for unknown reason.\"\n\n    return JSONResponse(\n        status_code=status_code,\n        content={\n            \"error\": \"RequestValidationError\",\n            \"message\": error_message\n        }\n    )",
  "./App/python-server/models/data_model.py": "\"\"\"\ndata_model.py\n\nThis module defines Pydantic data models used throughout the Eprice backend service.\nIt includes models for time ranges, Fingrid and price data points, error responses,\nand utility base classes for datetime validation.\n\"\"\"\n\nfrom pydantic import BaseModel, Field, field_validator, field_serializer\nfrom datetime import datetime, timezone\nfrom zoneinfo import ZoneInfo\n\n\nHELSINKI_TZ = ZoneInfo(\"Europe/Helsinki\")\n\nclass DateTimeValidatedModel(BaseModel):\n    \"\"\"\n    Base model that normalizes datetime fields after initialization.\n\n    This model ensures that all datetime fields listed in _datetime_fields are timezone-aware and converted to UTC.\n    If a datetime field is naive (lacks tzinfo), it is assumed to be in Europe/Helsinki time and converted to UTC.\n    This normalization happens automatically after model initialization, so all downstream code can safely assume\n    that these fields are always UTC-aware datetimes.\n\n    Intended for use as a base class for models that include datetime fields which may be provided in various formats.\n    \"\"\"\n\n    _datetime_fields = ('startTime', 'endTime', 'timestamp', 'startDate')\n\n    def model_post_init(self, __context):\n        for field_name in self._datetime_fields:\n            if hasattr(self, field_name):\n                value = getattr(self, field_name)\n                if isinstance(value, datetime):\n                    setattr(self, field_name, self.assume_helsinki_if_naive(value))\n\n    @staticmethod\n    def assume_helsinki_if_naive(dt: datetime) -> datetime:\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=HELSINKI_TZ)\n        return dt.astimezone(ZoneInfo(\"UTC\"))\n\nclass StartDateModel(BaseModel):\n    \"\"\"\n    Model containing a single startDate field as a datetime object.\n    \"\"\"\n    startDate: datetime\n\nclass TimeRange(DateTimeValidatedModel):\n    \"\"\"\n    Model representing a time range with start and end times in UTC.\n    \"\"\"\n    startTime: datetime = Field(\n        description=\"Start time in RFC 3339 format (e.g., 2024-05-01T00:00:00Z).\",\n        examples=[\"2024-05-01T00:00:00Z\"]\n    )\n    endTime: datetime = Field(\n        description=\"End time in RFC 3339 format (e.g., 2024-05-02T00:00:00Z).\",\n        examples=[\"2024-05-02T00:00:00Z\"]\n    )\n\nclass TimeRangeRequest(TimeRange):\n    \"\"\"\n    Request model for endpoints requiring a time range.\n\n    startTime and endTime are UTC-aware datetimes, validated by DateTimeValidatedModel.\n    \"\"\"\n\n    def start_datetime(self) -> datetime:\n        return self.startTime\n\n    def end_datetime(self) -> datetime:\n        return self.endTime\n\nclass FingridDataPoint(TimeRange):\n    \"\"\"\n    Model representing a single data point from Fingrid, including a value (in megawatts) and time range.\n\n    Attributes:\n        value (float): Value of the data point in megawatts (MW), must be non-negative.\n    \"\"\"\n    value: float = Field(\n        examples=[7883.61],\n        description=\"Value of the data point in megawatts (MW).\"\n    )\n\n    @field_validator(\"value\")\n    def validate_value_positive(cls, v):\n        \"\"\"\n        Validates that the value is non-negative.\n\n        Raises:\n            ValueError: If the value is negative.\n        \"\"\"\n        if v < 0:\n            raise ValueError(\"value must be non-negative\")\n        return v\n\nclass PriceDataPoint(BaseModel):\n    \"\"\"\n    Model representing a single electricity price data point.\n\n    Attributes:\n        startDate (datetime): Start time of the price data point. \n            When serialized (returned from the API), this is formatted as a UTC datetime string (RFC 3339, e.g., '2025-06-01T20:00:00Z').\n        price (float): Price in euro cents.\n    \"\"\"\n    startDate: datetime = Field(\n        description=\"Datetime, returned as naive datetime string in UTC datetime string in RFC 3339 format\",\n        # examples=[\"2025-06-01 23:00\"] # Uncomment this line if you want to use naive datetime in Helsinki time\n        examples=[\"2025-06-01T20:00:00Z\"]\n    )\n    price: float = Field(\n        description=\"Floating-point number representing the price in euro cents.\",\n        examples=[0.61]\n    )\n    # if startDate is wanted as naive datetime in Helsinki time, uncomment the serializer below\n    # @field_serializer('startDate')\n    # def serialize_start_date(self, dt: datetime, _info):\n    #     dt_helsinki = dt.astimezone(HELSINKI_TZ)\n    #     naive_helsinki = dt_helsinki.replace(tzinfo=None)\n    #     return naive_helsinki.strftime('%Y-%m-%d %H:%M')\n\n    @field_serializer('startDate')\n    def serialize_start_date(self, dt: datetime, _info):\n        dt_utc = dt.astimezone(timezone.utc)\n        return dt_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n\nclass HourlyAvgPricePoint(BaseModel):\n    \"\"\"\n    Model representing an hourly average price point.\n\n    Attributes:\n        hour (int): Hour of the day (0-23) in Helsinki time.\n        avgPrice (float): Average price in euro cents for that hour.\n    \"\"\"\n    hour: int = Field(\n        description=\"Hour of the day (0-23) in Helsinki time.\",\n        examples=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n    )\n    avgPrice: float = Field(\n        description=\"Average price in euro cents for that hour.\",\n        examples=[0.61]\n    )\n\nclass PriceAvgByWeekdayPoint(BaseModel):\n    \"\"\"\n    Model representing average price by weekday.\n\n    Attributes:\n        weekday (int): Day of the week (0=Monday, 6=Sunday).\n        avgPrice (float): Average price in euro cents for that weekday.\n    \"\"\"\n    weekday: int = Field(\n        description=\"Day of the week (0=Monday, 6=Sunday).\",\n        examples=[0, 1, 2, 3, 4, 5, 6]\n    )\n    avgPrice: float = Field(\n        description=\"Average price in euro cents for that weekday.\",\n        examples=[0.61]\n    )\n\nclass ErrorResponse(BaseModel):\n    \"\"\"\n    Model for error responses returned by the API.\n\n    Attributes:\n        error (str): Error message describing the issue.\n    \"\"\"\n    error: str = Field(\n        description=\"Error message describing the issue.\",\n        examples=[\"An error occurred\"]\n    )\n",
  "./App/python-server/models/user_model.py": "\"\"\"\nuser_model.py defines Pydantic models for user-related data validation in the Eprice backend.\n\nModels:\n- EmailRequest: Validates and represents an email address for requests such as verification code resending.\n- UserCode: Validates an email and a verification code, ensuring the code matches the required format (ABC-123).\n- User: Validates user registration and login data, enforcing email format and password strength.\n\nAll models use Pydantic for type validation and include custom field validators for additional constraints.\n\nIntended Usage:\n- Used in FastAPI route handlers for request body validation.\n- Ensures consistent and secure data formats for authentication and user management endpoints.\n\"\"\"\nfrom pydantic import BaseModel, EmailStr, field_validator, ValidationError\nfrom pydantic_core import PydanticCustomError\nfrom typing import Optional\nimport re\n\nclass EmailRequest(BaseModel):\n    \"\"\"\n    Pydantic model for validating an email address in request bodies.\n    This can be extended with additional validation rules as needed.\n\n    Attributes:\n        email (EmailStr): The user's email address.\n    \n    Raises:\n        ValidationError: If the email address is not valid.\n        Custom exception handler is set up in the main(entrypoint) file.\n    \"\"\"\n    email: EmailStr\n\nclass UserCode(BaseModel):\n    \"\"\"\n    Pydantic model for validating an email and verification code.\n\n    Attributes:\n        email (EmailStr): The user's email address.\n        code (str): The verification code in format ABC-123.\n    \"\"\"\n    email: EmailStr\n    code: str\n\n    @field_validator('code')\n    def validate_code(cls, code: str) -> str:\n        \"\"\"\n        Validate that the verification code matches the required format (ABC-123).\n\n        Args:\n            code (str): The verification code to validate.\n\n        Returns:\n            str: The validated code.\n\n        Raises:\n            ValidationError: If the code does not match the required format.\n            Custom exception handler is set up in the main(entrypoint) file.\n        \"\"\"\n        pattern = r'^[A-Z]{3}-\\d{3}$'\n        if not re.match(pattern, code):\n            assert 0>10, \"Invalid code format. Expected format: ABC-123\"\n        return code\n\nclass User(BaseModel):\n    \"\"\"\n    Pydantic model for validating user registration and login data.\n\n    Attributes:\n        email (EmailStr): The user's email address.\n        password (str): The user's password.\n    \"\"\"\n    email: EmailStr\n    password: str\n\n    @field_validator('password')\n    def validate_password(cls, password: str) -> str:\n        \"\"\"\n        Validate that the password meets minimum strength requirements.\n\n        Args:\n            password (str): The password to validate.\n\n        Returns:\n            str: The validated password.\n\n        Raises:\n            ValidationError: If the password does not meet the minimum length requirement.\n            Custom exception handler is set up in the main(entrypoint) file.\n        \"\"\"\n        assert len(password) >= 4, \"Password must be at least 4 characters long\"\n        return password",
  "./App/python-server/repositories/fingrid_repository.py": "import asyncpg\nfrom utils.fingrid_service_tools import convert_to_fingrid_entry\nfrom datetime import datetime\n\nclass FingridRepository:\n    \"\"\"\n    Repository class for value data operations in the Evalue backend.\n\n    Provides asynchronous methods for inserting and retrieving value entries,\n    as well as finding missing entries. Interacts directly with the PostgreSQL\n    database using asyncpg.\n\n    Args:\n        database_url (str): The database connection URL.\n    \"\"\"\n    def __init__(self, database_url: str):\n        \"\"\"\n        Initialize the PorssisahkoRepository with a database connection URL.\n\n        Args:\n            database_url (str): The database connection URL.\n        \"\"\"\n        self.database_url = database_url\n    \n    async def insert_entry(self, value: float, iso_date: str, predicted: bool = False, convert_to_helsinki_time: bool = True, dataset_id: int = 0):\n        \"\"\"\n        Insert a single entry into the fingrid table.\n\n        Args:\n            value (float): The value value.\n            iso_date (str): The date in ISO 8601 format.\n            predicted (bool): Indicates if the value is predicted. Default is False.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Convert the entry to the correct format\n            entry = convert_to_fingrid_entry(value, iso_date, predicted, convert_to_helsinki_time, dataset_id)\n\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Insert the entry into the database\n            await conn.execute(\n                \"\"\"\n                INSERT INTO fingrid (datetime_orig,datetime,date,year,month,day,hour,weekday,dataset_id,value)\n                \"\"\"\n                \"VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)\"\n                \"ON CONFLICT (datetime, dataset_id) DO NOTHING\",\n                entry[\"datetime_orig\"],  # datetime_orig in UTC time zone aware format\n                entry[\"datetime\"],  # datetime int helsinki time\n                entry[\"date\"],\n                entry[\"year\"],\n                entry[\"month\"],\n                entry[\"day\"],\n                entry[\"hour\"],\n                entry[\"weekday\"],\n                entry[\"dataset_id\"],\n                entry[\"value\"]\n            )\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n\n    async def get_entries(self, start_date: datetime, end_date: datetime, dataset_id, select_columns: str = \"*\"):\n        \"\"\"\n        Retrieve entries from the porssisahko table between two dates.\n\n        Args:\n            start_date (datetime): The start date as a datetime object.\n            end_date (datetime): The end date as a datetime object.\n            select_columns (str): The columns to select from the table. Default is \"*\".\n\n        Returns:\n            list[dict]: A list of dictionaries containing the data for each entry.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the query\n            rows = await conn.fetch(\n                f\"\"\"\n                SELECT {select_columns}\n                FROM fingrid\n                WHERE datetime BETWEEN $1 AND $2\n                AND dataset_id = $3\n                \"\"\",\n                start_date,\n                end_date,\n                dataset_id\n            )\n\n            # Convert rows to a list of dictionaries\n            return [dict(row) for row in rows]\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n    async def get_missing_entries(self, start_date: datetime, end_date: datetime):\n        \"\"\"\n        Retrieve missing hourly entries from the porssisahko table between two dates.\n\n        Args:\n            start_date (datetime): The start date as a datetime object.\n            end_date (datetime): The end date as a datetime object.\n\n        Returns:\n            list[tuple]: A list of tuples where each tuple contains the date (YYYY-MM-DD) and hour (0-23).\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the query to find missing entries\n            rows = await conn.fetch(\n                \"\"\"\n                WITH date_range AS (\n                    SELECT generate_series(\n                        $1::TIMESTAMP,\n                        $2::TIMESTAMP,\n                        '1 hour'::INTERVAL\n                    ) AS datetime\n                )\n                SELECT dr.datetime\n                FROM date_range dr\n                LEFT JOIN fingrid p ON dr.datetime = p.datetime\n                WHERE p.datetime IS NULL\n                \"\"\",\n                start_date,\n                end_date\n            )\n\n            # Return the missing entries as a list of tuples\n            return [\n                (row[\"datetime\"].strftime(\"%Y-%m-%d\"), row[\"datetime\"].hour)\n                for row in rows\n            ]\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()    ",
  "./App/python-server/repositories/porssisahko_repository.py": "\"\"\"\nporssisahko_repository.py defines the PorssisahkoRepository class for price data operations in the Eprice backend.\n\nThe repository provides asynchronous methods for:\n- Inserting single or multiple price entries into the porssisahko table.\n- Retrieving entries within a date range.\n- Finding missing hourly entries within a date range.\n\nAll operations interact directly with a PostgreSQL database using asyncpg for asynchronous access.\nThis repository is intended to be used by service and controller layers to abstract database logic\nfrom business and API logic.\n\nDependencies:\n- asyncpg for asynchronous PostgreSQL operations.\n- utils.porssisahko_tools for entry conversion utilities.\n\nIntended Usage:\n- Instantiate with a database connection URL.\n- Use in services or controllers for all price data-related database actions.\n\"\"\"\n\nimport asyncpg\nfrom utils.porssisahko_tools import convert_to_porssisahko_entry\nfrom datetime import datetime\n\nclass PorssisahkoRepository:\n    \"\"\"\n    Repository class for price data operations in the Eprice backend.\n\n    Provides asynchronous methods for inserting and retrieving price entries,\n    as well as finding missing entries. Interacts directly with the PostgreSQL\n    database using asyncpg.\n\n    Args:\n        database_url (str): The database connection URL.\n    \"\"\"\n    def __init__(self, database_url: str):\n        \"\"\"\n        Initialize the PorssisahkoRepository with a database connection URL.\n\n        Args:\n            database_url (str): The database connection URL.\n        \"\"\"\n        self.database_url = database_url\n\n    async def insert_entry(self, price: float, iso_date: str, predicted: bool = False, convert_to_helsinki_time: bool = True):\n        \"\"\"\n        Insert a single entry into the porssisahko table.\n\n        Args:\n            price (float): The price value.\n            iso_date (str): The date in ISO 8601 format.\n            predicted (bool): Indicates if the price is predicted. Default is False.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Convert the entry to the correct format\n            entry = convert_to_porssisahko_entry(price, iso_date, predicted, convert_to_helsinki_time)\n\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Insert the entry into the database\n            await conn.execute(\n                \"\"\"\n                INSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\n                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n                ON CONFLICT (Datetime) DO NOTHING\n                \"\"\",\n                entry[\"datetime\"],\n                entry[\"date\"],\n                entry[\"year\"],\n                entry[\"month\"],\n                entry[\"day\"],\n                entry[\"hour\"],\n                entry[\"weekday\"],\n                entry[\"price\"]\n            )\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n    async def insert_entries(self, entries: list, convert_to_helsinki_time: bool = True):\n        \"\"\"\n        Insert multiple entries into the porssisahko table.\n\n        Args:\n            entries (list[dict]): A list of dictionaries containing the data to insert.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Convert entries to the correct format\n            formatted_entries = [\n                convert_to_porssisahko_entry(entry[\"price\"], entry[\"startDate\"], predicted=entry.get(\"predicted\", False), convert_to_helsinki_time=convert_to_helsinki_time)\n                for entry in entries\n            ]\n\n            # Prepare the insert query\n            insert_query = \"\"\"\n                INSERT INTO porssisahko (datetime, date, year, month, day, hour, weekday, price)\n                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n                ON CONFLICT (Datetime) DO NOTHING\n            \"\"\"\n            # Create a list of tuples for the entries\n            values = [\n                (\n                    entry[\"datetime\"],\n                    entry[\"date\"],\n                    entry[\"year\"],\n                    entry[\"month\"],\n                    entry[\"day\"],\n                    entry[\"hour\"],\n                    entry[\"weekday\"],\n                    entry[\"price\"]\n                )\n                for entry in formatted_entries\n            ]\n\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the insert query with the list of values\n            await conn.executemany(insert_query, values)\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n    async def get_entries(self, start_date: datetime, end_date: datetime, select_columns: str = \"*\"):\n        \"\"\"\n        Retrieve entries from the porssisahko table between two dates.\n\n        Args:\n            start_date (datetime): The start date as a datetime object.\n            end_date (datetime): The end date as a datetime object.\n            select_columns (str): The columns to select from the table. Default is \"*\".\n\n        Returns:\n            list[dict]: A list of dictionaries containing the data for each entry.\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the query\n            rows = await conn.fetch(\n                f\"\"\"\n                SELECT {select_columns}\n                FROM porssisahko\n                WHERE datetime BETWEEN $1 AND $2\n                \"\"\",\n                start_date,\n                end_date\n            )\n\n            # Convert rows to a list of dictionaries\n            return [dict(row) for row in rows]\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()\n\n    async def get_missing_entries(self, start_date: datetime, end_date: datetime):\n        \"\"\"\n        Retrieve missing hourly entries from the porssisahko table between two dates.\n\n        Args:\n            start_date (datetime): The start date as a datetime object.\n            end_date (datetime): The end date as a datetime object.\n\n        Returns:\n            list[tuple]: A list of tuples where each tuple contains the date (YYYY-MM-DD) and hour (0-23).\n\n        Raises:\n            asyncpg.PostgresError: If a database error occurs.\n        \"\"\"\n        conn = None\n        try:\n            # Connect to the database\n            conn = await asyncpg.connect(self.database_url)\n\n            # Execute the query to find missing entries\n            rows = await conn.fetch(\n                \"\"\"\n                WITH date_range AS (\n                    SELECT generate_series(\n                        $1::TIMESTAMP,\n                        $2::TIMESTAMP,\n                        '1 hour'::INTERVAL\n                    ) AS datetime\n                )\n                SELECT dr.datetime\n                FROM date_range dr\n                LEFT JOIN porssisahko p ON dr.datetime = p.datetime\n                WHERE p.datetime IS NULL\n                \"\"\",\n                start_date,\n                end_date\n            )\n\n            # Return the missing entries as a list of tuples\n            return [\n                (row[\"datetime\"].strftime(\"%Y-%m-%d\"), row[\"datetime\"].hour)\n                for row in rows\n            ]\n        except asyncpg.PostgresError as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            if conn:\n                await conn.close()",
  "./App/python-server/repositories/user_repository.py": "\"\"\"\nuser_repository.py defines the UserRepository class for user-related database operations in the Eprice backend.\n\nThe repository provides asynchronous methods for:\n- Retrieving user records by email.\n- Creating new user accounts with hashed passwords and verification codes.\n- Verifying user email addresses using verification codes.\n- Updating verification codes for users.\n\nAll operations interact directly with a PostgreSQL database using asyncpg for asynchronous access.\nThis repository is intended to be used by service and controller layers to abstract database logic\nfrom business and API logic.\n\nDependencies:\n- asyncpg for asynchronous PostgreSQL operations.\n\nIntended Usage:\n- Instantiate with a database connection URL.\n- Use in authentication and user management services for all user-related database actions.\n\"\"\"\nimport asyncpg\n\nclass UserRepository:\n    \"\"\"\n    Repository class for user-related database operations in the Eprice backend.\n\n    Provides asynchronous methods for retrieving, creating, and updating user records,\n    as well as verifying user email addresses. Interacts directly with the PostgreSQL\n    database using asyncpg.\n\n    Args:\n        database_url (str): The database connection URL.\n    \"\"\"\n    def __init__(self, database_url: str):\n        \"\"\"\n        Initialize the UserRepository with a database connection URL.\n\n        Args:\n            database_url (str): The database connection URL.\n        \"\"\"\n        self.database_url = database_url\n\n    async def get_user_by_email(self, email: str):\n        \"\"\"\n        Retrieve a user record by email address.\n\n        Args:\n            email (str): The user's email address.\n\n        Returns:\n            Record or None: The user record if found, otherwise None.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            user = await conn.fetchrow(\"SELECT * FROM users WHERE email = $1\", email)\n            return user\n        finally:\n            await conn.close()\n    \n    async def create_user(self, email: str, password_hash: str, verification_code: str):\n        \"\"\"\n        Create a new user record in the database.\n\n        Args:\n            email (str): The user's email address.\n            password_hash (str): The hashed password.\n            verification_code (str): The email verification code.\n\n        Raises:\n            Exception: If the user could not be created.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            await conn.execute(\n                \"INSERT INTO users (email, password_hash, verification_code) VALUES ($1, $2, $3)\",\n                email, password_hash, verification_code\n            )\n        finally:\n            await conn.close()\n\n    async def verify_code(self, email: str, verification_code: str):\n        \"\"\"\n        Verify a user's email address using a verification code.\n\n        Args:\n            email (str): The user's email address.\n            verification_code (str): The verification code to check.\n\n        Returns:\n            str: The result of the update operation.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            result = await conn.execute(\n                \"UPDATE users SET is_verified = TRUE WHERE email = $1 AND verification_code = $2\",\n                email, verification_code\n            )\n            return result\n        finally:\n            await conn.close()\n\n    async def update_code(self, email: str, new_code: str):\n        \"\"\"\n        Update a user's verification code.\n\n        Args:\n            email (str): The user's email address.\n            new_code (str): The new verification code.\n\n        Raises:\n            Exception: If the update operation fails.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            await conn.execute(\n                \"UPDATE users SET verification_code = $1 WHERE email = $2\",\n                new_code, email\n            )\n        finally:\n            await conn.close()\n\n    async def delete_user(self, email: str):\n        \"\"\"\n        Remove a user record from the database.\n\n        Args:\n            email (str): The user's email address.\n\n        Raises:\n            Exception: If the user could not be removed.\n        \"\"\"\n        conn = await asyncpg.connect(self.database_url)\n        try:\n            await conn.execute(\"DELETE FROM users WHERE email = $1\", email)\n        finally:\n            await conn.close()\n",
  "./App/python-server/requirements.txt": "fastapi==0.115.12\nasyncpg==0.30.0\npasslib[bcrypt]==1.7.4\npython-jose==3.4.0\nuvicorn==0.34.2\npydantic[email]==2.11.4\ndotenv==0.9.9\nhttpx==0.28.1\nasyncio==3.4.3\napscheduler==3.11.0\nrequests==2.32.3\nfastapi-mail==1.4.2 # For sending emails\n# langchain>=0.3.24\n# langchain-community>=0.3.22\n# langchain-experimental>=0.3.4\n# langchain-huggingface>=0.1.2\n# langchain-text-splitters>=0.3.8\n# numpy>=2.2.5\n# openai>=1.76.0\n# pillow>=11.2.1\n# psycopg[binary]>=3.2.7\n# transformers>=4.51.3\n# langchain-postgres>=0.0.13\n# langchain-openai>=0.3.18",
  "./App/python-server/scheduled_tasks/porssisahko_scheduler.py": "\"\"\"\nporssisahko_scheduler.py defines scheduled tasks for fetching and inserting electricity price data into the Eprice backend database.\n\nFeatures:\n- Periodically fetches the latest price data from the Pörssisähkö API and inserts it into the database.\n- Detects and fills missing hourly price entries by querying the API for specific dates and hours.\n- Uses APScheduler to schedule tasks at specified intervals or times.\n- Provides synchronous wrappers for running asynchronous tasks in a scheduler context.\n- Handles API and database errors with logging for monitoring and debugging.\n\nDependencies:\n- requests for HTTP requests to the external API.\n- apscheduler for scheduling background tasks.\n- repositories.porssisahko_repository for database operations.\n- config.secrets for database configuration.\n- asyncio for running async functions in a synchronous context.\n\nIntended Usage:\n- Used as part of the backend service to ensure the database is kept up-to-date with the latest and complete price data.\n- Can be extended with additional scheduled tasks or triggers as needed.\n\"\"\"\n\nimport asyncio\nimport requests\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.triggers.cron import CronTrigger\n#from apscheduler.triggers.interval import IntervalTrigger\nfrom datetime import datetime, timedelta\nfrom repositories.porssisahko_repository import PorssisahkoRepository\nfrom config.secrets import DATABASE_URL\n\n# Initialize the repository with the database URL\nporssisahko_repository = PorssisahkoRepository(DATABASE_URL)\n\n# The task to fetch data and insert it into the database\nasync def fetch_and_insert_porssisahko_data():\n    \"\"\"\n    Fetch the latest price data from the Pörssisähkö API and insert it into the database.\n\n    Raises:\n        requests.RequestException: If there is an error fetching data from the API.\n        Exception: For any unexpected errors during data insertion.\n    \"\"\"\n    try:\n        # Fetch data from the API\n        response = requests.get(\"https://api.porssisahko.net/v1/latest-prices.json\")\n        response.raise_for_status() # Raise an exception for HTTP errors\n        data = response.json()\n\n        # Insert the data into the database using the repository\n        await porssisahko_repository.insert_entries(data[\"prices\"])\n\n        print(f\"Database successfully updated at {datetime.now()}\")\n    except requests.RequestException as e:\n        print(f\"Error fetching data from the API: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n\nasync def fetch_and_insert_missing_porssisahko_data(start_datetime_str: str):\n    \"\"\"\n    Detect and insert missing hourly price entries into the database.\n\n    Args:\n        start_datetime_str (str): The ISO format string representing the start datetime.\n\n    Raises:\n        requests.RequestException: If there is an error fetching data from the API.\n        Exception: For any unexpected errors during data insertion.\n    \"\"\"\n    try:\n        # Convert the start_datetime string to a datetime object (db likes ISO format)\n        start_datetime = datetime.fromisoformat(start_datetime_str)\n        \n        # Calculate the end datetime (24 hours later)\n        end_datetime = datetime.now() + timedelta(days=1)\n        end_datetime = end_datetime.replace(minute=0, second=0, microsecond=0)\n        \n        # Retrieve missing entries from the repository\n        missing_entries = await porssisahko_repository.get_missing_entries(\n            start_datetime, end_datetime\n        )\n\n        if not missing_entries:\n            print(f\"No missing entries found between {start_datetime} and {end_datetime}.\")\n            return\n\n        print(f\"Found {len(missing_entries)} missing entries. Fetching data...\")\n\n        # Fetch data for the missing entries\n        for date, hour in missing_entries:\n            # Construct the API URL for the specific date and hour\n            api_url = f\"https://api.porssisahko.net/v1/price.json?date={date}&hour={hour}\"\n            response = requests.get(api_url)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            data = response.json()  # Parse the JSON response\n\n            # NOTE: Since there is inconsistency in the API response format, we need to handle both cases:\n            # latest prices (utc) and hourly prices (helsinki time)\n            # Insert the data into the database -- datetime format:  \"2022-11-14THH:00:00.000Z\"\n            await porssisahko_repository.insert_entry(data[\"price\"], f\"{date}T{(hour):02d}:00.000Z\", convert_to_helsinki_time=False)\n\n        print(f\"Missing data successfully inserted into the database.\")\n    except requests.RequestException as e:\n        print(f\"Error fetching data from the API: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n\n# we need a wrapper to run the async task in a synchronous context\ndef fetch_and_insert_porssisahko_data_sync():\n    \"\"\"\n    Synchronous wrapper to run fetch_and_insert_porssisahko_data in an event loop.\n    \"\"\"\n    asyncio.run(fetch_and_insert_porssisahko_data())\n\ndef fetch_and_insert_missing_porssisahko_data_sync(start_datetime_str: str):\n    \"\"\"\n    Synchronous wrapper to run fetch_and_insert_missing_porssisahko_data in an event loop.\n\n    Args:\n        start_datetime_str (str): The ISO format string representing the start datetime.\n    \"\"\"\n    asyncio.run(fetch_and_insert_missing_porssisahko_data(start_datetime_str))\n\n\n# Set up the scheduler\nps_scheduler = BackgroundScheduler()\n# Trigger to run the task every day at 14:15\nps_trigger = CronTrigger(hour=14, minute=15)\n\n# NOTE DEBUG: For debugging/testing purposes, you can use an interval trigger to run every 15 seconds or so\n#ps_trigger = IntervalTrigger(seconds=10)\n\nps_scheduler.add_job(fetch_and_insert_porssisahko_data_sync, ps_trigger)\nps_scheduler.start()\n\n# Ensure the scheduler shuts down properly on application exit\ndef shutdown_scheduler():\n    \"\"\"\n    Shut down the APScheduler instance gracefully on application exit.\n    \"\"\"\n    print(\"Shutting down scheduler...\")\n    ps_scheduler.shutdown()\n\n",
  "./App/python-server/services/auth_service.py": "\"\"\"\nauth_service.py defines AuthService, which provides authentication and user management logic for the Eprice backend.\n\nThis service handles password hashing and verification, JWT token creation, user authentication,\nregistration, email verification, and verification code management. It interacts with the user\nrepository for database operations and with email utilities for sending verification codes.\n\nKey Responsibilities:\n- Securely hash and verify user passwords using bcrypt.\n- Generate and validate JWT access tokens for authenticated sessions.\n- Register new users, including generating and emailing verification codes.\n- Authenticate users by verifying credentials.\n- Verify user email addresses using codes sent via email.\n- Regenerate and resend verification codes as needed.\n\nDependencies:\n- passlib for password hashing.\n- jose for JWT encoding.\n- async database repository for user data.\n- email utilities for sending verification codes.\n\nIntended Usage:\n- Instantiated with a UserRepository instance.\n- Used by FastAPI controllers to perform authentication-related operations.\n\"\"\"\n\nfrom passlib.context import CryptContext\nfrom jose import jwt\nfrom datetime import datetime, timedelta\nimport random\nimport string\nfrom repositories.user_repository import UserRepository\nfrom config.secrets import JWT_SECRET, ALGORITHM, ACCESS_TOKEN_EXPIRE_MINUTES\nfrom utils.email_tools import send_email_async\n\nclass AuthService:\n    \"\"\"\n    Service class for authentication and user management logic in the Eprice backend.\n\n    Handles password hashing and verification, JWT token creation, user authentication,\n    registration, email verification, and verification code management. Interacts with the\n    user repository for database operations and with email utilities for sending verification codes.\n\n    Args:\n        user_repository (UserRepository): The repository instance for user database operations.\n    \"\"\"\n    def __init__(self, user_repository: UserRepository):\n        \"\"\"\n        Initialize the AuthService with a user repository.\n\n        Args:\n            user_repository (UserRepository): The repository instance for user database operations.\n        \"\"\"\n        self.user_repository = user_repository\n        self.pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\n    def get_password_hash(self, password: str) -> str:\n        \"\"\"\n        Hash a plaintext password using bcrypt.\n\n        Args:\n            password (str): The plaintext password.\n\n        Returns:\n            str: The hashed password.\n        \"\"\"\n        return self.pwd_context.hash(password)\n    \n    def generate_verification_code(self):\n        \"\"\"\n        Generate a random verification code in the format ABC-123.\n\n        Returns:\n            str: The generated verification code.\n        \"\"\"\n        letters = ''.join(random.choices(string.ascii_uppercase, k=3))\n        digits = ''.join(random.choices(string.digits, k=3))\n        return f\"{letters}-{digits}\"\n\n    def verify_password(self, plain_password: str, hashed_password: str) -> bool:\n        \"\"\"\n        Verify a plaintext password against a hashed password.\n\n        Args:\n            plain_password (str): The plaintext password.\n            hashed_password (str): The hashed password.\n\n        Returns:\n            bool: True if the password matches, False otherwise.\n        \"\"\"\n        return self.pwd_context.verify(plain_password, hashed_password)\n\n    def create_access_token(self, data: dict, expires_delta: timedelta = None):\n        \"\"\"\n        Create a JWT access token for the given data.\n\n        Args:\n            data (dict): The payload data to encode in the token.\n            expires_delta (timedelta, optional): Token expiration time. Defaults to configured value.\n\n        Returns:\n            str: The encoded JWT token.\n        \"\"\"\n        to_encode = data.copy()\n        expire = datetime.utcnow() + (expires_delta or timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))\n        to_encode.update({\"exp\": expire})\n        return jwt.encode(to_encode, JWT_SECRET, algorithm=ALGORITHM)\n\n    async def authenticate_user(self, email: str, password: str):\n        \"\"\"\n        Authenticate a user by email and password.\n\n        Args:\n            email (str): The user's email address.\n            password (str): The user's plaintext password.\n\n        Returns:\n            dict or None: The user record if authentication succeeds, None otherwise.\n        \"\"\"\n        user = await self.user_repository.get_user_by_email(email)\n        if not user or not self.verify_password(password, user[\"password_hash\"]):\n            return None\n        return user\n\n    async def register_user(self, email: str, password: str):\n        \"\"\"\n        Register a new user and send a verification code via email.\n\n        Args:\n            email (str): The user's email address.\n            password (str): The user's plaintext password.\n\n        Raises:\n            Exception: If user creation fails.\n        \"\"\"\n        hashed_password = self.get_password_hash(password)\n        code = self.generate_verification_code()\n        \n        await self.user_repository.create_user(email, hashed_password, code)\n        # Only send email if user creation succeeded\n        await send_email_async(email, code)\n\n    async def verify_user(self, email: str, code: str):\n        \"\"\"\n        Verify a user's email address using a verification code.\n\n        Args:\n            email (str): The user's email address.\n            code (str): The verification code.\n\n        Raises:\n            Exception: If verification fails.\n        \"\"\"\n        result = await self.user_repository.verify_code(email, code)\n        if not result:\n            raise Exception(\"Verification failed\")\n        \n    async def update_verification_code(self, email: str):\n        \"\"\"\n        Generate and update a new verification code for the user, and send it via email.\n\n        Args:\n            email (str): The user's email address.\n\n        Raises:\n            Exception: If updating the code fails.\n        \"\"\"\n        new_code = self.generate_verification_code()\n        await self.user_repository.update_code(email, new_code)\n        # Only send email if update succeeded\n        await send_email_async(email, new_code)\n        \n    async def remove_user(self, email: str):\n        \"\"\"\n        Remove a user from the database.\n\n        Args:\n            email (str): The user's email address.\n\n        Raises:\n            Exception: If user removal fails.\n        \"\"\"\n        await self.user_repository.delete_user(email)\n        ",
  "./App/python-server/services/data_service.py": "\"\"\"\ndata_service.py\n\nThis module provides service classes for handling data operations in the Eprice backend.\nIt includes services for fetching and processing Fingrid electricity data and price data,\ncombining information from external APIs and the database, and providing unified access\nto the application's core data models.\n\"\"\"\n\nfrom models.data_model import *\nfrom ext_apis.ext_apis import *\nfrom repositories.porssisahko_repository import *\nfrom repositories.fingrid_repository import FingridRepository\nfrom utils.porssisahko_service_tools import *\nfrom utils.fingrid_service_tools import *\nfrom config.secrets import DATABASE_URL\nfrom datetime import datetime\nfrom zoneinfo import ZoneInfo\n\nclass FingridDataService:\n    \"\"\"\n    Service class for fetching Fingrid data from the external API.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the FingridDataService with the external API fetcher.\n        \"\"\"\n        self.ext_api_fetcher = FetchFingridData()\n        self.fingrid_repository = FingridRepository(DATABASE_URL)\n        self.fingrid_service_tools = FingridServiceTools(self.ext_api_fetcher, self.fingrid_repository)\n\n    async def fingrid_data(self, dataset_id: int) -> FingridDataPoint:\n        \"\"\"\n        Fetch the latest Fingrid data for a given dataset ID.\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n\n        Returns:\n            FingridDataPoint: The latest data point.\n        \n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        return await self.ext_api_fetcher.fetch_fingrid_data(dataset_id)\n    \n    async def fingrid_data_range(self, dataset_id: int, time_range: TimeRange) -> List[FingridDataPoint]:\n        \"\"\"\n        Fetch Fingrid data for a given dataset ID and time range.\n\n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n            start_time (datetime): Start time in UTC.\n            end_time (datetime): End time in UTC.\n\n        Returns:\n            List[FingridDataPoint]: List of data points for the given range.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n            \n        try:\n            result = await self.fingrid_service_tools.fetch_and_process_data(time_range, dataset_id)\n            return result if result else await self.ext_api_fetcher.fetch_fingrid_data_range(dataset_id, time_range)\n        except Exception:\n            print(f\"Failed to fetch Fingrid data for dataset_id {dataset_id} in range {time_range.startTime} to {time_range.endTime}\")\n            return await self.ext_api_fetcher.fetch_fingrid_data_range(dataset_id, time_range)\n\n\nclass PriceDataService:\n    \"\"\"\n    Service class for handling price data operations, including fetching from the database and external API.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the PriceDataService with required repositories and helper services.\n        \"\"\"\n        self.ext_api_fetcher = FetchPriceData()\n        self.porssisahko_repository = PorssisahkoRepository(DATABASE_URL)\n        self.porssisahko_service_tools = PorssisahkoServiceTools(self.ext_api_fetcher, self.porssisahko_repository)\n\n    async def price_data_latest(self) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch the latest 48 hours of price data, preferring the database but falling back to the external API if needed.\n\n        Returns:\n            List[PriceDataPoint]: List of price data points for the latest 48 hours.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        start_time, end_time = self.porssisahko_service_tools.expected_time_range()\n        time_range = TimeRange(startTime=start_time, endTime=end_time)\n        try:\n            result = await self.porssisahko_service_tools.fetch_and_process_data(time_range)\n            return result if result else await self.ext_api_fetcher.fetch_price_data_latest()\n        except Exception:\n            return await self.ext_api_fetcher.fetch_price_data_latest()\n\n    async def price_data_range(self, time_range : TimeRangeRequest) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch price data for a given time range, preferring the database but falling back to the external API if needed.\n\n        Args:\n            start_date (datetime): Start of the time range.\n            end_date (datetime): End of the time range.\n\n        Returns:\n            List[PriceDataPoint]: List of price data points for the given range.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n\n        try:\n            result = await self.porssisahko_service_tools.fetch_and_process_data(time_range)\n            return result if result else await self.ext_api_fetcher.fetch_price_data_range(time_range)\n        except Exception:\n            return await self.ext_api_fetcher.fetch_price_data_range(time_range)\n\n    async def price_data_today(self) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch price data for the current day in Helsinki time.\n\n        Returns:\n            List[PriceDataPoint]: List of today's price data points.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        data = await self.price_data_latest()\n        if data:\n            now_fi = datetime.now(ZoneInfo(\"Europe/Helsinki\"))\n            today_fi = now_fi.date()\n            filtered_data = [item for item in data if item.startDate.astimezone(ZoneInfo(\"Europe/Helsinki\")).date() == today_fi]\n            return sorted(filtered_data, key=lambda x: x.startDate, reverse=False)\n        else:\n            return await self.ext_api_fetcher.fetch_price_data_today()\n\n    async def price_data_hourly_avg(self, time_range):\n        \"\"\"\n        Fetch hourly average price data for a given time range.\n\n        Args:\n            time_range (TimeRangeRequest): Start and end time as datetime objects.\n\n        Returns:\n            List[PriceHourlyAvgPricePoint]: List of hourly average price points.\n\n        Raises:\n            HTTPException: If the API call fails or no data is available.\n        \"\"\"\n        data = await self.price_data_range(time_range)\n        return self.porssisahko_service_tools.calculate_hourly_avg_price(data)\n    \n\n    \n    async def price_data_avg_by_weekday(self, time_range: TimeRangeRequest) -> List[PriceAvgByWeekdayPoint]:\n        \"\"\"\n        Calculate average price by weekday for a given time range.\n\n        Args:\n            time_range (TimeRangeRequest): Start and end time as datetime objects.\n\n        Returns:\n            List[PriceAvgByWeekdayPoint]: List of average prices by weekday.\n        \"\"\"\n        data = await self.price_data_range(time_range)\n        \n        return self.porssisahko_service_tools.calculate_avg_by_weekday(data)\n    ",
  "./App/python-server/utils/email_tools.py": "\"\"\"\nemail_tools.py provides utility functions for sending emails in the Eprice backend.\n\nFeatures:\n- Asynchronous email sending using FastAPI-Mail.\n- Configures SMTP connection using environment variables from the secrets configuration.\n- Sends verification emails with a code and a direct verification link for user registration and authentication flows.\n\nDependencies:\n- fastapi_mail for asynchronous email delivery.\n- config.secrets for SMTP credentials and configuration.\n\nIntended Usage:\n- Used by authentication and user management services to send verification codes to users.\n- Can be extended for other email-related utilities as needed.\n\"\"\"\n\nfrom fastapi_mail import FastMail, MessageSchema, ConnectionConfig\nfrom config.secrets import (\n    MAIL_USERNAME,\n    MAIL_FROM,\n    MAIL_PORT,\n    MAIL_SERVER,\n    MAIL_FROM_NAME,\n    MAIL_PASSWORD\n)\n\n\nconf = ConnectionConfig(\n    MAIL_USERNAME=MAIL_USERNAME,\n    MAIL_PASSWORD=MAIL_PASSWORD,\n    MAIL_FROM=MAIL_FROM,\n    MAIL_PORT=MAIL_PORT,\n    MAIL_SERVER=MAIL_SERVER,\n    MAIL_FROM_NAME=MAIL_FROM_NAME,\n    MAIL_STARTTLS=True,      # Add this line\n    MAIL_SSL_TLS=False       # And this line (set to True if your SMTP requires SSL/TLS)\n)\n\nasync def send_email_async(email_to: str, verification_code: str):\n    '''\n    Send an email asynchronously with a verification code and a link to verify the email address.\n\n    Args:\n        email_to (str): The recipient's email address.\n        verification_code (str): The verification code to be sent in the email.\n    '''\n\n    subject = 'Verify your email address'\n    body = f'''\n    <html>\n        <body style=\"font-family: Arial, sans-serif; text-align: center;\">\n            <h1>Verify your email address</h1>\n            <p>Use the code below to verify your email address:</p>\n            <div style=\"display: inline-block; margin: 20px auto; padding: 16px 32px; background: #f3f3f3; border-radius: 8px; font-size: 2em; font-weight: bold; color: #333;\">\n                {verification_code}\n            </div>\n            <p>Or click the link below to verify directly:</p>\n            <a href=\"http://localhost:5173/auth/verify?email={email_to}&code={verification_code}\"\n               style=\"display: inline-block; margin-top: 16px; padding: 12px 24px; background: #2563eb; color: #fff; text-decoration: none; border-radius: 6px; font-size: 1.1em;\">\n                Verify Email\n            </a>\n        </body>\n    </html>\n    '''\n    message = MessageSchema(\n        subject=subject,\n        recipients=[email_to],\n        body=body,\n        subtype='html',\n    )\n    \n    fm = FastMail(conf)\n    await fm.send_message(message, template_name='email.html')\n",
  "./App/python-server/utils/fingrid_service_tools.py": "from models.data_model import *\nfrom ext_apis.ext_apis import *\nfrom repositories.fingrid_repository import *\nfrom utils.porssisahko_service_tools import *\nfrom datetime import datetime, timedelta\nfrom zoneinfo import ZoneInfo\n\n\nclass FingridServiceTools:\n    \"\"\"\n    A class containing utility methods for Fingrid-related operations.\n    \"\"\"\n    def __init__(self, ext_api_fetcher, fingrid_repository):\n        \"\"\"\n        Initialize the FingridTools with the FingridRepository.\n        This repository is used to interact with the Fingrid database.\n        \"\"\"\n        self.fingrid_repository = fingrid_repository\n        self.ext_api_fetcher = ext_api_fetcher\n\n    async def fetch_and_process_data(self, time_range:TimeRange, dataset_id: int):\n        \"\"\"\n        Fetch and process Fingrid data for a given dataset ID.\n        \n        Args:\n            dataset_id (int): The Fingrid dataset ID.\n        \n        Returns:\n            List[FingridDataPoint]: Processed list of Fingrid data points.\n        \"\"\"\n        try:\n            start_naive_hki = time_range.startTime.astimezone(ZoneInfo(\"Europe/Helsinki\")).replace(tzinfo=None)\n            end_naive_hki = time_range.endTime.astimezone(ZoneInfo(\"Europe/Helsinki\")).replace(tzinfo=None)\n\n\n            raw_data = await self.fingrid_repository.get_entries(\n                start_date=start_naive_hki,\n                end_date=end_naive_hki,\n                dataset_id=dataset_id,\n                select_columns=\"datetime, value, dataset_id\"\n            )\n\n            result = self.convert_to_fingrid_data(raw_data)\n\n            missing_entries = self.find_missing_entries_utc(time_range, result, dataset_id)\n            if missing_entries:\n                print(f\"Database has {len(missing_entries)} entries missing in the range {time_range.startTime} to {time_range.endTime} with dataset ID {dataset_id}.\")\n                await self.fill_missing_entries(result, missing_entries, dataset_id)\n            return sorted(result, key=lambda x: x.startTime, reverse=False)\n        except Exception as e:\n            print(f\"Error while fetching and processing data: {e}\")\n            raise\n\n        \n\n    def convert_to_fingrid_data(self, data: List[dict]) -> List[FingridDataPoint]:\n        \"\"\"\n        Convert a list of database dicts to a sorted list of PriceDataPoint objects in UTC.\n\n        Args:\n            data (List[dict]): List of dicts with 'datetime' and 'price'.\n\n        Returns:\n            List[PriceDataPoint]: Sorted list of PriceDataPoint objects (startDate in UTC).\n        \"\"\"\n        if not data:\n            return []\n        return sorted([\n            FingridDataPoint(\n                startTime=item[\"datetime\"].astimezone(ZoneInfo(\"UTC\")),\n                endTime=(item[\"datetime\"] + timedelta(hours=1)).astimezone(ZoneInfo(\"UTC\")),\n                value=float(item[\"value\"])\n            ) for item in data\n        ], key=lambda x: x.startTime, reverse=False)\n\n\n    def find_missing_entries_utc(self, time_range:TimeRange, data_utc: List[FingridDataPoint], dataset_id: int) -> List[StartDateModel]:\n        \"\"\"\n        Find missing hourly entries in the given UTC time range.\n\n        Args:\n            start_date_utc (datetime): Start of the UTC time range.\n            end_date_utc (datetime): End of the UTC time range.\n            data_utc (List[PriceDataPoint]): List of available data points.\n\n        Returns:\n            List[StartDateModel]: List of StartDateModel objects for missing hours (all in UTC).\n        \"\"\"\n        result = []\n        current_date_utc = time_range.startTime\n        try:\n            while current_date_utc <= time_range.endTime:\n                if not any(item.startTime == current_date_utc for item in data_utc):\n                    result.append(StartDateModel(startDate=current_date_utc))\n                current_date_utc += timedelta(hours=1)\n            result.sort(key=lambda x: x.startDate, reverse=False)\n        except Exception as e:\n            print(f\"Error while finding missing entries: {e}\")\n            raise\n        return result\n    \n    async def fill_missing_entries(self, result: List[FingridDataPoint], missing_entries: List[StartDateModel], dataset_id: int):\n        \"\"\"\n        Fetch and insert missing price data entries from the external API.\n\n        Args:\n            result (List[PriceDataPoint]): List to append new data points to (modified in place).\n            missing_entries (List[PriceDataPoint]): List of missing data points to fetch.\n\n        Side effects:\n            Updates the result list and inserts new entries into the database.\n        \"\"\"\n\n        self.result = result\n        if not missing_entries:\n            return\n        start_time_utc_aware = min(missing_entries, key=lambda x: x.startDate).startDate\n        end_time_utc_aware = max(missing_entries, key=lambda x: x.startDate).startDate\n        time_range = TimeRange(startTime=start_time_utc_aware, endTime=end_time_utc_aware + timedelta(hours=1))\n        \n        try:\n            # Fetch data for entire range of missing entries\n            fetched_range = await self.ext_api_fetcher.fetch_fingrid_data_range(dataset_id, time_range)\n            if not fetched_range:\n                print(\"No data fetched for the missing entries range.\")\n                return\n            print(f\"Fetched {len(fetched_range)} entries for the range {time_range.startTime} to {time_range.endTime} from external API for dataset ID {dataset_id}.\")\n            counter = 0\n            for entry in fetched_range:\n                #if entry not in result:\n                if any(item.startTime == entry.startTime for item in result):\n                    continue\n                utc_dt = entry.startTime\n                iso_str = (utc_dt + timedelta(hours=0)).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n\n                result.append(FingridDataPoint(\n                    startTime=utc_dt,\n                    endTime=utc_dt + timedelta(hours=1),\n                    value=entry.value\n                ))\n                await self.fingrid_repository.insert_entry(\n                    value=entry.value,\n                    iso_date=iso_str,\n                    dataset_id=dataset_id\n                )\n                counter += 1\n            print(f\"Inserted {counter} missing entries into the database for dataset ID {dataset_id}.\")\n            # for missing in missing_entries:\n            #     time_range = TimeRange(startTime=missing.startDate, endTime=missing.startDate + timedelta(hours=1))\n            #     fetched = await self.ext_api_fetcher.fetch_fingrid_data_range(dataset_id, time_range)\n            #     if not fetched:\n            #         continue\n\n            #     datapoint = fetched[0]\n\n            #     utc_dt = datapoint.startTime\n            #     iso_str = (utc_dt + timedelta(hours=0)).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n\n            #     result.append(FingridDataPoint(\n            #         startTime=utc_dt,\n            #         endTime=utc_dt + timedelta(hours=1),\n            #         value=datapoint.value\n            #     ))\n            #     await self.fingrid_repository.insert_entry(\n            #         value=datapoint.value,\n            #         iso_date=iso_str,\n            #         dataset_id=dataset_id\n            #     )\n        except Exception as e:\n            print(f\"Error while filling missing entries: {e}\")\n\ndef convert_to_fingrid_entry(value, iso_date, predicted=False, convert_to_helsinki_time=True, dataset_id=-1):\n    \"\"\"\n    Converts a price and ISO 8601 date into a dictionary for the porssisahko table.\n\n    Args:\n        price (float): The price value.\n        iso_date (str): The date in ISO 8601 format (e.g., \"2022-11-14T22:00:00.000Z\").\n        predicted (bool): Indicates if the price is predicted. Default is False.\n    Returns:\n        dict: A dictionary with keys: Datetime, Date, Year, Month, Day, Hour, Weekday, Price, Predicted.\n    Raises:\n        ValueError: If the ISO date is not in the correct format.\n    \"\"\"\n  \n    try:\n        if convert_to_helsinki_time:\n            # Parse the ISO date string to a UTC datetime\n            dt_utc = datetime.fromisoformat(iso_date.replace(\"Z\", \"+00:00\"))\n            # Convert to Helsinki time\n            dt_helsinki = dt_utc.astimezone(ZoneInfo(\"Europe/Helsinki\"))\n            dt_naive = dt_helsinki.replace(tzinfo=None)\n        else:\n            # Parse the ISO date string to a naive datetime (UTC)\n            dt_naive = datetime.fromisoformat(iso_date.replace(\"Z\", \"+00:00\")).replace(tzinfo=None)\n        # Extract the weekday\n        weekday = dt_naive.weekday()\n\n        # Return the dictionary\n        return {\n            \"datetime_orig\": iso_date,  # Original datetime in UTC\n            \"datetime\": dt_naive,  # Use offset-naive datetime\n            \"date\": dt_naive.date(),  # Extract the date part\n            \"year\": dt_naive.year,\n            \"month\": dt_naive.month,\n            \"day\": dt_naive.day,\n            \"hour\": dt_naive.hour,\n            \"weekday\": weekday,\n            \"dataset_id\": dataset_id,\n            \"value\": value,\n            \"predicted\": predicted\n        }\n    except ValueError as e:\n        # Handle invalid date format or parsing errors\n        raise ValueError(f\"Invalid ISO date format: {iso_date}. Error: {e}\")\n\n\n",
  "./App/python-server/utils/porssisahko_service_tools.py": "\"\"\"\nporssisahko_service_tools.py\n\nThis module provides helper services for handling electricity price data time ranges, \nconverting database results to application models, and filling missing data entries \nfrom external APIs. It is used to unify and process price data from both the database \nand external sources for the Eprice backend.\n\"\"\"\n\nfrom models.data_model import *\nfrom ext_apis.ext_apis import *\nfrom repositories.porssisahko_repository import *\nfrom utils.porssisahko_service_tools import *\nfrom datetime import datetime, timedelta\nfrom zoneinfo import ZoneInfo\n\n\nclass PorssisahkoServiceTools:\n    \"\"\"\n    PorssisahkoServiceTools has functions for time range calculations, data conversion, and filling missing entries.\n    \"\"\"\n\n    def __init__(self, ext_api_fetcher: FetchPriceData, porssisahko_repository: PorssisahkoRepository):\n        \"\"\"\n        Initialize the PorssisahkoServiceTools with external API and database fetchers.\n        \"\"\"\n        self.ext_api_fetcher = ext_api_fetcher\n        self.database_fetcher = porssisahko_repository\n\n    def expected_time_range(self) -> tuple[datetime, datetime]:\n        \"\"\"\n        Calculate the expected time range for the latest 48 hours based on Helsinki time.\n\n        Returns:\n            tuple[datetime, datetime]: Start and end datetimes (naive, Europe/Helsinki time).\n        \"\"\"\n        now_naive = datetime.now(ZoneInfo(\"Europe/Helsinki\")).replace(tzinfo=None)\n        if now_naive.hour >= 14:\n            end_time = (now_naive + timedelta(days=2)).replace(hour=0, minute=0, second=0, microsecond=0)\n        else:\n            end_time = (now_naive + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n        start_time = (end_time - timedelta(hours=48)).replace(tzinfo=None)\n        return start_time, end_time\n\n    def convert_to_price_data(self, data: List[dict]) -> List[PriceDataPoint]:\n        \"\"\"\n        Convert a list of database dicts to a sorted list of PriceDataPoint objects in UTC.\n\n        Args:\n            data (List[dict]): List of dicts with 'datetime' and 'price'.\n\n        Returns:\n            List[PriceDataPoint]: Sorted list of PriceDataPoint objects (startDate in UTC).\n        \"\"\"\n        if not data:\n            return []\n        return sorted([\n            PriceDataPoint(\n                startDate=item[\"datetime\"].astimezone(ZoneInfo(\"UTC\")),\n                price=item[\"price\"]\n            ) for item in data\n        ], key=lambda x: x.startDate, reverse=False)\n\n    async def fill_missing_entries(self, result: List[PriceDataPoint], missing_entries: List[PriceDataPoint]):\n        \"\"\"\n        Fetch and insert missing price data entries from the external API.\n\n        Args:\n            result (List[PriceDataPoint]): List to append new data points to (modified in place).\n            missing_entries (List[PriceDataPoint]): List of missing data points to fetch.\n\n        Side effects:\n            Updates the result list and inserts new entries into the database.\n        \"\"\"\n        try:\n            for missing in missing_entries:\n                time_range = TimeRange(startTime=missing.startDate, endTime=missing.startDate)\n                fetched = await self.ext_api_fetcher.fetch_price_data_range(time_range)\n                if not fetched:\n                    continue\n\n                datapoint = fetched[0]\n                utc_dt = datapoint.startDate\n                iso_str = (utc_dt + timedelta(hours=0)).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n\n                result.append(PriceDataPoint(\n                    startDate=utc_dt,\n                    price=datapoint.price\n                ))\n                await self.database_fetcher.insert_entry(\n                    price=datapoint.price,\n                    iso_date=iso_str\n                )\n        except Exception as e:\n            print(f\"Error filling missing entries: {e}\")\n            raise\n\n    async def fetch_and_process_data(self, time_range:TimeRange) -> List[PriceDataPoint]:\n        \"\"\"\n        Fetch and process price data from the database, fill missing entries from the external API if needed.\n\n        Args:\n            start_date (datetime): Start of the time range.\n            end_date (datetime): End of the time range.\n\n        Returns:\n            List[PriceDataPoint]: Sorted list of price data points for the range, including filled-in values if needed.\n        \"\"\"\n\n\n        start_naive_hki = time_range.startTime.astimezone(ZoneInfo(\"Europe/Helsinki\")).replace(tzinfo=None)\n        end_naive_hki = time_range.endTime.astimezone(ZoneInfo(\"Europe/Helsinki\")).replace(tzinfo=None)\n\n\n        raw_data = await self.database_fetcher.get_entries(\n            start_date=start_naive_hki,\n            end_date=end_naive_hki,\n            select_columns=\"datetime, price\"\n        )\n\n\n        result = self.convert_to_price_data(raw_data)\n\n        missing_entries = self.find_missing_entries_utc(\n            time_range.startTime.astimezone(ZoneInfo(\"UTC\")),\n            time_range.endTime.astimezone(ZoneInfo(\"UTC\")),\n            result\n        )\n        if missing_entries:\n            await self.fill_missing_entries(result, missing_entries)\n\n        return sorted(result, key=lambda x: x.startDate, reverse=False)\n\n    def find_missing_entries_utc(self, start_date_utc: datetime, end_date_utc: datetime, data_utc: List[PriceDataPoint]):\n        \"\"\"\n        Find missing hourly entries in the given UTC time range.\n\n        Args:\n            start_date_utc (datetime): Start of the UTC time range.\n            end_date_utc (datetime): End of the UTC time range.\n            data_utc (List[PriceDataPoint]): List of available data points.\n\n        Returns:\n            List[StartDateModel]: List of StartDateModel objects for missing hours (all in UTC).\n        \"\"\"\n        result = []\n        current_date_utc = start_date_utc\n        while current_date_utc <= end_date_utc:\n            if not any(item.startDate == current_date_utc for item in data_utc):\n                result.append(StartDateModel(startDate=current_date_utc))\n            current_date_utc += timedelta(hours=1)\n        result.sort(key=lambda x: x.startDate, reverse=False)\n        return result\n\n    def calculate_hourly_avg_price(self, data):\n        \"\"\"\n        Calculate hourly average prices from a list of price data points.\n\n        Args:\n            data (List[PriceDataPoint]): List of price data points.\n\n        Returns:\n            List[PriceHourlyAvgPricePoint]: List of hourly average price points. Hours are in Helsinki time.\n        \"\"\"\n        hourly_avg = {}\n        for point in data:\n            hour = point.startDate.astimezone(ZoneInfo(\"Europe/Helsinki\")).hour\n            hourly_avg.setdefault(hour, []).append(point.price)\n\n        result = [\n            HourlyAvgPricePoint(\n                hour=hour,\n                avgPrice=round(sum(prices) / len(prices), 3)\n            )\n            for hour, prices in hourly_avg.items()\n        ]\n        return sorted(result, key=lambda x: x.hour)\n    \n    def calculate_avg_by_weekday(self, data: List[PriceDataPoint]) -> List[PriceAvgByWeekdayPoint]:\n        \"\"\"\n        Calculate average price by weekday from a list of price data points.\n\n        Args:\n            data (List[PriceDataPoint]): List of price data points.\n\n        Returns:\n            List[PriceAvgByWeekdayPoint]: List of average prices by weekday.\n        \"\"\"\n        tz = ZoneInfo(\"Europe/Helsinki\")\n        weekday_prices = {}\n        for point in data:\n            dt = point.startDate.astimezone(tz) if tz else point.startDate\n            weekday = dt.weekday()\n            weekday_prices.setdefault(weekday, []).append(point.price)\n\n        result = [\n            PriceAvgByWeekdayPoint(\n                weekday=weekday,\n                avgPrice=round(sum(prices) / len(prices), 3)\n            )\n            for weekday, prices in weekday_prices.items()\n        ]\n        return sorted(result, key=lambda x: x.weekday)\n",
  "./App/python-server/utils/porssisahko_tools.py": "\"\"\"\nporssisahko_tools.py provides utility functions for working with price data and database readiness in the Eprice backend.\n\nFeatures:\n- Asynchronous function to wait for the PostgreSQL database to become available before starting the application.\n- Conversion utility to transform price and ISO 8601 date data into a dictionary format suitable for the porssisahko table.\n\nIntended Usage:\n- Used by repository and service layers to ensure database readiness and to prepare data for insertion into the porssisahko table.\n- Can be extended with additional utilities for price data processing as needed.\n\nDependencies:\n- asyncpg for asynchronous PostgreSQL operations.\n- Python standard library modules: datetime and time.\n\"\"\"\n\nfrom datetime import datetime\nfrom zoneinfo import ZoneInfo\nimport asyncpg\nimport time\n\nasync def wait_for_database(database_url):\n    \"\"\"\n    Wait for the database to be ready by attempting to connect to it.\n\n    Args:\n        database_url: The URL of the database to connect to.\n\n    Raises:\n        Exception: If the database is not ready after multiple attempts.\n        (logs attempts and failure)\n    \"\"\"\n    max_retries = 10\n    retry_delay = 5  # seconds\n    for attempt in range(max_retries):\n        try:\n            conn = await asyncpg.connect(database_url)\n            await conn.close()\n            print(\"Database is ready.\")\n            return\n        except Exception as e:\n            print(f\"Database not ready (attempt {attempt + 1}/{max_retries}): {e}\")\n            time.sleep(retry_delay)\n    raise Exception(\"Database is not ready after multiple attempts.\")\n\n\ndef convert_to_porssisahko_entry(price, iso_date, predicted=False, convert_to_helsinki_time=True):\n    \"\"\"\n    Converts a price and ISO 8601 date into a dictionary for the porssisahko table.\n\n    Args:\n        price (float): The price value.\n        iso_date (str): The date in ISO 8601 format (e.g., \"2022-11-14T22:00:00.000Z\").\n        predicted (bool): Indicates if the price is predicted. Default is False.\n    Returns:\n        dict: A dictionary with keys: Datetime, Date, Year, Month, Day, Hour, Weekday, Price, Predicted.\n    Raises:\n        ValueError: If the ISO date is not in the correct format.\n    \"\"\"\n    try:\n        if convert_to_helsinki_time:\n            # Parse the ISO date string to a UTC datetime\n            dt_utc = datetime.fromisoformat(iso_date.replace(\"Z\", \"+00:00\"))\n            # Convert to Helsinki time\n            dt_helsinki = dt_utc.astimezone(ZoneInfo(\"Europe/Helsinki\"))\n            dt_naive = dt_helsinki.replace(tzinfo=None)\n        else:\n            # Parse the ISO date string to a naive datetime (UTC)\n            dt_naive = datetime.fromisoformat(iso_date.replace(\"Z\", \"+00:00\")).replace(tzinfo=None)\n        # Extract the weekday\n        weekday = dt_naive.weekday()\n\n        # Return the dictionary\n        return {\n            \"datetime\": dt_naive,  # Use offset-naive datetime\n            \"date\": dt_naive.date(),  # Extract the date part\n            \"year\": dt_naive.year,\n            \"month\": dt_naive.month,\n            \"day\": dt_naive.day,\n            \"hour\": dt_naive.hour,\n            \"weekday\": weekday,\n            \"price\": price,\n            \"predicted\": predicted\n        }\n    except ValueError as e:\n        # Handle invalid date format or parsing errors\n        raise ValueError(f\"Invalid ISO date format: {iso_date}. Error: {e}\")\n\n\n",
  "./Documents/CONTRIBUTIONS.md": "# Contributions\n\n\n\n├── LICENSE\n├── README.md\n├── App\n│   ├── README.md\n│   ├── compose.yaml (anything with containers, Paavo)\n│   ├── project.env (anything with project and container level configurations, Paavo)\n│   │\n│   ├── backend-tests (Setup, Paavo)\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   └── tests\n│   │       ├── test_auth_controller.py (Juho & Paavo)\n│   │       └── test_data_controller.py (Juho)\n│   │\n│   ├── chat-engine (Paavo)\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── _compose.yaml\n│   │   ├── agent_app.py\n│   │   ├── agent_manager.py\n│   │   ├── app.py\n│   │   ├── autoloading_uml.py\n│   │   ├── chat_app.py\n│   │   ├── chat_manager_with_tools.py\n│   │   ├── file_app.py\n│   │   ├── diagrams\n│   │   └── utils\n│   │       ├── __init__.py\n│   │       ├── db_calls.py\n│   │       ├── helpers.py\n│   │       └── tools.py\n│   │\n│   ├── client (Paavo)\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── docker\n│   │   ├── src\n│   │   │   ├── app.css\n│   │   │   ├── app.html\n│   │   │   ├── hooks.server.js\n│   │   │   ├── lib\n│   │   │   │   ├── apis\n│   │   │   │   │   └── data-api.js\n│   │   │   │   ├── assets\n│   │   │   │   │   ├── image12.jpg\n│   │   │   │   │   └── image3.png\n│   │   │   │   ├── components\n│   │   │   │   │   ├── ChatBot.svelte\n│   │   │   │   │   ├── ChatView1.svelte\n│   │   │   │   │   ├── ChatView2.svelte\n│   │   │   │   │   ├── ChatView3.svelte\n│   │   │   │   │   ├── ChatView4.svelte\n│   │   │   │   │   ├── PriceCards.svelte\n│   │   │   │   │   ├── Tabs.svelte\n│   │   │   │   │   └── layout\n│   │   │   │   │       ├── Clock.svelte\n│   │   │   │   │       ├── Footer.svelte\n│   │   │   │   │       ├── Header.svelte\n│   │   │   │   │       └── User.svelte\n│   │   │   │   ├── states\n│   │   │   │   │   ├── usePricesState.svelte.js\n│   │   │   │   │   └── userState.svelte.js\n│   │   │   │   └── utils\n│   │   │   │       ├── clock.js\n│   │   │   │       ├── date-helpers.js\n│   │   │   │       └── stats-helpers.js\n│   │   │   └── routes\n│   │   │       ├── +layout.js\n│   │   │       ├── +layout.server.js\n│   │   │       ├── +layout.svelte\n│   │   │       ├── +page.svelte\n│   │   │       ├── api\n│   │   │       │   └── devchat\n│   │   │       │       └── +server.js\n│   │   │       ├── auth\n│   │   │       │   └── [action]\n│   │   │       │       ├── +page.js\n│   │   │       │       ├── +page.server.js\n│   │   │       │       └── +page.svelte\n│   │   │       ├── chat\n│   │   │       │   └── +page.svelte\n│   │   │       ├── epc\n│   │   │       │   ├── +page.server.js\n│   │   │       │   └── +page.svelte\n│   │   │       ├── logout\n│   │   │       │   ├── +page.js\n│   │   │       │   ├── +page.server.js\n│   │   │       │   └── +page.svelte\n│   │   │       ├── price\n│   │   │       │   ├── +page.server.js\n│   │   │       │   └── +page.svelte\n│   │   │       └── send\n│   │   │           ├── +page.server.js\n│   │   │           └── +page.svelte\n│   │   └── static\n│   │       └── favicon.png\n│   │\n│   ├── data-preparation (Paavo)\n│   │   ├── README.md\n│   │   └── scripts\n│   │       ├── Dockerfile\n│   │       ├── README.md\n│   │       ├── clean_porssisahko.py\n│   │       ├── populate_porssisahko.py\n│   │       └── retrieve_porssisahko_update.sh\n│   │\n│   ├── database-migrations (Paavo)\n│   │   ├── V1__users.sql\n│   │   ├── V2__porssisahko.sql\n│   │   ├── V3__timezone.sql\n│   │   ├── V4__users_add_role.sql\n│   │   ├── V5__porssisahko_load_entries.sql\n│   │   ├── V6__users_add_isverified.sql\n│   │   │   None (bad version naming -- accidentally missed V7)\n│   │   ├── V8__extension_vector.sql\n│   │   └── V9__code.sql\n│   │   ├── V10__code_constraint.sql\n│   │   ├── V11__documents.sql\n│   │   ├── V12__files.sql\n│   │\n│   ├── e2e-tests (Setup, Paavo)\n│   │   ├── Dockerfile\n│   │   └── tests\n│   │       └── frontend.spec.js\n│   │\n│   ├── python-server (Juho & Paavo)\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── main.py (Juho & Paavo)\n│   │   ├── requirements.txt\n│   │   ├── config\n│   │   │   ├── __init__.py\n│   │   │   └── secrets.py\n│   │   ├── controllers\n│   │   │   ├── auth_controller.py (Juho & Paavo)\n│   │   │   └── data_controller.py (Juho)\n│   │   ├── ext_apis\n│   │   │   └── ext_apis.py (Juho)\n│   │   ├── models\n│   │   │   ├── custom_exception.py (Juho)\n│   │   │   ├── data_model.py (Juho)\n│   │   │   └── user_model.py (Paavo)\n│   │   ├── repositories\n│   │   │   ├── porssisahko_repository.py (Paavo)\n│   │   │   └── user_repository.py (Paavo)\n│   │   ├── scheduled_tasks\n│   │   │   └── porssisahko_scheduler.py (Paavo)\n│   │   ├── services\n│   │   │   ├── auth_service.py (Paavo)\n│   │   │   └── data_service.py (Juho)\n│   │   └── utils\n│   │       ├── email_tools.py (Paavo)\n│   │       ├── porssisahko_service_tools.py (Juho)\n│   │       └── porssisahko_tools.py (Paavo)\n│   \n│\n└── Documents\n    ├── README.md\n    ├── backend_design.md\n    ├── openapi_endpoint_descriptions.md\n    ├── project_description.md\n    ├── project_directory_structure.txt\n    ├── api_definitions\n        ├── openapi.json\n        ├── README.md\n    └── diagrams\n        └── sources\n            ├── authentication_call_sequence_diagram.wsd\n            ├── authentication_class_diagram.wsd\n            ├── authentication_use_case.wsd\n            ├── data_access_call_sequence_diagram.wsd\n            ├── llm_retrieval.wsd\n            ├── services_diagram.wsd\n            ├── tool_calling.wsd\n            └── use_case.wsd\n\n\n## Notebooks\n\nAdditionally, the project/developer chat's retrieval mechanism requires some amount of parsing/cleaning/loading (all project code and documents), which is done offline using scripts and `document_loading.ipynb` inside `Eprice/Notebooks/`. These are at least in some parts based on Paavo's (heh, me in 3rd person...) other previous and contemporary projects/works. Not everything is included in this repo, but will be later made available in `https://github.com/PaavoReinikka`. \n\nTo aid in the documentation of the project, there is also a UML diagram generator dashboard in `Eprice/Notebooks/` (using PlantUML syntax). It was written with Python (see Notebooks README for details and instructions).\n\n## Developer chat\n\nThe developer chat is based on OpenAI's gtp-4o-mini, which is basically free for research (and such) purposes. There are also a lot of open-source generative text models (chat and completion) in HF, and also good datasets for fine-tuning. Some of the experiments, with various open models, during this project will eventually be published in `https://github.com/PaavoReinikka`.\n\nThe embedding model used here, is open-source from HuggingFace. There are many good models available at HF, and a there is a leader board for embedding models, which is a good place to start looking (`https://huggingface.co/spaces/mteb/leaderboard`).\n\n## Documentation\n\nMost of the documentation was written by Paavo Reinikka, with some help from Juho Ahopelto, and with a lot of help from Copilot.\n\n",
  "./Documents/KONTRIBUUTIOT.md": "# Kontribuutiot\n\nTämä osio dokumentoi projektin hakemistorakenteen ja siihen osallistuneiden henkilöiden vastuualueet. Jos joku osa-alue on kokonaan yhden henkilön tekemä, sen kansioita/tiedostoja ei tässä erikseen listata. `Eprice/Documents/` kansiosta löytyy yksityiskohtainen dokumentaatio koko projektista.\n\n```code\n|– projektinhallinta \"juuren\" tasolla, \"full stack plus\", (Paavo)\n|\n├── LICENSE\n├── README.md\n├── App\n│   ├── README.md\n│   ├── compose.yaml – konttien hallinta (Paavo)\n│   ├── project.env – projektin ja konttien konfiguraatiot (Paavo)\n│   |\n│   ├── backend-tests – testauksen alustus (Paavo)\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   └── tests\n│   │       ├── test_auth_controller.py – (Juho & Paavo)\n│   │       └── test_data_controller.py – (Juho)\n│   |\n│   ├── chat-engine – (Developer chat, Paavo)\n│   |\n│   ├── client – käyttöliittymä (Paavo)\n│   |\n│   ├── data-preparation – datan esikäsittely ja lataus (Paavo)\n│   |\n│   ├── database-migrations – tietokantamuutokset (Paavo)\n│   │   ├── V1–V12__*.sql – eri tietokantaversiot ja skeemamuutokset\n│   │   │   Huom. V7 puuttuu versionointivirheen takia\n│   |\n│   ├── e2e-tests – kokonaisuuden testauksen alustus (Paavo, mutta en ole ehtinyt vielä kirjoittaa juurikaan testejä)\n│   |\n│   ├── python-server (Juho & Paavo)\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── main.py\n│   │   ├── requirements.txt\n|   |   |\n│   │   ├── config - konfiguraatiot ja salaisuudet\n│   │   │   ├── __init__.py\n│   │   │   └── secrets.py\n|   |   |\n│   │   ├── controllers - autentikointi- ja datakontrollerit\n│   │   │   ├── auth_controller.py (Juho & Paavo)\n│   │   │   └── data_controller.py (Juho)\n|   |   |\n│   │   ├── ext_apis - ulkoiset rajapintakutsut\n│   │   │   └── ext_apis.py (Juho)\n|   |   |\n│   │   ├── models - tietomallit ja poikkeukset\n│   │   │   ├── custom_exception.py (Juho)\n│   │   │   ├── data_model.py (Juho)\n│   │   │   └── user_model.py (Paavo)\n|   |   |\n│   │   ├── repositories – tietokantakerros\n│   │   │   ├── porssisahko_repository.py (Paavo)\n│   │   │   └── user_repository.py (Paavo)\n|   |   |\n│   │   ├── scheduled_tasks – ajoitetut tehtävät (esim. sähkönhintadatan ajantasaisuus)\n│   │   │   └── porssisahko_scheduler.py (Paavo)\n|   |   |\n│   │   ├── services – logiikka reitityksen (controllers) ja tietokannan (repositories) / ulkoisten apien välillä \n│       │   ├── auth_service.py (Paavo)\n│       │   └── data_service.py (Juho)\n|       |\n│       └── utils – apufunctioita yms.\n│          ├── email_tools.py (Paavo)\n│          ├── porssisahko_service_tools.py (Juho)\n│          └── porssisahko_tools.py (Paavo)\n│   \n│\n└── Documents – projektin dokumentaatio\n    ├── README.md\n    ├── backend_design.md\n    ├── openapi_endpoint_descriptions.md\n    ├── project_description.md\n    ├── project_directory_structure.txt\n    |\n    ├── api_definitions\n    |    ├── openapi.json\n    |    ├── README.md\n    |\n    └── diagrams\n        └── sources – PlantUML-muotoiset kaaviot\n            ├── authentication_call_sequence_diagram.wsd\n            ├── authentication_class_diagram.wsd\n            ├── authentication_use_case.wsd\n            ├── data_access_call_sequence_diagram.wsd\n            ├── llm_retrieval.wsd\n            ├── services_diagram.wsd\n            ├── tool_calling.wsd\n            └── use_case.wsd\n```\n\n## Notebooks hakemisto (tavallaan extraa)\n\nDeveloper chatin retrieval toiminto vaatii tietyn määrän esikäsittelyä (parserointi / siivous / lataus), joka tehdään offline-tilassa skripteillä ja tiedostolla `Eprice/Notebooks/document_loading.ipynb`. Nämä perustuvat osittain Paavon (heh, siis minun) aiempiin ja rinnakkaisiin projekteihin. Kaikki ei ole vielä mukana tässä repossa, mutta tullaan julkaisemaan myöhemmin osoitteessa https://github.com/PaavoReinikka.\n\nProjektin dokumentoinnin avuksi on koodattu myös UML-kaavioiden generointityökalu, joka käyttää PlantUML-syntaksia ja on toteutettu Pythonilla (lisätietoja Notebooks README:ssä).\n\n## Kehittäjächat / Developer-Chat\n\n`Eprice/App/chat-engine` kontissa ajettava developer chat käyttää OpenAI:n gpt-4o-mini -mallia, joka on käytännössä ilmainen tutkimus- ja kehityskäyttöön. Projektiin liittyy myös kokeiluja useiden avoimen lähdekoodin generatiivisten mallien kanssa (sekä chatti- että completionmalleilla), jotka julkaistaan myöhemmin osoitteessa https://github.com/PaavoReinikka.\n\nEmbeddaukseen käytetty malli on avoin malli HuggingFacesta. Sieltä löytyy useita laadukkaita vaihtoehtoja, ja hyvä aloituspiste on MTEB Leaderboard: https://huggingface.co/spaces/mteb/leaderboard.\n\n## Dokumentaatio\n\nDokumentaatio on pääosin Paavo Reinikan tekemää. Juho Ahopelto dokumentoi serverin endpointit (openapi endpoints). Dokumentaation teossa on käytetty myös kielimalleja.",
  "./Documents/README.md": "## Documents and materials for Eprice project\n\nDocuments and diagrams in this folder describe different aspects of Eprice app. The files have also been parsed and saved in the database in machine readable format. The project/developer chat uses them as context information.\n\n### Sequence diagrams\n\nInstall vscode sequance diagram -extension (see it's documentation, and https://bramp.github.io/js-sequence-diagrams/ for examples).\n\nYou can open preview in vscode, and export as png/svg.\n\n### class diagrams\n\nInstall PlanUML extension, and install Graphviz:\n\n`sudo apt install graphviz` (on linux)\n\nYou might also need java/jre, which you very likely already have -- if you can't render these, try googling the solution.\n",
  "./Documents/backend_design.md": "# Controller-Service-Repository Pattern & CRUD: An Overview\n\n## Introduction\n\nModern web applications often separate concerns into distinct layers to improve maintainability, testability, and scalability. One popular approach in the backend is the controller-service-repository pattern. This pattern is especially useful for applications with complex business logic and database interactions. We are aiming to comply with this design pattern in the project.\n\nA server that serves a Svelte frontend can use this pattern to handle requests—especially those involving sensitive information — securely and efficiently.\n\n## The Layers Explained\n\n1. **Controller**\n\n   - **Role:** Handles HTTP requests and responses.\n   - **Responsibility:** Receives input from the client (e.g., a Svelte form), calls the appropriate service methods, and returns the result.\n   - **Example:** The controller defines endpoints such as `/api/auth/register` and `/api/auth/login`. These endpoints receive user data, call the service layer, and return JSON responses.\n   - **Error Handling:** Exceptions are mainly caught at the controller level. The controller always returns valid JSON to the frontend, including appropriate error messages when necessary, while avoiding leaking sensitive information.\n\n2. **Service**\n\n   - **Role:** Contains business logic.\n   - **Responsibility:** Implements the core functionality of the application, such as validating credentials, generating tokens, or sending emails. It orchestrates calls to the repository and other utilities.\n   - **Example:** The service layer handles password hashing, verification code generation, and calls to the repository for user data. It may also send emails after successful registration or code updates.\n\n3. **Repository**\n\n   - **Role:** Manages data persistence.\n   - **Responsibility:** Handles all interactions with the database. It provides methods for CRUD operations (Create, Read, Update, Delete) on data models.\n   - **Example:** The repository provides methods like `get_user_by_email`, `create_user`, `verify_code`, and `update_code` to interact with the `users` table in the database.\n\n## CRUD Operations\n\nCRUD stands for Create, Read, Update, Delete—the four basic operations for persistent storage.\n\n- **Create:** Add new records (e.g., registering a new user).\n- **Read:** Retrieve records (e.g., fetching a user by email).\n- **Update:** Modify existing records (e.g., updating a verification code).\n- **Delete:** Remove records (for example, deleting a user).\n\n### Example: Authentication Flow\n\n1. **Register (Create)**\n    - Controller: Receives registration data from the Svelte frontend.\n    - Service: Hashes the password, generates a verification code, and calls the repository to create the user.\n    - Repository: Inserts the new user into the database.\n\n2. **Login (Read)**\n    - Controller: Receives login credentials.\n    - Service: Fetches the user by email and verifies the password.\n    - Repository: Retrieves the user record from the database.\n\n3. **Verify Email (Update)**\n    - Controller: Receives a verification code.\n    - Service: Calls the repository to update the user's verification status.\n    - Repository: Updates the `is_verified` field in the database.\n\n## Why Use Controller-Service-Repository Pattern?\n\n- **Separation of Concerns:** Each layer has a single responsibility, making the codebase easier to understand and maintain.\n- **Testability:** Business logic can be tested independently from HTTP and database layers.\n- **Reusability:** Services and repositories can be reused across different controllers or even applications.\n- **Security:** Sensitive operations (like authentication) are handled server-side, reducing exposure to the client.\n- **Consistent Error Handling:** Exceptions are caught at the controller level, and valid JSON responses are always returned to the frontend, including appropriate error messages without leaking sensitive details.\n\n## How It Works with Svelte\n\nA Svelte frontend can use form actions and server-side calls for sensitive or restricted operations. When a user submits a form (such as login or register), the request is sent to the FastAPI backend, which processes it through the controller-service-repository pipeline. This ensures data is validated, business rules are enforced, and database operations are performed securely.\n\n## Summary Diagram\n\n```plantuml\n@startuml\n[Svelte Form] --> [Controller (FastAPI Route)]\n[Controller (FastAPI Route)] --> [Service (Business Logic)]\n[Service (Business Logic)] --> [Repository (Database Access)]\n[Repository (Database Access)] --> [Database]\n@enduml\n```\n\nThis pattern helps build robust, maintainable, and secure web applications, especially when handling authentication and other sensitive operations.",
  "./Documents/diagrams/sources/authentication_call_sequence_diagram.wsd": "@startuml\ntitle Sequence Diagram for User Registration, Login, Verification, and Logout using form-actions and JWT\n\nClient -> ClientServer: register form-action\nClientServer -> Server: api/auth/register\nServer -> Database: insert new user into database\nnote over Server, Database: hash password and attempt to insert\\nif email not already registered\nalt registration success\n    Server -> EmailService: send verification code to email\n    EmailService --> Server: email sent\n    Server --> Client: registration success (prompt verify)\nelse registration failure\n    Server --> Client: registration failure\nend\nnote over Client: On success: prompt for verification code\n\nClient -> ClientServer: verify form-action (email, code)\nClientServer -> Server: api/auth/verify\nServer -> Database: update user is_verified if code matches\nDatabase --> Server: success/failure\nalt verification success\n    Server --> Client: verification success\nelse verification failure\n    Server --> Client: verification failure\nend\n\nClient -> ClientServer: resend verification code form-action (email)\nClientServer -> Server: api/auth/resend\nServer -> Database: update verification_code for user\nalt update success\n    Server -> EmailService: send new verification code to email\n    EmailService --> Server: email sent\n    Server --> Client: resend success\nelse update failure\n    Server --> Client: resend failure\nend\n\nClient -> ClientServer: login form-action (email, password)\nClientServer -> Server: api/auth/login\nServer -> Database: select user from database\nDatabase --> Server: user data / not found\nalt user found\n    Server -> Server: verify password hash\n    alt password correct and is_verified\n        Server -> Server: pack user info in JWT\n        Server --> ClientServer: login success and set-cookie (JWT)\n        note over Client, ClientServer: JWT available to client components\n    else not verified\n        Server --> ClientServer: login failure (email not verified)\n    else password incorrect\n        Server --> ClientServer: login failure (wrong credentials)\n    end\nelse user not found\n    Server --> ClientServer: login failure (wrong credentials)\nend\n\nClient -> ClientServer: logout form-action\nClientServer -> Server: api/auth/logout\nServer --> ClientServer: delete JWT cookie\nnote over ClientServer, Client: Client server clears JWT payload\n\n@enduml",
  "./Documents/diagrams/sources/authentication_class_diagram.wsd": "@startuml\ntitle FastAPI Server-side Authentication (Updated)\n\npackage \"Models\" {\n    class User {\n        + email: EmailStr\n        + password: str\n        + validate_password(password: str): str\n    }\n    class UserCode {\n        + email: EmailStr\n        + code: str\n        + validate_code(code: str): str\n    }\n    class EmailRequest {\n        + email: EmailStr\n    }\n}\n\npackage \"Repositories\" {\n    class UserRepository {\n        - database_url: str\n        + get_user_by_email(email: str): dict | None\n        + create_user(email: str, password_hash: str, verification_code: str): None\n        + verify_code(email: str, verification_code: str): Any\n        + update_code(email: str, new_code: str): None\n    }\n}\n\npackage \"Services\" {\n    class AuthService {\n        - user_repository: UserRepository\n        - pwd_context: CryptContext\n        + get_password_hash(password: str): str\n        + generate_verification_code(): str\n        + verify_password(plain_password: str, hashed_password: str): bool\n        + create_access_token(data: dict, expires_delta: timedelta = None): str\n        + authenticate_user(email: str, password: str): dict | None\n        + register_user(email: str, password: str): None\n        + verify_user(email: str, code: str): None\n        + update_verification_code(email: str): None\n    }\n}\n\npackage \"Controllers\" {\n    class AuthController {\n        + register(user: User, response: Response): dict\n        + login(user: User, response: Response): dict\n        + logout(response: Response): dict\n        + verify(user_code: UserCode, response: Response): dict\n        + resend_verification_code(request: EmailRequest, response: Response): dict\n        + create_jwt_middleware(public_routes: list): Callable\n    }\n}\n\n' Relationships\nAuthService --> UserRepository\nAuthController --> AuthService\nAuthController --> User\nAuthController --> UserCode\nAuthController --> EmailRequest\n\n@enduml",
  "./Documents/diagrams/sources/authentication_use_case.wsd": "@startuml\ntitle Use Case Diagram for authentication in Electricity Market App\n\nactor \"Signed-in User\" as User\nactor \"Visitor\" as Visitor\n\npackage \"Frontend (Svelte)\" {\n    usecase \"Register\" as Register\n    usecase \"Login\" as Login\n    usecase \"Logout\" as Logout\n    usecase \"Verify Email\" as VerifyEmail\n    usecase \"Resend Verification Code\" as ResendCode\n}\n\npackage \"Backend (FastAPI)\" {\n    package \"User Authentication\" as Auth  {\n        usecase \"Signal front to\\n remove JWT\" as RemoveJWT\n        usecase \"Hash Password and\\n send with email to DB\\n(un-verified)\" as HashPassword\n        usecase \"Verify Status & Password\\nand pack JWT\" as VerifyPassword\n        usecase \"Generate Verification Code\\nand send email\" as GenCode\n        usecase \"Verify User Code\" as VerifyUserCode\n        usecase \"Update Verification Code\\nand send email\" as UpdateCode\n    }\n    usecase \"Manage route access\\nwith JWT's\" as ManageJWT\n}\n\npackage \"Database\" {\n    usecase \"Store and Retrieve User Data\" as UserDB\n}\n\nUser --> Login\nUser --> Logout\nUser --> Register\nUser --> VerifyEmail\nUser --> ResendCode\nVisitor --> Register\nVisitor --> Login\n\nRegister --> HashPassword\nRegister --> GenCode\n\nLogin --> VerifyPassword\nVerifyPassword <--> UserDB\nVerifyPassword <--> ManageJWT\nLogout <--> RemoveJWT\n\nHashPassword --> UserDB\nGenCode --> UserDB\n\nVerifyEmail --> VerifyUserCode\nVerifyUserCode --> UserDB\n\nResendCode --> UpdateCode\nUpdateCode --> UserDB\n\n@enduml",
  "./Documents/diagrams/sources/data_access_call_sequence_diagram.wsd": "@startuml\ntitle FastAPI Server, access to data\n\nTitle: Sequence Diagram for data access\nParticipant Client\nParticipant Server\nParticipant Database\nParticipant ExternalAPI\nClient->Server: POST /api/data/ <identifier>\nNote over Server: Check credentials on\\nprotected routes\nServer->Database: query if cached\nDatabase->Server: Success/Failure [,data]\nNote over Server: On success: return data\\ndirectly to client\\non failure: make api call\nServer->ExternalAPI: api call\nExternalAPI->Server: success/failure [,data]\nServer->Client: Success/Failure [,data]\nServer->Database: On success: cache data\nNote over Client: On success: display data\n\n@enduml",
  "./Documents/diagrams/sources/frontend_structure.wsd": "@startuml\ntitle Eprice Svelte Frontend Structure\n\npackage \"src\" {\n  package \"routes\" {\n    [auth]\n    [chat]\n    [price]\n    [epc]\n    [logout]\n    [send]\n    [api]\n  }\n  package \"lib\" {\n    [apis]\n    [components]\n    [states]\n    [utils]\n    [assets]\n  }\n}\n\n[components] ..> [states] : uses\n[components] ..> [apis] : fetches data\n[routes] ..> [components] : composes UI\n[routes] ..> [apis] : (server-side) fetches data\n[routes] ..> [states] : manages state\n\n@enduml",
  "./Documents/diagrams/sources/llm_retrieval.wsd": "@startuml\ntitle LLM engine with retrieval (pre-processed data)\nUser -> LLM_Client: User initiates a query\nnote over LLM_Client: Client embeds the query\nnote over Database: DB has text chunks\\nand embeddings\nLLM_Client -> Database: Send embedded query\nnote over Database: similarity search\\nfor relevant texts\nDatabase -> LLM_Client: Return relevant text chunks\nnote over LLM_Client: Combine query with\\nretrieved text chunks\nLLM_Client -> LLM_Model: formatted prompt with query and context\nLLM_Model -> LLM_Client: Return generated response\nLLM_Client -> User: return generated response\n@enduml",
  "./Documents/diagrams/sources/services_diagram.wsd": "@startuml\ntitle Main services and their interactions\nskinparam rectangle {\n    BackgroundColor #FDF6E3\n    BorderColor #586e75\n}\nskinparam cloud {\n    BackgroundColor #FDF6E3\n    BorderColor #586e75\n}\n\ncloud \"Docker Network\\nmanages networking\" as DockerNetwork\n\npackage \"Database Layer\" {\n    [Database] <<container>> \n    [Database Migrations] <<container>> \n}\n\npackage \"Backend Layer\" {\n    [Server] <<container>> \n    [Developer Chat] <<container>> \n}\n\npackage \"Frontend Layer\" {\n    [Client] <<container>> \n    [E2E Tests] <<container>>\n}\n\ncloud \"External APIs\" as ExternalAPIs\n\n[Database Migrations] --> [Database] : Applies migrations\n[Server] --> [Database] : Reads/Writes data\n[Client] --> [Server] : API calls (port 8000)\n[Client] --> [Developer Chat] : Dev chat (port 7860-7863)\n[Gradio Dashboards] --> [Developer Chat] : Access via Gradio\\n(optional)\n[E2E Tests] --> [Client] : Tests frontend\n[Server] --> ExternalAPIs : Makes external API calls\n[Developer Chat] --> [Database] : Uses for retrieval\n\nnote right of Client\nRuns on port 5173\nend note\n\n@enduml",
  "./Documents/diagrams/sources/tool_calling.wsd": "@startuml\ntitle LLM engine with tool calling\nUser->LLM_Client: User initiates a query\nNote over LLM_Client: Client adds system messages\\nand tool calling instructions\nLLM_Client->LLM_Model: formatted prompt\nNote over LLM_Model: LLM processes the prompt and\\nidentifies if a tool is needed\nLLM_Model->LLM_Client: return tool call instructions\nLLM_Client->Tool: Call tool with parameters\nTool->LLM_Client: return tool results\nNote over LLM_Client: Client adds system messages\\nand results to the prompt\nLLM_Client->LLM_Model: formatted prompt with tool results\nLLM_Model->LLM_Client: return generated response\nLLM_Client->User: return generated response\n@enduml",
  "./Documents/diagrams/sources/use_case.wsd": "@startuml\ntitle Use Case Diagram for Electricity Market App\n\nactor \"Signed-in User\" as User\nactor \"Visitor\" as Visitor\n\npackage \"Frontend (Svelte)\" {\n    usecase \"View Current Electricity Prices\" as ViewPrices\n    usecase \"Request Historical/predicted Data\" as RequestHistorical\n    usecase \"Chat with LLM\" as ChatWithLLM\n}\n\npackage \"Database\" {\n    usecase \"Store and Retrieve Cached Data\" as CacheDB\n    usecase \"Store and Retrieve User Data\\n(Auth service only)\" as UserDB\n    usecase \"Text chunks and\\nvector embeddings\" as VectorDB\n}\n\npackage \"LLM\" {\n    usecase \"Embed Query and Search Vector DB\" as EmbedSearch\n    usecase \"Format prompt with query\\nand context\" as PromptLLM\n    usecase \"LLM engine\" as LLMengine\n}\n\npackage \"Backend (FastAPI)\" {\n    usecase \"Fetch Current Prices\" as FetchPrices\n    usecase \"Fetch Data\" as FetchData\n    usecase \"Check Cache\" as CheckCache\n    usecase \"Retrieve Data from External APIs\\nand chache\" as RetrieveExternal\n}\n\npackage \"External APIs\" {\n    usecase \"Fetch Data from External APIs\" as ExternalAPIs\n}\n\nVisitor --> ViewPrices\nUser --> ViewPrices\nUser --> RequestHistorical\nUser --> ChatWithLLM\n\nViewPrices --> FetchPrices\nRequestHistorical --> FetchData\nFetchPrices --> CheckCache\nFetchData --> CheckCache\nCheckCache --> CacheDB\nCheckCache --> RetrieveExternal\nRetrieveExternal --> CacheDB\nRetrieveExternal --> ExternalAPIs\n\nChatWithLLM --> EmbedSearch\nEmbedSearch --> VectorDB\nEmbedSearch --> PromptLLM\nPromptLLM --> LLMengine\n\n@enduml",
  "./Documents/frontend_description.md": "# Svelte Frontend Design: An Overview\n\n## Introduction\n\nThe Eprice frontend is built using [Svelte](https://svelte.dev/), a modern JavaScript framework for building fast and reactive user interfaces. The project structure is organized to promote maintainability, scalability, and clear separation of concerns. The frontend communicates with the backend via HTTP APIs and leverages Svelte’s built-in features for routing, state management, and component-based development.\n\n## Directory Structure\n\nThe main frontend code resides in `App/client/src`, which is organized as follows:\n\n- **routes/**: SvelteKit routing and server-side logic for each page.\n- **lib/**: Shared libraries, including API clients, reusable components, state management, and utility functions.\n- **static/**: Static assets such as images and favicon.\n\n## Key Concepts\n\n- **Component-Based UI**: UI is composed of reusable Svelte components (e.g., `ChatBot.svelte`, `PriceCards.svelte`).\n- **Routing**: SvelteKit file-based routing handles navigation and server-side logic for each route.\n- **State Management**: Shared state is managed using Svelte stores (see `states/`).\n- **API Integration**: Communication with the backend is handled via API modules in `lib/apis/`.\n- **Server-Side Logic**: `+page.server.js` and `+layout.server.js` files enable server-side data fetching and actions.\n\n---\n\n## Routing and Page Structure\n\nThe frontend uses SvelteKit’s file-based routing system, where each file or directory under `src/routes/` corresponds to a route in the application. This enables clear organization of pages and their associated logic:\n\n- **Top-Level Pages:** Each directory (e.g., `/auth`, `/chat`, `/price`, `/epc`, `/logout`, `/send`) represents a distinct page or feature.\n- **Server-Side Logic:** Files ending with `.server.js` (e.g., `+page.server.js`, `+layout.server.js`) handle server-side data fetching, form actions, and secure operations.\n- **Layouts:** Shared layouts are defined in `+layout.svelte` and `+layout.js`, providing consistent structure and logic across multiple pages.\n- **API Routes:** The `api/` directory (e.g., `api/devchat/+server.js`) is used for defining custom backend endpoints that can be called from the frontend. The devchat api is a \"mock\" api, and it's only purpose is to verify that the developer chat container is up and running (it is not part of production configuration).\n\nThis structure allows for both client-side interactivity and secure server-side processing, making it easy to build dynamic and secure web applications.\n\n---\n\n## Components\n\nThe UI is built from modular, reusable Svelte components located in `lib/components/`. Each component encapsulates its own markup, styles, and logic, making it easy to compose complex interfaces from simple building blocks.\n\n- **Feature Components:** Components like `ChatBot.svelte`, `PriceCards.svelte`, and the various `ChatView*.svelte` files implement core features and views of the application.\n- **Layout Components:** The `layout/` subdirectory contains shared UI elements such as `Header.svelte`, `Footer.svelte`, `Clock.svelte`, and `User.svelte`, which are used across multiple pages for a consistent look and feel.\n- **Reusability:** Components are designed to be reusable and composable, allowing for rapid development and easy maintenance.\n\nThis approach enables a clear separation of concerns, improves code readability, and supports scalable frontend development.\n\n---\n\n## State Management\n\nState management in the Eprice frontend leverages Svelte's built-in reactivity, which provides a simple way to share state across components. Shared states are organized under `lib/states/`, making it easy to manage application-wide data such as user authentication status or price data.\n\n- **Example – User State:**  \n  The `userState.svelte.js` file defines a stateful object for managing the current user's authentication and profile information. This object can be imported and used by any component or page that needs to react to changes in user status (e.g., login, logout, or profile updates).\n- **Usage:**  \n  Components can follow the `user` state to display user-specific UI or trigger actions when the authentication state changes. For example, the header component can show the user's email when logged in, or a login button otherwise.\n\nWe also use stateful variables across the frontend (defined with `$state(<value>)`). It is possible to mix normal variables and Svelte's stateful variables in code, and there are various mechanisms besides stateful variables that allow reactivity -- updating our frontend to fully leverage Svelte's in-built features is a major backlog item. \n\n---\n\n## API Integration\n\nCommunication between the frontend and backend is handled through dedicated API modules located in `lib/apis/`. These modules encapsulate HTTP requests, making it easy for components and pages to interact with backend endpoints in a consistent and reusable way.\n\n- **API Modules:**  \n  For example, `data-api.js` provides functions to fetch or send data related to prices or other business logic. By centralizing API calls, the codebase avoids duplication and makes it easier to update endpoints or request logic in one place.\n\n- **Usage in Components and Pages:**  \n  Components and pages import these API modules to perform actions such as fetching price data, submitting forms, or handling authentication (*see the subchapter for caviats*). This keeps UI logic separate from data-fetching logic and improves maintainability.\n\n- **Error Handling:**  \n  API modules can include error handling to ensure that failures are managed gracefully and meaningful feedback is provided to the user interface.\n\nThis approach ensures a clear separation between UI and data access logic, supports code reuse, and simplifies future changes to backend communication.\n\n---\n\n### Note on API Call Encapsulation\n\nWhile the ideal approach is to encapsulate all API calls within modules under `lib/apis/` and import them wherever needed, time constraints during development led to a pragmatic divergence from this pattern. In the current implementation, many server-side API calls are defined directly within the corresponding `+page.server.js` files, rather than being abstracted into reusable modules.\n\n- **Current Approach:**  \n  For example, in the `price/` and `epc/` routes, the logic for fetching or posting data to the backend is written directly inside the `+page.server.js` files. This means that data-fetching and business logic are mixed with route handling, which can make the code harder to maintain and reuse.\n\n- **Backlog / To-Do:**  \n  Refactoring these server-side API calls into dedicated modules under `lib/apis/` is a planned improvement. This would bring the server-side code in line with the client-side pattern, promoting better separation of concerns, easier testing, and improved maintainability.\n\nThis divergence is a known technical debt and is tracked as a backlog item for future development.\n\n---\n\n## Client Directory Structure and Route Purposes\n\nHere’s a high-level overview of `App/client/src` -- the directory is organized to support a modular and scalable frontend:\n\n- **routes/**  \n  Contains all the application routes. Each subdirectory or file corresponds to a page or API endpoint:\n  - `/auth/` – Handles user authentication (login, register, verification.).\n  - `/chat/` – Provides the chat interface and related features.\n  - `/price/` – First main view, allows user to retrieve price data and visualize it in various ways.\n  - `/epc/` – Second main view,  allows user to retrieve consumption/production data and visualize it in various ways.\n  - `/logout/` – Manages user logout functionality.\n  - `/send/` – Handles re-sending email verification.\n  - `/api/` – Contains custom API endpoints for internal or development use (e.g., `devchat`).\n\n- **lib/**  \n  Shared code and resources:\n  - `apis/` – API client modules for backend communication.\n  - `components/` – Reusable Svelte UI components.\n  - `states/` – For state management across pages and components.\n  - `utils/` – Utility/helper functions.\n  - `assets/` – Static assets like images.\n\n- **static/**  \n  Public static files (e.g., favicon, images) served directly by the frontend.\n\nThis structure supports clear separation of concerns, making it easier to maintain, extend, and onboard new contributors. Each route directory typically contains:\n- `+page.svelte` for the UI,\n- `+page.server.js` for server-side logic,\n- and optionally `+page.js` for client-side logic.\n\n---\n\n## Global Styles and UI Frameworks\n\nThe frontend also includes global configuration and styling files at the root of the `client` directory:\n\n- **app.css**  \n  This file contains global CSS styles that apply to the entire application. It is the main entry point for custom styles and for importing third-party CSS frameworks.\n\n- **app.html**  \n  The HTML template for the application. It defines the base structure of the HTML document, including where the Svelte app is mounted.\n\n- We also use `<style>` blocks here and there in our pages and compoenents. Moving all these in global `app.css` is also a backlog item.\n\n### UI Frameworks\n\n- **Tailwind CSS**  \n  The project uses [Tailwind CSS](https://tailwindcss.com/) for utility-first, responsive styling. Tailwind classes are used throughout Svelte components to rapidly build custom designs without leaving the markup.\n\n- **Skeleton UI**  \n  [Skeleton](https://www.skeleton.dev/) (via Skeleton Labs) is integrated for ready-made, accessible UI components and design tokens. This helps maintain a consistent look and feel while speeding up development.\n\nThese tools and files ensure a consistent, modern, and customizable user interface across the application.\n\n---",
  "./Documents/openapi_endpoint_descriptions.md": "# Eprice API Endpoint Overview\n\nThis document provides a concise technical overview of the main API endpoints exposed by the Eprice backend, as described in the OpenAPI specification.  \nThe API is organized into **public data endpoints** (for electricity, etc.) and **authentication endpoints**.  \nAll other endpoints require authentication via JWT.\n\nThe API is designed for both public and authenticated use. Public endpoints provide enough data for basic electricity price queries and user authentication, while authenticated endpoints allow access to more detailed or user-specific data.\n\n- The use of POST for range queries (instead of GET with query parameters) allows for more complex request bodies and easier extension in the future.\n- The API is well-structured for integration with frontend applications and external systems.\n\n---\n\n## Public Endpoints\n\nThese endpoints are accessible **without authentication** (no JWT required):\n\n### Electricity Data endpoint\n\n- **GET `/api/public/data`**  \n  Returns a list of market price data points (historical and/or current).  \n  **Response:** Array of objects with `startDate` (RFC 3339 UTC string) and `price` (euro cents).\n\n### Authentication endpoints\n\n- **POST `/api/auth/register`**  \n  Registers a new user.  \n  **Request:** JSON with `email` and `password`.  \n  **Response:** Confirmation or validation error.\n\n- **POST `/api/auth/login`**  \n  Authenticates a user and returns a JWT (usually set as a cookie).  \n  **Request:** JSON with `email` and `password`.  \n  **Response:** Confirmation or validation error.\n\n- **POST `/api/auth/verify`**  \n  Verifies a user by checking the verification code.  \n  **Request:** JSON with `email` and `code`.  \n  **Response:** Confirmation or validation error.\n\n- **POST `/api/auth/resend`**  \n  Resends the verification code to the user's email.  \n  **Request:** JSON with `email`.  \n  **Response:** Confirmation or validation error.\n\n- **GET `/api/auth/logout`**  \n  Logs out the user by clearing the JWT cookie.  \n  **Response:** Confirmation.\n\n---\n\n## Protected Data Endpoints\n\nThe following endpoints **require authentication** (JWT):\n\n### Market Price data endpoint\n\n- **GET `/api/data/today`**  \n  Returns today's market price data points.  \n  **Response:** Array of objects with `startDate` and `price`.\n\n- **POST `/api/price/range`**  \n  Returns market price data for a specified time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of price data points.\n\n### Wind Power data endpoint\n\n- **GET `/api/windpower`**  \n  Returns the latest wind power production forecast.  \n  **Response:** Object with `startTime`, `endTime`, and `value`.\n\n- **POST `/api/windpower/range`**  \n  Returns wind power production data for a given time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of forecast data points.\n\n### Consumption data endpoint\n\n- **GET `/api/consumption`**  \n  Returns the latest electricity consumption forecast.  \n  **Response:** Object with `startTime`, `endTime`, and `value`.\n\n- **POST `/api/consumption/range`**  \n  Returns consumption data for a given time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of consumption data points.\n\n### Production data endpoint\n\n- **GET `/api/production`**  \n  Returns the latest electricity production forecast.  \n  **Response:** Object with `startTime`, `endTime`, and `value`.\n\n- **POST `/api/production/range`**  \n  Returns production data for a given time range.  \n  **Request:** JSON with `startTime` and `endTime` (RFC 3339).  \n  **Response:** Array of production data points.\n\n---\n\n## General Notes about api endpoints\n\n- **Validation:**  \n  Most endpoints validate input and return a 422 error for malformed requests.\n\n- **Error Handling:**  \n  On server errors, endpoints return a 500 status with an error message.\n\n- **Authentication:**  \n  Only the endpoints listed under \"Public Endpoints\" are accessible without a JWT.  \n  All other endpoints require authentication.\n\n- **Data Format:**  \n  All timestamps are in RFC 3339 UTC format. Numeric values are typically floats (e.g., price, temperature).\n\n---\n\n## API Docs\n\n- **GET `/docs`**  \n  API documentation (Swagger UI).\n\n- **GET `/openapi.json`**  \n  OpenAPI specification in JSON format.\n\n",
  "./Documents/project_description.md": "# Project Overview\n\nThis is a software development project for Taitotalo's Python programmer course. We develop a web application using container technology, with Svelte and JavaScript frontend (using deno), FastAPI backend, Postgres database with vector extension, Flyway migrations, Playwright e2e-tests, Pytest backend tests, and a chat-engine (HuggingFace, Langchain and OpenAI). A concise description of the project is given in this document, but more technical information can be found from `./App/README.md`.\n\n\n## Application Overview\n\nEprice is an application that show users market electricity price and additional related information, such as electricity consumption and production. For non-registered users, only the current 24 hour period is covered, and only the market price is shown. For registered users, also historical data is available for market price and for production/consumption. The data is represented with graphs and statistics.\n\nFor registered users there is also a chat-engine available, which has access to specific source material, and which augments the user queries with retrieved context information. \n\n---\n\n## Services Overview\n\nThe system consists of multiple containerized services that work together to provide functionality for the Eprice application, including a database, backend server, client application, and additional components for migrations, testing, and a chat engine.\n\n### 1. Database\n- **Ports**: `5432` (default PostgreSQL port)\n- **Purpose**: Stores application data, including user information and embeddings for retrieval.\n\n### 2. Database Migrations\n- **Purpose**: Handles database schema migrations.\n- **Depends On**: `Database`\n\n### 3. Server\n- **Ports**: `8000`\n- **Purpose**: Provides backend APIs for the client and other services.\n- **Depends On**: `Database`\n- **Additional Functionality**: Makes external API calls.\n\n### 4. Client\n- **Ports**: `5173` (Svelte client)\n- **Purpose**: Frontend application for user interaction.\n- **Depends On**: `Server`, `Chat Engine`\n\n### 5. E2E Tests\n- **Purpose**: Runs end-to-end tests for the system.\n- **Depends On**: `Client`\n\n### (Extra) Chat Engine\n- **Ports**: `7860-7861`\n- **Purpose**: Provides a chat engine for interaction with the project. This is meant for developers and maintainers, not the app's end-users. Chat uses retrieval augmented generation, and the retriever has access to project documentation and code. There are 2 distinct clients: A traditional streaming chat, and an Agent (for more complex reasoning tasks).\n- **Depends On**: `Database`\n\n---\n\n## Main services and their interactions\n\n![Main services and their interactions](./diagrams/images/services_diagram.png)\n\n\n## Additional Services Overview\n\n### Data-preparation\n\n- **Purpose**: Can be used independently to retrieve or update data. Saves data to a location that is available to migrations and the database.\n\n### Backend-tests\n\n- **Purpose**: Can be used to test backend functionality independently from the frontend. Beyond fault testing, backend-tests also give additional information and warnings which would be hidden from the e2e-tests.\n- **Depends On**: Database and Server.\n\nThe backend tests are mainly for development purposes, as the e2e-tests should cover the main functionalities by the end of development.\n",
  "./Documents/project_directory_structure.txt": "├── LICENSE\n├── README.md\n├── App\n│   ├── README.md\n│   ├── compose.yaml\n│   ├── project.env\n│   ├── backend-tests\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   └── tests\n│   │       ├── test_auth_controller.py\n│   │       └── test_data_controller.py\n│   ├── chat-engine\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── _compose.yaml\n│   │   ├── agent_app.py\n│   │   ├── agent_manager.py\n│   │   ├── app.py\n│   │   ├── autoloading_uml.py\n│   │   ├── chat_app.py\n│   │   ├── chat_manager_with_tools.py\n│   │   ├── file_app.py\n│   │   ├── diagrams\n│   │   └── utils\n│   │       ├── __init__.py\n│   │       ├── db_calls.py\n│   │       ├── helpers.py\n│   │       └── tools.py\n│   ├── client\n│   │   ├── Dockerfile\n│   │   ├── README.md\n│   │   ├── src\n│   │   │   ├── app.css\n│   │   │   ├── app.html\n│   │   │   ├── hooks.server.js\n│   │   │   ├── lib\n│   │   │   │   ├── apis\n│   │   │   │   │   └── data-api.js\n│   │   │   │   ├── assets\n│   │   │   │   │   ├── image12.jpg\n│   │   │   │   │   └── image3.png\n│   │   │   │   ├── components\n│   │   │   │   │   ├── ChatBot.svelte\n│   │   │   │   │   ├── ChatView1.svelte\n│   │   │   │   │   ├── ChatView2.svelte\n│   │   │   │   │   ├── ChatView3.svelte\n│   │   │   │   │   ├── ChatView4.svelte\n│   │   │   │   │   ├── PriceCards.svelte\n│   │   │   │   │   ├── Tabs.svelte\n│   │   │   │   │   └── layout\n│   │   │   │   │       ├── Clock.svelte\n│   │   │   │   │       ├── Footer.svelte\n│   │   │   │   │       ├── Header.svelte\n│   │   │   │   │       └── User.svelte\n│   │   │   │   ├── states\n│   │   │   │   │   ├── usePricesState.svelte.js\n│   │   │   │   │   └── userState.svelte.js\n│   │   │   │   └── utils\n│   │   │   │       ├── clock.js\n│   │   │   │       ├── date-helpers.js\n│   │   │   │       └── stats-helpers.js\n│   │   │   └── routes\n│   │   │       ├── +layout.js\n│   │   │       ├── +layout.server.js\n│   │   │       ├── +layout.svelte\n│   │   │       ├── +page.server.js\n│   │   │       ├── +page.svelte\n│   │   │       ├── api\n│   │   │       │   └── devchat\n│   │   │       │       └── +server.js\n│   │   │       ├── auth\n│   │   │       │   └── [action]\n│   │   │       │       ├── +page.js\n│   │   │       │       ├── +page.server.js\n│   │   │       │       └── +page.svelte\n│   │   │       ├── chat\n│   │   │       │   └── +page.svelte\n│   │   │       ├── epc\n│   │   │       │   ├── +page.server.js\n│   │   │       │   └── +page.svelte\n│   │   │       ├── logout\n│   │   │       │   ├── +page.js\n│   │   │       │   ├── +page.server.js\n│   │   │       │   └── +page.svelte\n│   │   │       ├── price\n│   │   │       │   ├── +page.server.js\n│   │   │       │   └── +page.svelte\n│   │   │       └── send\n│   │   │           ├── +page.server.js\n│   │   │           └── +page.svelte\n│   │   └── static\n│   │       └── favicon.png\n│   ├── data-preparation\n│   │   ├── README.md\n│   │   └── scripts\n│   │       ├── Dockerfile\n│   │       ├── README.md\n│   │       ├── clean_porssisahko.py\n│   │       ├── populate_porssisahko.py\n│   │       └── retrieve_porssisahko_update.sh\n│   ├── database-migrations\n│   │   ├── V10__code_constraint.sql\n│   │   ├── V11__documents.sql\n│   │   ├── V12__files.sql\n│   │   ├── V13__fingrid.sql\n│   │   ├── V14__fingrid_load_entries.sql\n│   │   ├── V1__users.sql\n│   │   ├── V2__porssisahko.sql\n│   │   ├── V3__timezone.sql\n│   │   ├── V4__users_add_role.sql\n│   │   ├── V5__porssisahko_load_entries.sql\n│   │   ├── V6__users_add_isverified.sql\n│   │   ├── V8__extension_vector.sql\n│   │   └── V9__code.sql\n│   ├── e2e-tests\n│   │   ├── Dockerfile\n│   │   └── tests\n│   │       └── frontend.spec.js\n│   └── python-server\n│       ├── Dockerfile\n│       ├── README.md\n│       ├── main.py\n│       ├── requirements.txt\n│       ├── config\n│       │   ├── __init__.py\n│       │   └── secrets.py\n│       ├── controllers\n│       │   ├── auth_controller.py\n│       │   └── data_controller.py\n│       ├── ext_apis\n│       │   └── ext_apis.py\n│       ├── models\n│       │   ├── custom_exception.py\n│       │   ├── data_model.py\n│       │   └── user_model.py\n│       ├── repositories\n│       │   ├── fingrid_repository.py\n│       │   ├── porssisahko_repository.py\n│       │   └── user_repository.py\n│       ├── scheduled_tasks\n│       │   └── porssisahko_scheduler.py\n│       ├── services\n│       │   ├── auth_service.py\n│       │   └── data_service.py\n│       └── utils\n│           ├── email_tools.py\n│           ├── fingrid_service_tools.py\n│           ├── porssisahko_service_tools.py\n│           └── porssisahko_tools.py\n└── Documents\n    ├── CONTRIBUTIONS.md\n    ├── KONTRIBUUTIOT.md\n    ├── README.md\n    ├── backend_design.md\n    ├── frontend_description.md\n    ├── openapi_endpoint_descriptions.md\n    ├── project_description.md\n    ├── project_directory_structure.txt\n    ├── projektin_kuvaus.md\n    └── diagrams\n        └── sources\n            ├── authentication_call_sequence_diagram.wsd\n            ├── authentication_class_diagram.wsd\n            ├── authentication_use_case.wsd\n            ├── data_access_call_sequence_diagram.wsd\n            ├── frontend_structure.wsd\n            ├── llm_retrieval.wsd\n            ├── services_diagram.wsd\n            ├── tool_calling.wsd\n            └── use_case.wsd\n",
  "./Documents/projektin_kuvaus.md": "# Projektin yleiskuvaus\n\nTämä on ohjelmistokehitysprojekti Taitotalon Python-ohjelmoijan kurssille. Kehitämme web-sovelluksen, joka hyödyntää konttiteknologiaa. Frontend on toteutettu Svelte- ja JavaScript-teknologioilla (käyttäen deno:a), ja backend FastAPI:lla. Tietokantana toimii Postgres vektorilaajennuksella, ja skeemamuutokset hoidetaan Flyway-migraatioilla. Päättymistestit tehdään Playwrightilla ja backendin yksikkötestaus Pytestillä. Projektissa on myös chat-engine, joka hyödyntää HuggingFacea, Langchainia ja OpenAI:ta.\n\nTässä dokumentissa annetaan tiivis kuvaus projektista, mutta tarkempi tekninen dokumentaatio löytyy tiedostosta ./App/README.md.\n\n## Sovelluksen yleiskuvaus\n\nEprice on sovellus, joka näyttää käyttäjälle pörssisähkön hinnan ja siihen liittyvää lisätietoa, kuten sähkönkulutusta ja -tuotantoa. Rekisteröitymättömille käyttäjille näytetään vain kuluvan vuorokauden markkinahinta. Rekisteröityneet käyttäjät saavat myös historiallisen datan näkyviin — sekä markkinahinnan että tuotannon ja kulutuksen osalta. Tiedot esitetään graafisesti ja tilastollisesti.\n\nRekisteröityneet käyttäjät voivat myös käyttää chat-engineä, joka hyödyntää projektin omaa lähdemateriaalia ja täydentää käyttäjän kysymyksiä kontekstuaalisella tiedolla.\n\n---\n\n## Palveluiden yleiskuvaus\n\nJärjestelmä koostuu useista kontitetuista palveluista, jotka toimivat yhdessä Eprice-sovelluksen tarjoamiseksi. Näihin kuuluvat tietokanta, backend-palvelin, käyttöliittymä, sekä lisäkomponentit migraatioihin, testaukseen ja chattiin.\n\n### 1. Database\n- **Portti**: 5432 (PostgreSQL:n oletusportti)\n- **Tarkoitus**: Tallentaa sovelluksen dataa, mukaan lukien käyttäjätiedot ja hakua varten käytettävät embeddingit.\n\n### 2. Database Migrations\n\n- **Tarkoitus**: Hallitsee tietokannan skeemamuutoksia.\n- **Riippuu**: Database\n\n### 3. Server\n- **Portti**: 8000\n- **Tarkoitus**: Tarjoaa backend-API:t käyttöliittymälle ja muille palveluille.\n- **Riippuvuu**: Database\n- **Lisätoiminto**: Tekee ulkoisia API-kutsuja.\n\n### 4. Client\n- **Portti**: 5173 (Svelte-käyttöliittymä)\n- **Tarkoitus**: Frontend-sovellus käyttäjän vuorovaikutukseen.\n- **Riippuvuu**: Server, Chat Engine\n\n### 5. E2E Tests\n- **Tarkoitus**: Suorittaa järjestelmän päättymistestit.\n- **Riippuvuu**: Client\n\n## Lisäpalveluiden yleiskuvaus\n\n### Chat Engine (for: Developer Chat)\n- **Portit**: 7860–7861\n- **Tarkoitus**: Tarjoaa chat-moottorin projektin sisäiseen vuorovaikutukseen. Tämä on suunnattu kehittäjille ja ylläpitäjille, ei sovelluksen loppukäyttäjille. Chat perustuu retrieval augmented generation -menetelmään, ja siinä käytetty retriever saa käyttöönsä projektin dokumentaation ja koodin. Palvelussa on kaksi erillistä clientia: perinteinen streamaava chat ja Agent, joka on tarkoitettu monimutkaisempaan päättelyyn.\n- **Riippuvuudet**: Database\n\n\n### Data-preparation\n- **Tarkoitus**: Voidaan käyttää itsenäisesti datan hakemiseen tai päivittämiseen. Tiedot tallennetaan sellaiseen sijaintiin, joka on migraatioiden ja tietokannan saatavilla.\n\n### Backend-tests\n- **Tarkoitus**: Mahdollistaa backend-toimintojen testauksen riippumatta frontendistä. Vianmäärityksen lisäksi testit tuottavat lisätietoa ja varoituksia, jotka eivät välttämättä näy e2e-testeissä.\n- **Riippuvuudet**: Database ja Server\n\nBackend-testit on tarkoitettu ensisijaisesti kehitysvaiheeseen. Varsinaiset päätoiminnot pyritään kattamaan e2e-testeillä kehityksen loppuun mennessä.\n\n<div style=\"page-break-after: always;\"></div>\n\n## Pääpalvelut ja niiden vuorovaikutus\n\n![Pääpalvelut ja niiden vuorovaikutus](./diagrams/images/services_diagram.png)\n\n",
  "./LICENSE": "MIT License\n\nCopyright (c) 2025 Markus Kojo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
  "./README.md": "\n# Eprice project\n\nA complete README file in `./App/README.md`.\n\nApp directory holds the application code and all configurations.\n\nAdditional documentation in `./Documents`.\n\n"
}